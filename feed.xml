<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SuperMap iDesktop .NET – Production-Grade Container Orchestration</title>
    <link>https://docstest.github.io/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 07 Mar 2019 15:46:10 -0800</lastBuildDate>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://docstest.github.io/</link>
    </image>
    
	<atom:link href="https://docstest.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: Raw Block Volume support to Beta</title>
      <link>https://docstest.github.io/blog/2019/03/07/raw-block-volume-support-to-beta/</link>
      <pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/03/07/raw-block-volume-support-to-beta/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p&gt;

&lt;p&gt;Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.&lt;/p&gt;

&lt;h2 id=&#34;what-are-block-devices&#34;&gt;What are block devices?&lt;/h2&gt;

&lt;p&gt;Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.&lt;/p&gt;

&lt;p&gt;Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.&lt;/p&gt;

&lt;h2 id=&#34;why-add-raw-block-volumes-to-kubernetes&#34;&gt;Why add raw block volumes to kubernetes?&lt;/h2&gt;

&lt;p&gt;There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).&lt;/p&gt;

&lt;p&gt;From a programmer&amp;rsquo;s perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.&lt;/p&gt;

&lt;p&gt;As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-raw-blocks&#34;&gt;Which volume plugins support raw blocks?&lt;/h2&gt;

&lt;p&gt;As of the publishing of this blog, the following in-tree volumes types support raw blocks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AWS EBS&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;Fibre Channel&lt;/li&gt;
&lt;li&gt;GCE PD&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;Local volumes&lt;/li&gt;
&lt;li&gt;RBD (Ceph)&lt;/li&gt;
&lt;li&gt;Vsphere&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Out-of-tree &lt;a href=&#34;https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/&#34; target=&#34;_blank&#34;&gt;CSI volume drivers&lt;/a&gt; may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation &lt;a href=&#34;https://kubernetes-csi.github.io/docs/raw-block.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-raw-block-volume-api&#34;&gt;Kubernetes raw block volume API&lt;/h2&gt;

&lt;p&gt;Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; objects which bind to &lt;code&gt;PersistentVolume&lt;/code&gt; objects, and are attached to Pods in Kubernetes by including them in the volumes array of the &lt;code&gt;PodSpec&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are 2 important differences however. First, to request a raw block &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, you must set &lt;code&gt;volumeMode = &amp;quot;Block&amp;quot;&lt;/code&gt; in the &lt;code&gt;PersistentVolumeClaimSpec&lt;/code&gt;. Leaving &lt;code&gt;volumeMode&lt;/code&gt; blank is the same as specifying &lt;code&gt;volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code&gt; which results in the traditional behavior. &lt;code&gt;PersistentVolumes&lt;/code&gt; also have a &lt;code&gt;volumeMode&lt;/code&gt; field in their &lt;code&gt;PersistentVolumeSpec&lt;/code&gt;, and &lt;code&gt;&amp;quot;Block&amp;quot;&lt;/code&gt; type PVCs can only bind to &lt;code&gt;&amp;quot;Block&amp;quot;&lt;/code&gt; type PVs and &lt;code&gt;&amp;quot;Filesystem&amp;quot;&lt;/code&gt; PVCs can only bind to &lt;code&gt;&amp;quot;Filesystem&amp;quot;&lt;/code&gt; PVs.&lt;/p&gt;

&lt;p&gt;Secondly, when using a raw block volume in your Pods, you must specify a &lt;code&gt;VolumeDevice&lt;/code&gt; in the Container portion of the &lt;code&gt;PodSpec&lt;/code&gt; rather than a &lt;code&gt;VolumeMount&lt;/code&gt;. &lt;code&gt;VolumeDevices&lt;/code&gt; have &lt;code&gt;devicePaths&lt;/code&gt; instead of &lt;code&gt;mountPaths&lt;/code&gt;, and inside the container, applications will see a device at that path instead of a mounted file system.&lt;/p&gt;

&lt;p&gt;Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.&lt;/p&gt;

&lt;h2 id=&#34;creating-a-new-raw-block-pvc&#34;&gt;Creating a new raw block PVC&lt;/h2&gt;

&lt;p&gt;First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Block
  storageClassName: my-sc
  resources:
    requests:
    storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;using-a-raw-block-pvc&#34;&gt;Using a raw block PVC&lt;/h2&gt;

&lt;p&gt;When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: busybox
      command:
        - sleep
        - “3600”
      volumeDevices:
        - devicePath: /dev/block
          name: my-volume
      imagePullPolicy: IfNotPresent
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-raw-block-devices-to-my-csi-plugin&#34;&gt;As a storage vendor, how do I add support for raw block devices to my CSI plugin?&lt;/h2&gt;

&lt;p&gt;Raw block support for CSI plugins is still alpha, but support can be added today. The &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;CSI specification&lt;/a&gt; details how to handle requests for volume that have the &lt;code&gt;BlockVolume&lt;/code&gt; capability instead of the &lt;code&gt;MountVolume&lt;/code&gt; capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see &lt;a href=&#34;https://kubernetes-csi.github.io/docs/raw-block.html&#34; target=&#34;_blank&#34;&gt;documentation here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;issues-gotchas&#34;&gt;Issues/gotchas&lt;/h2&gt;

&lt;p&gt;Because block devices are actually devices, it’s possible to do low-level actions on them from inside containers that wouldn’t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.&lt;/p&gt;

&lt;p&gt;By default, Linux won’t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the &lt;code&gt;SYS_RAWIO&lt;/code&gt; capability to the container security context to allow this. See documentation &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Also, while Kubernetes is guaranteed to deliver a block device to the container, there’s no guarantee that it’s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How do I get involved?&lt;/p&gt;

&lt;p&gt;Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a&gt;
&lt;a href=&#34;https://github.com/container-storage-interface/community/blob/master/README.md&#34; target=&#34;_blank&#34;&gt;https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors who helped add block volume support to Kubernetes including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ben Swartzlander (&lt;a href=&#34;https://github.com/bswartz&#34; target=&#34;_blank&#34;&gt;https://github.com/bswartz&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Brad Childs (&lt;a href=&#34;https://github.com/childsb&#34; target=&#34;_blank&#34;&gt;https://github.com/childsb&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Erin Boyd (&lt;a href=&#34;https://github.com/erinboyd&#34; target=&#34;_blank&#34;&gt;https://github.com/erinboyd&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Masaki Kimura (&lt;a href=&#34;https://github.com/mkimuram&#34; target=&#34;_blank&#34;&gt;https://github.com/mkimuram&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Matthew Wong (&lt;a href=&#34;https://github.com/wongma7&#34; target=&#34;_blank&#34;&gt;https://github.com/wongma7&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;https://github.com/msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Mitsuhiro Tanino (&lt;a href=&#34;https://github.com/mtanino&#34; target=&#34;_blank&#34;&gt;https://github.com/mtanino&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;https://github.com/saad-ali&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Automate Operations on your Cluster with OperatorHub.io</title>
      <link>https://docstest.github.io/blog/2019/02/28/automate-operations-on-your-cluster-with-operatorhub.io/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/02/28/automate-operations-on-your-cluster-with-operatorhub.io/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;
Diane Mueller, Director of Community Development, Cloud Platforms, Red Hat&lt;/p&gt;

&lt;p&gt;One of the important challenges facing developers and Kubernetes administrators has been a lack of ability to quickly find common services that are operationally ready for Kubernetes. Typically, the presence of an Operator for a specific service - a pattern that was introduced in 2016 and has gained momentum - is a good signal for the operational readiness of the service on Kubernetes. However, there has to date not existed a registry of Operators to simplify the discovery of such services.&lt;/p&gt;

&lt;p&gt;To help address this challenge, today Red Hat is launching OperatorHub.io in collaboration with AWS, Google Cloud and Microsoft. OperatorHub.io enables developers and Kubernetes administrators to find and install curated Operator-backed services with a base level of documentation, active maintainership by communities or vendors, basic testing, and packaging for optimized life-cycle management on Kubernetes.&lt;/p&gt;

&lt;p&gt;The Operators currently in OperatorHub.io are just the start. We invite the Kubernetes community to join us in building a vibrant community for Operators by developing, packaging, and publishing Operators on OperatorHub.io.&lt;/p&gt;

&lt;h2 id=&#34;what-does-operatorhub-io-provide&#34;&gt;What does OperatorHub.io provide?&lt;/h2&gt;

&lt;p&gt;OperatorHub.io is designed to address the needs of both Kubernetes developers and users. For the former it provides a common registry where they can publish their Operators alongside with descriptions, relevant details like version, image, code repository and have them be readily packaged for installation. They can also update already published Operators to new versions when they are released.&lt;/p&gt;

&lt;p&gt;Users get the ability to discover and download Operators at a central location, that has content which has been screened for the previously mentioned criteria and scanned for known vulnerabilities. In addition, developers can guide users of their Operators with prescriptive examples of the &lt;code&gt;CustomResources&lt;/code&gt; that they introduce to interact with the application.&lt;/p&gt;

&lt;h2 id=&#34;what-is-an-operator&#34;&gt;What is an Operator?&lt;/h2&gt;

&lt;p&gt;Operators were first introduced in 2016 by CoreOS and have been used by Red Hat and the Kubernetes community as a way to package, deploy and manage a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and well-known tooling, like kubectl.&lt;/p&gt;

&lt;p&gt;An Operator is implemented as a custom controller that watches for certain Kubernetes resources to appear, be modified or deleted. These are typically &lt;code&gt;CustomResourceDefinitions&lt;/code&gt; that the Operator “owns.” In the spec properties of these objects the user declares the desired state of the application or the operation. The Operator’s reconciliation loop will pick these up and perform the required actions to achieve the desired state. For example, the intent to create a highly available etcd cluster could be expressed by creating an new resource of type &lt;code&gt;EtcdCluster&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: &amp;quot;etcd.database.coreos.com/v1beta2&amp;quot;
kind: &amp;quot;EtcdCluster&amp;quot;
metadata:
  name: &amp;quot;my-etcd-cluster&amp;quot;
spec:
  size: 3
  version: &amp;quot;3.3.12&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;EtcdOperator&lt;/code&gt; would be responsible for creating a 3-node etcd cluster running version v3.3.12 as a result. Similarly, an object of type &lt;code&gt;EtcdBackup&lt;/code&gt; could be defined to express the intent to create a consistent backup of the etcd database to an S3 bucket.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-create-and-run-an-operator&#34;&gt;How do I create and run an Operator?&lt;/h2&gt;

&lt;p&gt;One way to get started is with the &lt;a href=&#34;https://github.com/operator-framework&#34; target=&#34;_blank&#34;&gt;Operator Framework&lt;/a&gt;, an open source toolkit that provides an SDK, lifecycle management, metering and monitoring capabilities. It enables developers to build, test, and package Operators. Operators can be implemented in several programming and automation languages, including Go, Helm, and Ansible, all three of which are supported directly by the SDK.&lt;/p&gt;

&lt;p&gt;If you are interested in creating your own Operator, we recommend checking out the Operator Framework to &lt;a href=&#34;https://github.com/operator-framework/getting-started&#34; target=&#34;_blank&#34;&gt;get started&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Operators vary in where they fall along &lt;a href=&#34;https://github.com/operator-framework/operator-sdk/blob/master/doc/images/operator-maturity-model.png&#34; target=&#34;_blank&#34;&gt;the capability spectrum&lt;/a&gt; ranging from basic functionality to having specific operational logic for an application to automate advanced scenarios like backup, restore or tuning. Beyond basic installation, advanced Operators are designed to handle upgrades more seamlessly and react to failures automatically. Currently, Operators on OperatorHub.io span the maturity spectrum, but we anticipate their continuing maturation over time.&lt;/p&gt;

&lt;p&gt;While Operators on OperatorHub.io don’t need to be implemented using the SDK, they are packaged for deployment through the &lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager&#34; target=&#34;_blank&#34;&gt;Operator Lifecycle Manager&lt;/a&gt; (OLM). The format mainly consists of a YAML manifest referred to as &lt;code&gt;[ClusterServiceVersion]&lt;/code&gt;(&lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/building-your-csv.md&#34; target=&#34;_blank&#34;&gt;https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/building-your-csv.md&lt;/a&gt;) which provides information about the &lt;code&gt;CustomResourceDefinitions&lt;/code&gt; the Operator owns or requires, which RBAC definition it needs, where the image is stored, etc. This file is usually accompanied by additional YAML files which define the Operators’ own CRDs. This information is processed by OLM at the time a user requests to install an Operator to provide dependency resolution and automation.&lt;/p&gt;

&lt;h2 id=&#34;what-does-listing-of-an-operator-on-operatorhub-io-mean&#34;&gt;What does listing of an Operator on OperatorHub.io mean?&lt;/h2&gt;

&lt;p&gt;To be listed, Operators must successfully show cluster lifecycle features, be packaged as a CSV to be maintained through OLM, and have acceptable documentation for its intended users.&lt;/p&gt;

&lt;p&gt;Some examples of Operators that are currently listed on OperatorHub.io include: Amazon Web Services Operator, Couchbase Autonomous Operator, CrunchyData’s PostgreSQL, etcd Operator, Jaeger Operator for Kubernetes, Kubernetes Federation Operator, MongoDB Enterprise Operator, Percona MySQL Operator, PlanetScale’s Vitess Operator, Prometheus Operator, and Redis Operator.&lt;/p&gt;

&lt;h2 id=&#34;want-to-add-your-operator-to-operatorhub-io-follow-these-steps&#34;&gt;Want to add your Operator to OperatorHub.io? Follow these steps&lt;/h2&gt;

&lt;p&gt;If you have an existing Operator, follow the &lt;a href=&#34;https://www.operatorhub.io/contribute&#34; target=&#34;_blank&#34;&gt;contribution guide&lt;/a&gt; using a fork of the &lt;a href=&#34;https://github.com/operator-framework/community-operators/&#34; target=&#34;_blank&#34;&gt;community-operators&lt;/a&gt; repository. Each contribution contains the CSV, all of the &lt;code&gt;CustomResourceDefinitions&lt;/code&gt;, access control rules and references to the container image needed to install and run your Operator, plus other info like a description of its features and supported Kubernetes versions. A complete example, including multiple versions of the Operator, can be found with the &lt;a href=&#34;https://github.com/operator-framework/community-operators/tree/master/community-operators/etcd&#34; target=&#34;_blank&#34;&gt;EtcdOperator&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After testing out your Operator on your own cluster, submit a PR to the &lt;a href=&#34;https://github.com/operator-framework/community-operators&#34; target=&#34;_blank&#34;&gt;community repository&lt;/a&gt; with all of YAML files following &lt;a href=&#34;https://github.com/operator-framework/community-operators#adding-your-operator&#34; target=&#34;_blank&#34;&gt;this directory structure&lt;/a&gt;. Subsequent versions of the Operator can be published in the same way. At first this will be reviewed manually, but automation is on the way. After it’s merged by the maintainers, it will show up on OperatorHub.io along with its documentation and a convenient installation method.&lt;/p&gt;

&lt;h2 id=&#34;want-to-learn-more&#34;&gt;Want to learn more?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Attend one of the upcoming Kubernetes Operator Framework hands-on workshops at &lt;a href=&#34;https://www.socallinuxexpo.org/scale/17x/presentations/workshop-kubernetes-operator-framework&#34; target=&#34;_blank&#34;&gt;ScaleX&lt;/a&gt; in Pasadena on March 7 and at the &lt;a href=&#34;https://commons.openshift.org/gatherings/Santa_Clara_2019.html&#34; target=&#34;_blank&#34;&gt;OpenShift Commons Gathering on Operating at Scale in Santa Clara on March 11&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Listen to this &lt;a href=&#34;https://www.youtube.com/watch?v=GgEKEYH9MMM&amp;amp;feature=youtu.be&#34; target=&#34;_blank&#34;&gt;OpenShift Commons Briefing on “The State of Operators” with Daniel Messer and Diane Mueller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join in on the online conversations in the community &lt;a href=&#34;https://kubernetes.slack.com/messages/CAW0GV7A5&#34; target=&#34;_blank&#34;&gt;Kubernetes-Operator Slack Channel&lt;/a&gt; and the &lt;a href=&#34;https://groups.google.com/forum/#!forum/operator-framework&#34; target=&#34;_blank&#34;&gt;Operator Framework Google Group&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finally, read up on how to add your Operator to OperatorHub.io: &lt;a href=&#34;https://operatorhub.io/contribute&#34; target=&#34;_blank&#34;&gt;https://operatorhub.io/contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Building a Kubernetes Edge (Ingress) Control Plane for Envoy v2</title>
      <link>https://docstest.github.io/blog/2019/02/12/building-a-kubernetes-edge-control-plane-for-envoy-v2/</link>
      <pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/02/12/building-a-kubernetes-edge-control-plane-for-envoy-v2/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt;
Daniel Bryant, Product Architect, Datawire;
Flynn, Ambassador Lead Developer, Datawire;
Richard Li, CEO and Co-founder, Datawire&lt;/p&gt;

&lt;p&gt;Kubernetes has become the de facto runtime for container-based microservice applications, but this orchestration framework alone does not provide all of the infrastructure necessary for running a distributed system. Microservices typically communicate through Layer 7 protocols such as HTTP, gRPC, or WebSockets, and therefore having the ability to make routing decisions, manipulate protocol metadata, and observe at this layer is vital. However, traditional load balancers and edge proxies have predominantly focused on L3/4 traffic. This is where the &lt;a href=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34;&gt;Envoy Proxy&lt;/a&gt; comes into play.&lt;/p&gt;

&lt;p&gt;Envoy proxy was designed as a &lt;a href=&#34;https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a&#34; target=&#34;_blank&#34;&gt;universal data plane&lt;/a&gt; from the ground-up by the Lyft Engineering team for today&amp;rsquo;s distributed, L7-centric world, with broad support for L7 protocols, a real-time API for managing its configuration, first-class observability, and high performance within a small memory footprint. However, Envoy&amp;rsquo;s vast feature set and flexibility of operation also makes its configuration highly complicated &amp;ndash; this is evident from looking at its rich but verbose &lt;a href=&#34;https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc&#34; target=&#34;_blank&#34;&gt;control plane&lt;/a&gt; syntax.&lt;/p&gt;

&lt;p&gt;With the open source &lt;a href=&#34;https://www.getambassador.io&#34; target=&#34;_blank&#34;&gt;Ambassador API Gateway&lt;/a&gt;, we wanted to tackle the challenge of creating a new control plane that focuses on the use case of deploying Envoy as an forward-facing edge proxy within a Kubernetes cluster, in a way that is idiomatic to Kubernetes operators. In this article, we&amp;rsquo;ll walk through two major iterations of the Ambassador design, and how we integrated Ambassador with Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;ambassador-pre-2019-envoy-v1-apis-jinja-template-files-and-hot-restarts&#34;&gt;Ambassador pre-2019: Envoy v1 APIs, Jinja Template Files, and Hot Restarts&lt;/h2&gt;

&lt;p&gt;Ambassador itself is deployed within a container as a Kubernetes service, and uses annotations added to Kubernetes Services as its &lt;a href=&#34;https://www.getambassador.io/reference/configuration&#34; target=&#34;_blank&#34;&gt;core configuration model&lt;/a&gt;. This approach &lt;a href=&#34;https://www.getambassador.io/concepts/developers&#34; target=&#34;_blank&#34;&gt;enables application developers to manage routing&lt;/a&gt; as part of the Kubernetes service definition. We explicitly decided to go down this route because of &lt;a href=&#34;https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d&#34; target=&#34;_blank&#34;&gt;limitations&lt;/a&gt; in the current &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34; target=&#34;_blank&#34;&gt;Ingress API spec&lt;/a&gt;, and we liked the simplicity of extending Kubernetes services, rather than introducing another custom resource type. An example of an Ambassador annotation can be seen here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  name: my-service
  annotations:
    getambassador.io/config: |
      ---
        apiVersion: ambassador/v0
        kind:  Mapping
        name:  my_service_mapping
        prefix: /my-service/
        service: my-service
spec:
  selector:
    app: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Translating this simple Ambassador annotation config into valid &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/v1.6.0/configuration/overview/v1_overview&#34; target=&#34;_blank&#34;&gt;Envoy v1&lt;/a&gt; config was not a trivial task. By design, Ambassador&amp;rsquo;s configuration isn&amp;rsquo;t based on the same conceptual model as Envoy&amp;rsquo;s configuration &amp;ndash; we deliberately wanted to aggregate and simplify operations and config. Therefore, translating between one set of concepts to the other involves a fair amount of logic within Ambassador.&lt;/p&gt;

&lt;p&gt;In this first iteration of Ambassador we created a Python-based service that watched the Kubernetes API for changes to Service objects. When new or updated Ambassador annotations were detected, these were translated from the Ambassador syntax into an intermediate representation (IR) which embodied our core configuration model and concepts. Next, Ambassador translated this IR into a representative Envoy configuration which was saved as a file within pods associated with the running Ambassador k8s Service. Ambassador then &amp;ldquo;hot-restarted&amp;rdquo; the Envoy process running within the Ambassador pods, which triggered the loading of the new configuration.&lt;/p&gt;

&lt;p&gt;There were many benefits with this initial implementation. The mechanics involved were fundamentally simple, the transformation of Ambassador config into Envoy config was reliable, and the file-based hot restart integration with Envoy was dependable.&lt;/p&gt;

&lt;p&gt;However, there were also notable challenges with this version of Ambassador. First, although the hot restart was effective for the majority of our customers&amp;rsquo; use cases, it was not very fast, and some customers (particularly those with huge application deployments) found it was limiting the frequency with which they could change their configuration. Hot restart can also drop connections, especially long-lived connections like WebSockets or gRPC streams.&lt;/p&gt;

&lt;p&gt;More crucially, though, the first implementation of the IR allowed rapid prototyping but was primitive enough that it proved very difficult to make substantial changes. While this was a pain point from the beginning, it became a critical issue as Envoy shifted to the &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/configuration/overview/v2_overview&#34; target=&#34;_blank&#34;&gt;Envoy v2 API&lt;/a&gt;. It was clear that the v2 API would offer Ambassador many benefits &amp;ndash; as Matt Klein outlined in his blog post, &amp;ldquo;&lt;a href=&#34;https://blog.envoyproxy.io/the-universal-data-plane-api-d15cec7a&#34; target=&#34;_blank&#34;&gt;The universal data plane API&lt;/a&gt;&amp;rdquo; &amp;ndash; including access to new features and a solution to the connection-drop problem noted above, but it was also clear that the existing IR implementation was not capable of making the leap.&lt;/p&gt;

&lt;h2 id=&#34;ambassador-v0-50-envoy-v2-apis-ads-testing-with-kat-and-golang&#34;&gt;Ambassador &amp;gt;= v0.50: Envoy v2 APIs (ADS), Testing with KAT, and Golang&lt;/h2&gt;

&lt;p&gt;In consultation with the &lt;a href=&#34;http://d6e.co/slack&#34; target=&#34;_blank&#34;&gt;Ambassador community&lt;/a&gt;, the &lt;a href=&#34;www.datawire.io&#34; target=&#34;_blank&#34;&gt;Datawire&lt;/a&gt; team undertook a redesign of the internals of Ambassador in 2018. This was driven by two key goals. First, we wanted to integrate Envoy&amp;rsquo;s v2 configuration format, which would enable the support of features such as &lt;a href=&#34;https://www.getambassador.io/user-guide/sni/&#34; target=&#34;_blank&#34;&gt;SNI&lt;/a&gt;, &lt;a href=&#34;https://www.getambassador.io/user-guide/rate-limiting&#34; target=&#34;_blank&#34;&gt;rate limiting&lt;/a&gt; and &lt;a href=&#34;https://www.getambassador.io/user-guide/auth-tutorial&#34; target=&#34;_blank&#34;&gt;gRPC authentication APIs&lt;/a&gt;. Second, we also wanted to do much more robust semantic validation of Envoy configuration due to its increasing complexity (particularly when operating with large-scale application deployments).&lt;/p&gt;

&lt;h3 id=&#34;initial-stages&#34;&gt;Initial stages&lt;/h3&gt;

&lt;p&gt;We started by restructuring the Ambassador internals more along the lines of a multipass compiler. The class hierarchy was made to more closely mirror the separation of concerns between the Ambassador configuration resources, the IR, and the Envoy configuration resources. Core parts of Ambassador were also redesigned to facilitate contributions from the community outside Datawire. We decided to take this approach for several reasons. First, Envoy Proxy is a very fast moving project, and we realized that we needed an approach where a seemingly minor Envoy configuration change didn&amp;rsquo;t result in days of reengineering within Ambassador. In addition, we wanted to be able to provide semantic verification of configuration.&lt;/p&gt;

&lt;p&gt;As we started working more closely with Envoy v2, a testing challenge was quickly identified. As more and more features were being supported in Ambassador, more and more bugs appeared in Ambassador&amp;rsquo;s handling of less common but completely valid combinations of features. This drove to creation of a new testing requirement that meant Ambassador&amp;rsquo;s test suite needed to be reworked to automatically manage many combinations of features, rather than relying on humans to write each test individually. Moreover, we wanted the test suite to be fast in order to maximize engineering productivity.&lt;/p&gt;

&lt;p&gt;Thus, as part of the Ambassador rearchitecture, we introduced the &lt;a href=&#34;https://github.com/datawire/ambassador/tree/master/kat&#34; target=&#34;_blank&#34;&gt;Kubernetes Acceptance Test (KAT)&lt;/a&gt; framework. KAT is an extensible test framework that:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deploys a bunch of services (along with Ambassador) to a Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Run a series of verification queries against the spun up APIs&lt;/li&gt;
&lt;li&gt;Perform a bunch of assertions on those query results&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;KAT is designed for performance &amp;ndash; it batches test setup upfront, and then runs all the queries in step 3 asynchronously with a high performance client. The traffic driver in KAT runs locally using &lt;a href=&#34;https://www.telepresence.io&#34; target=&#34;_blank&#34;&gt;Telepresence&lt;/a&gt;, which makes it easier to debug issues.&lt;/p&gt;

&lt;h3 id=&#34;introducing-golang-to-the-ambassador-stack&#34;&gt;Introducing Golang to the Ambassador Stack&lt;/h3&gt;

&lt;p&gt;With the KAT test framework in place, we quickly ran into some issues with Envoy v2 configuration and hot restart, which presented the opportunity to switch to use Envoy’s Aggregated Discovery Service (ADS) APIs instead of hot restart. This completely eliminated the requirement for restart on configuration changes, which we found could lead to dropped connection under high loads or long-lived connections.&lt;/p&gt;

&lt;p&gt;However, we faced an interesting question as we considered the move to the ADS. The ADS is not as simple as one might expect: there are explicit ordering dependencies when sending updates to Envoy. The Envoy project has reference implementations of the ordering logic, but only in Go and Java, where Ambassador was primarily in Python. We agonized a bit, and decided that the simplest way forward was to accept the polyglot nature of our world, and do our ADS implementation in Go.&lt;/p&gt;

&lt;p&gt;We also found, with KAT,  that our testing had reached the point where Python’s performance with many network connections was a limitation, so we took advantage of Go here, as well, writing KAT’s querying and backend services primarily in Go. After all, what’s another Golang dependency when you’ve already taken the plunge?&lt;/p&gt;

&lt;p&gt;With a new test framework, new IR generating valid Envoy v2 configuration, and the ADS, we thought we were done with the major architectural changes in Ambassador 0.50. Alas, we hit one more issue. On the Azure Kubernetes Service, Ambassador annotation changes were no longer being detected.&lt;/p&gt;

&lt;p&gt;Working with the highly-responsive AKS engineering team, we were able to identify the issue &amp;ndash; namely, the Kubernetes API server in AKS is exposed through a chain of proxies, requiring clients to be updating to understand how to connect using the FQDN of the API server, which is provided through a mutating webhook in AKS. Unfortunately, support for this feature was not available in the official Kubernetes Python client, so this was the third spot where we chose to switch to Go instead of Python.&lt;/p&gt;

&lt;p&gt;This raises the interesting question of, “why not ditch all the Python code, and just rewrite Ambassador entirely in Go?” It’s a valid question. The main concern with a rewrite is that Ambassador and Envoy operate at different conceptual levels rather than simply expressing the same concepts with different syntax. Being certain that we’ve expressed the conceptual bridges in a new language is not a trivial challenge, and not something to undertake without already having really excellent test coverage in place&lt;/p&gt;

&lt;p&gt;At this point, we use Go to coverage very specific, well-contained functions that can be verified for correctness much more easily that we could verify a complete Golang rewrite. In the future, who knows? But for 0.50.0, this functional split let us both take advantage of Golang’s strengths, while letting us retain more confidence about all the changes already in 0.50.&lt;/p&gt;

&lt;h2 id=&#34;lessons-learned&#34;&gt;Lessons Learned&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve learned a lot in the process of building &lt;a href=&#34;https://blog.getambassador.io/ambassador-0-50-ga-release-notes-sni-new-authservice-and-envoy-v2-support-3b30a4d04c81&#34; target=&#34;_blank&#34;&gt;Ambassador 0.50&lt;/a&gt;. Some of our key takeaways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes and Envoy are very powerful frameworks, but they are also extremely fast moving targets &amp;ndash; there is sometimes no substitute for reading the source code and talking to the maintainers (who are fortunately all quite accessible!)&lt;/li&gt;
&lt;li&gt;The best supported libraries in the Kubernetes / Envoy ecosystem are written in Go. While we love Python, we have had to adopt Go so that we&amp;rsquo;re not forced to maintain too many components ourselves.&lt;/li&gt;
&lt;li&gt;Redesigning a test harness is sometimes necessary to move your software forward.&lt;/li&gt;
&lt;li&gt;The real cost in redesigning a test harness is often in porting your old tests to the new harness implementation.&lt;/li&gt;
&lt;li&gt;Designing (and implementing) an effective control plane for the edge proxy use case has been challenging, and the feedback from the open source community around Kubernetes, Envoy and Ambassador has been extremely useful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Migrating Ambassador to the Envoy v2 configuration and ADS APIs was a long and difficult journey that required lots of architecture and design discussions and plenty of coding, but early feedback from results have been positive. &lt;a href=&#34;https://blog.getambassador.io/announcing-ambassador-0-50-8dffab5b05e0&#34; target=&#34;_blank&#34;&gt;Ambassador 0.50 is available now&lt;/a&gt;, so you can take it for a test run and share your feedback with the community on our &lt;a href=&#34;http://d6e.co/slack&#34; target=&#34;_blank&#34;&gt;Slack channel&lt;/a&gt; or on &lt;a href=&#34;https://www.twitter.com/getambassadorio&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Runc and CVE-2019-5736</title>
      <link>https://docstest.github.io/blog/2019/02/11/runc-and-cve-2019-5736/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/02/11/runc-and-cve-2019-5736/</guid>
      <description>
        
        
        

&lt;p&gt;This morning &lt;a href=&#34;https://www.openwall.com/lists/oss-security/2019/02/11/2&#34; target=&#34;_blank&#34;&gt;a container escape vulnerability in runc was announced&lt;/a&gt;. We wanted to provide some guidance to Kubernetes users to ensure everyone is safe and secure.&lt;/p&gt;

&lt;h2 id=&#34;what-is-runc&#34;&gt;What Is Runc?&lt;/h2&gt;

&lt;p&gt;Very briefly, runc is the low-level tool which does the heavy lifting of spawning a Linux container. Other tools like Docker, Containerd, and CRI-O sit on top of runc to deal with things like data formatting and serialization, but runc is at the heart of all of these systems.&lt;/p&gt;

&lt;p&gt;Kubernetes in turn sits on top of those tools, and so while no part of Kubernetes itself is vulnerable, most Kubernetes installations are using runc under the hood.&lt;/p&gt;

&lt;h3 id=&#34;what-is-the-vulnerability&#34;&gt;What Is The Vulnerability?&lt;/h3&gt;

&lt;p&gt;While full details are still embargoed to give people time to patch, the rough version is that when running a process as root (UID 0) inside a container, that process can exploit a bug in runc to gain root privileges on the host running the container. This then allows them unlimited access to the server as well as any other containers on that server.&lt;/p&gt;

&lt;p&gt;If the process inside the container is either trusted (something you know is not hostile) or is not running as UID 0, then the vulnerability does not apply. It can also be prevented by SELinux, if an appropriate policy has been applied. RedHat Enterprise Linux and CentOS both include appropriate SELinux permissions with their packages and so are believed to be unaffected if SELinux is enabled.&lt;/p&gt;

&lt;p&gt;The most common source of risk is attacker-controller container images, such as unvetted images from public repositories.&lt;/p&gt;

&lt;h3 id=&#34;what-should-i-do&#34;&gt;What Should I Do?&lt;/h3&gt;

&lt;p&gt;As with all security issues, the two main options are to mitigate the vulnerability or upgrade your version of runc to one that includes the fix.&lt;/p&gt;

&lt;p&gt;As the exploit requires UID 0 within the container, a direct mitigation is to ensure all your containers are running as a non-0 user. This can be set within the container image, or via your pod specification:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Pod&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;run-as-uid-&lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;securityContext:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1000&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# ...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This can also be enforced globally using a PodSecurityPolicy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;policy/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PodSecurityPolicy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;non-root&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;privileged:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;allowPrivilegeEscalation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Require the container to run without root privileges.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;rule:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;MustRunAsNonRoot&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Setting a policy like this is highly encouraged given the overall risks of running as UID 0 inside a container.&lt;/p&gt;

&lt;p&gt;Another potential mitigation is to ensure all your container images are vetted and trusted. This can be accomplished by building all your images yourself, or by vetting the contents of an image and then pinning to the image version hash (&lt;code&gt;image: external/someimage@sha256:7832659873hacdef&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Upgrading runc can generally be accomplished by upgrading the package &lt;code&gt;runc&lt;/code&gt; for your distribution or by upgrading your OS image if using immutable images. This is a list of known safe versions for various distributions and platforms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu - &lt;a href=&#34;https://people.canonical.com/~ubuntu-security/cve/2019/CVE-2019-5736.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;runc 1.0.0~rc4+dfsg1-6ubuntu0.18.10.1&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Debian - &lt;a href=&#34;https://security-tracker.debian.org/tracker/CVE-2019-5736&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;runc 1.0.0~rc6+dfsg1-2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RedHat Enterprise Linux - &lt;a href=&#34;https://access.redhat.com/security/vulnerabilities/runcescape&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docker 1.13.1-91.git07f3374.el7&lt;/code&gt;&lt;/a&gt; (if SELinux is disabled)&lt;/li&gt;
&lt;li&gt;Amazon Linux - &lt;a href=&#34;https://alas.aws.amazon.com/ALAS-2019-1156.html&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;docker 18.06.1ce-7.25.amzn1.x86_64&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoreOS - Stable: &lt;a href=&#34;https://coreos.com/releases/#1967.5.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;1967.5.0&lt;/code&gt;&lt;/a&gt; / Beta: &lt;a href=&#34;https://coreos.com/releases/#2023.2.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;2023.2.0&lt;/code&gt;&lt;/a&gt; / Alpha: &lt;a href=&#34;https://coreos.com/releases/#2051.0.0&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;2051.0.0&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kops Debian - &lt;a href=&#34;https://github.com/kubernetes/kops/pull/6460&#34; target=&#34;_blank&#34;&gt;in progress&lt;/a&gt; (see &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md&#34; target=&#34;_blank&#34;&gt;advisory&lt;/a&gt; for how to address until Kops Debian is patched)&lt;/li&gt;
&lt;li&gt;Docker - &lt;a href=&#34;https://github.com/docker/docker-ce/releases/tag/v18.09.2&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;18.09.2&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some platforms have also posted more specific instructions:&lt;/p&gt;

&lt;h4 id=&#34;google-container-engine-gke&#34;&gt;Google Container Engine (GKE)&lt;/h4&gt;

&lt;p&gt;Google has issued a &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/security-bulletins#february-11-2019-runc&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with more detailed information but in short, if you are using the default GKE node image then you are safe. If you are using an Ubuntu node image then you will need to mitigate or upgrade to an image with a fixed version of runc.&lt;/p&gt;

&lt;h4 id=&#34;amazon-elastic-container-service-for-kubernetes-eks&#34;&gt;Amazon Elastic Container Service for Kubernetes (EKS)&lt;/h4&gt;

&lt;p&gt;Amazon has also issued a &lt;a href=&#34;https://aws.amazon.com/security/security-bulletins/AWS-2019-002/&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with more detailed information. All EKS users should mitigate the issue or upgrade to a new node image.&lt;/p&gt;

&lt;h4 id=&#34;azure-kubernetes-service-aks&#34;&gt;Azure Kubernetes Service (AKS)&lt;/h4&gt;

&lt;p&gt;Microsoft has issued a &lt;a href=&#34;https://azure.microsoft.com/en-us/updates/cve-2019-5736-and-runc-vulnerability/&#34; target=&#34;_blank&#34;&gt;security bulletin&lt;/a&gt; with detailed information on mitigating the issue. Microsoft recommends all AKS users to upgrade their cluster to mitigate the issue.&lt;/p&gt;

&lt;h4 id=&#34;kops&#34;&gt;Kops&lt;/h4&gt;

&lt;p&gt;Kops has issued an &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/advisories/cve_2019_5736.md&#34; target=&#34;_blank&#34;&gt;advisory&lt;/a&gt; with detailed information on mitigating this issue.&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;We don&amp;rsquo;t have specific confirmation that Docker for Mac and Docker for Windows are vulnerable, however it seems likely. Docker has released a fix in &lt;a href=&#34;https://github.com/docker/docker-ce/releases/tag/v18.09.2&#34; target=&#34;_blank&#34;&gt;version 18.09.2&lt;/a&gt; and it is recommended you upgrade to it. This also applies to other deploy systems using Docker under the hood.&lt;/p&gt;

&lt;p&gt;If you are unable to upgrade Docker, the Rancher team has provided backports of the fix for many older versions at &lt;a href=&#34;https://github.com/rancher/runc-cve&#34; target=&#34;_blank&#34;&gt;github.com/rancher/runc-cve&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;getting-more-information&#34;&gt;Getting More Information&lt;/h2&gt;

&lt;p&gt;If you have any further questions about how this vulnerability impacts Kubernetes, please join us at &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;discuss.kubernetes.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you would like to get in contact with the &lt;a href=&#34;https://github.com/opencontainers/org/blob/master/README.md#communications&#34; target=&#34;_blank&#34;&gt;runc team&lt;/a&gt;, you can reach them on &lt;a href=&#34;https://groups.google.com/a/opencontainers.org/forum/#!forum/dev&#34; target=&#34;_blank&#34;&gt;Google Groups&lt;/a&gt; or &lt;code&gt;#opencontainers&lt;/code&gt; on Freenode IRC.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Poseidon-Firmament Scheduler – Flow Network Graph Based Scheduler</title>
      <link>https://docstest.github.io/blog/2019/02/06/poseidon-firmament-scheduler-flow-network-graph-based-scheduler/</link>
      <pubDate>Wed, 06 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/02/06/poseidon-firmament-scheduler-flow-network-graph-based-scheduler/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Deepak Vij (Huawei), Shivram Shrivastava (Huawei)&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Cluster Management systems such as Mesos, Google Borg, Kubernetes etc. in a cloud scale datacenter environment (also termed as &lt;strong&gt;&lt;em&gt;Datacenter-as-a-Computer&lt;/em&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;em&gt;Warehouse-Scale Computing - WSC&lt;/em&gt;&lt;/strong&gt;) typically manage application workloads by performing tasks such as tracking machine live-ness, starting, monitoring, terminating workloads and more importantly using a &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; to decide on workload placements.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; essentially performs the scheduling of workloads to compute resources – combining the global placement of work across the WSC environment makes the “warehouse-scale computer” more efficient, increases utilization, and saves energy. &lt;strong&gt;Cluster Scheduler&lt;/strong&gt; examples are Google Borg, Kubernetes, Firmament, Mesos, Tarcil, Quasar, Quincy, Swarm, YARN, Nomad, Sparrow, Apollo etc.&lt;/p&gt;

&lt;p&gt;In this blog post, we briefly describe the novel Firmament flow network graph based scheduling approach (&lt;a href=&#34;https://www.usenix.org/conference/osdi16/technical-sessions/presentation/gog&#34; target=&#34;_blank&#34;&gt;OSDI paper&lt;/a&gt;) in Kubernetes. We specifically describe the Firmament Scheduler and how it integrates with the Kubernetes cluster manager using Poseidon as the integration glue. We have seen extremely impressive scheduling throughput performance benchmarking numbers with this novel scheduling approach. Originally, Firmament Scheduler was conceptualized, designed and implemented by University of Cambridge researchers, &lt;a href=&#34;http://www.malteschwarzkopf.de/&#34; target=&#34;_blank&#34;&gt;Malte Schwarzkopf&lt;/a&gt; &amp;amp; &lt;a href=&#34;http://ionelgog.org/&#34; target=&#34;_blank&#34;&gt;Ionel Gog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;poseidon-firmament-scheduler-how-it-works&#34;&gt;Poseidon-Firmament Scheduler – How It Works&lt;/h2&gt;

&lt;p&gt;At a very high level, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34; target=&#34;_blank&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; augments the current Kubernetes scheduling capabilities by incorporating novel flow network graph based scheduling capabilities alongside the default Kubernetes Scheduler. It models the scheduling problem as a constraint-based optimization over a flow network graph – by reducing scheduling to a min-cost max-flow optimization problem. Due to the inherent rescheduling capabilities, the new scheduler enables a globally optimal scheduling environment that constantly keeps refining the workloads placements dynamically.&lt;/p&gt;

&lt;h2 id=&#34;key-advantages&#34;&gt;Key Advantages&lt;/h2&gt;

&lt;p&gt;Flow graph scheduling based &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34; target=&#34;_blank&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; provides the following key advantages:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Workloads (pods) are bulk scheduled to enable scheduling decisions at massive scale.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Based on the extensive performance test results, Poseidon-Firmament scales much better than Kubernetes default scheduler as the number of nodes increase in a cluster. This is due to the fact that Poseidon-Firmament is able to amortize more and more work across workloads.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Poseidon-Firmament Scheduler outperforms the Kubernetes default scheduler by a wide margin when it comes to throughput performance numbers for scenarios where compute resource requirements are somewhat uniform across jobs (Replicasets/Deployments/Jobs). Poseidon-Firmament scheduler end-to-end throughput performance numbers, including bind time, consistently get better as the number of nodes in a cluster increase. For example, for a 2,700 node cluster (shown in the graphs &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon/blob/master/docs/benchmark/README.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;), Poseidon-Firmament scheduler achieves a 7X or greater end-to-end throughput than the Kubernetes default scheduler, which includes bind time.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Availability of complex rule constraints.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scheduling in Poseidon-Firmament is very dynamic; it keeps cluster resources in a global optimal state during every scheduling run.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Highly efficient resource utilizations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;firmament-flow-network-graph-an-overview&#34;&gt;Firmament Flow Network Graph – An Overview&lt;/h2&gt;

&lt;p&gt;Firmament scheduler runs a min-cost flow algorithm over the flow network to find an optimal flow, from which it extracts the implied workload (pod placements). A flow network is a directed graph whose arcs carry flow from source nodes (i.e. pod nodes) to a sink node. A cost and capacity associated with each arc constrain the flow, and specify preferential routes for it.&lt;/p&gt;

&lt;p&gt;Figure 1 below shows an example of a flow network for a cluster with two tasks (workloads or pods) and four machines (nodes) – each workload on the left hand side, is a source of one unit of flow. All such flow must be drained into the sink node (S) for a feasible solution to the optimization problem.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2019-02-03-poseidon-firmament-scheduler/example-of-a-flow-network.png&#34;
         alt=&#34;Figure 1. Example of a Flow Network&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 1. Example of a Flow Network&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;h2 id=&#34;poseidon-mediation-layer-an-overview&#34;&gt;Poseidon Mediation Layer – An Overview&lt;/h2&gt;

&lt;p&gt;Poseidon is a service that acts as the integration glue for the Firmament scheduler with Kubernetes. It augments the current Kubernetes scheduling capabilities by incorporating new flow network graph based Firmament scheduling capabilities alongside the default Kubernetes Scheduler; multiple schedulers running simultaneously. Figure 2 below describes the high level overall design as far as how Poseidon integration glue works in conjunction with the underlying Firmament flow network graph based scheduler.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2019-02-03-poseidon-firmament-scheduler/firmament-kubernetes-integration-overview.png&#34;
         alt=&#34;Figure 2. Firmament Kubernetes Integration Overview&#34; width=&#34;600&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Figure 2. Firmament Kubernetes Integration Overview&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;As part of the Kubernetes multiple schedulers support, each new pod is typically scheduled by the default scheduler, but Kubernetes can be instructed to use another scheduler by specifying the name of another custom scheduler (in our case, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34; target=&#34;_blank&#34;&gt;Poseidon-Firmament&lt;/a&gt;) at the time of pod deployment. In this case, the default scheduler will ignore that Pod and allow Poseidon scheduler to schedule the Pod to a relevant node.&lt;/p&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;Note:&lt;/strong&gt; For details about the design of this project see the &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon/blob/master/docs/design/README.md&#34; target=&#34;_blank&#34;&gt;design document&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;h2 id=&#34;possible-use-case-scenarios-when-to-use-it&#34;&gt;Possible Use Case Scenarios – When To Use It&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34; target=&#34;_blank&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; enables extremely high throughput scheduling environment at scale due to its bulk scheduling approach superiority versus K8s pod-at-a-time approach. In our extensive tests, we have observed substantial throughput benefits as long as resource requirements (CPU/Memory) for incoming Pods is uniform across jobs (Replicasets/Deployments/Jobs), mainly due to efficient amortization of work across jobs.&lt;/p&gt;

&lt;p&gt;Although, &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/poseidon-firmament-alternate-scheduler/&#34; target=&#34;_blank&#34;&gt;Poseidon-Firmament scheduler&lt;/a&gt; is capable of scheduling various types of workloads (service, batch, etc.), following are the few use cases where it excels the most:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;For “Big Data/AI” jobs consisting of a large number of tasks, throughput benefits are tremendous.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Substantial throughput benefits also for service or batch job scenarios where workload resource requirements are uniform across jobs (Replicasets/Deplyments/Jobs).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;current-project-stage&#34;&gt;Current Project Stage&lt;/h2&gt;

&lt;p&gt;Currently Poseidon-Firmament project is an incubation project. Alpha Release is available at &lt;a href=&#34;https://github.com/kubernetes-sigs/poseidon&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-sigs/poseidon&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Update on Volume Snapshot Alpha for Kubernetes</title>
      <link>https://docstest.github.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/</link>
      <pubDate>Thu, 17 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/01/17/update-on-volume-snapshot-alpha-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Xu (Google), Xing Yang (Huawei), Saad Ali (Google)&lt;/p&gt;

&lt;p&gt;Volume snapshotting support was introduced in Kubernetes v1.12 as an alpha feature. In Kubernetes v1.13, it remains an alpha feature, but a few enhancements were added and some breaking changes were made. This post summarizes the changes.&lt;/p&gt;

&lt;h2 id=&#34;breaking-changes&#34;&gt;Breaking Changes&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v1.0.0&#34; target=&#34;_blank&#34;&gt;CSI spec v1.0&lt;/a&gt; introduced a few breaking changes to the volume snapshot feature. CSI driver maintainers should be aware of these changes as they upgrade their drivers to support v1.0.&lt;/p&gt;

&lt;h2 id=&#34;snapshotstatus-replaced-with-boolean-readytouse&#34;&gt;SnapshotStatus replaced with Boolean ReadyToUse&lt;/h2&gt;

&lt;p&gt;CSI v0.3.0, defined a &lt;code&gt;SnapshotStatus&lt;/code&gt; enum in &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; which indicates whether the snapshot is &lt;code&gt;READY&lt;/code&gt;, &lt;code&gt;UPLOADING&lt;/code&gt;, or &lt;code&gt;ERROR_UPLOADING&lt;/code&gt;. In CSI v1.0, &lt;code&gt;SnapshotStatus&lt;/code&gt; has been removed from &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; and replaced with a &lt;code&gt;boolean ReadyToUse&lt;/code&gt;. A &lt;code&gt;ReadyToUse&lt;/code&gt; value of &lt;code&gt;true&lt;/code&gt; indicates that post snapshot processing (such as uploading) is complete and the snapshot is ready to be used as a source to create a volume.&lt;/p&gt;

&lt;p&gt;Storage systems that need to do post snapshot processing (such as uploading after the snapshot is cut) should return a successful &lt;code&gt;CreateSnapshotResponse&lt;/code&gt; with the &lt;code&gt;ReadyToUse&lt;/code&gt; field set to &lt;code&gt;false&lt;/code&gt; as soon as the snapshot has been taken.  This indicates that the Container Orchestration System (CO) can resume any workload that was quiesced for the snapshot to be taken. The CO can then repeatedly call &lt;code&gt;CreateSnapshot&lt;/code&gt; until the &lt;code&gt;ReadyToUse&lt;/code&gt; field is set to &lt;code&gt;true&lt;/code&gt; or the call returns an error indicating a problem in processing. The CSI &lt;code&gt;ListSnapshot&lt;/code&gt; call could be used along with &lt;code&gt;snapshot_id&lt;/code&gt; filtering to determine if the snapshot is ready to use, but is not recommended because it provides no way to detect errors during processing (the &lt;code&gt;ReadyToUse&lt;/code&gt; field simply remains &lt;code&gt;false&lt;/code&gt; indefinitely).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1&#34; target=&#34;_blank&#34;&gt;v1.x.x releases&lt;/a&gt; of the CSI external-snapshotter sidecar container already handle this change by calling &lt;code&gt;CreateSnapshot&lt;/code&gt; instead of &lt;code&gt;ListSnapshots&lt;/code&gt; to check if a snapshot is ready to use. When upgrading their drivers to CSI 1.0, driver maintainers should use the appropriate 1.0 compatible sidecar container.&lt;/p&gt;

&lt;p&gt;To be consistent with the change in the CSI spec, the &lt;code&gt;Ready&lt;/code&gt; field in the &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object has been renamed to &lt;code&gt;ReadyToUse&lt;/code&gt;. This change is visible to the user when running &lt;code&gt;kubectl describe volumesnapshot&lt;/code&gt; to view the details of a snapshot.&lt;/p&gt;

&lt;h2 id=&#34;timestamp-data-type&#34;&gt;Timestamp Data Type&lt;/h2&gt;

&lt;p&gt;The creation time of a snapshot is available to Kubernetes admins as part of the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; API object. This field is populated using the &lt;code&gt;creation_time&lt;/code&gt; field in the CSI &lt;code&gt;CreateSnapshotResponse&lt;/code&gt;. In CSI v1.0, this &lt;code&gt;creation_time&lt;/code&gt; field type was changed to &lt;a href=&#34;https://godoc.org/github.com/golang/protobuf/ptypes/timestamp&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;.google.protobuf.Timestamp&lt;/code&gt;&lt;/a&gt; instead of &lt;code&gt;int64&lt;/code&gt;. When upgrading drivers to CSI 1.0, driver maintainers must make changes accordingly. The &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v1.0.1&#34; target=&#34;_blank&#34;&gt;v1.x.x releases&lt;/a&gt; of the CSI external-snapshotter sidecar container has been updated to handle this change.&lt;/p&gt;

&lt;h2 id=&#34;deprecations&#34;&gt;Deprecations&lt;/h2&gt;

&lt;p&gt;The following &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; parameters are deprecated and will be removed in a future release.  They will be replaced with parameters listed in the &lt;code&gt;Replacement&lt;/code&gt; section below.&lt;/p&gt;

&lt;p&gt;Deprecated
Replacement
csiSnapshotterSecretName
csi.storage.k8s.io/snapshotter-secret-name
csiSnapshotterSecretNameSpace
csi.storage.k8s.io/snapshotter-secret-namespace&lt;/p&gt;

&lt;h2 id=&#34;new-features&#34;&gt;New Features&lt;/h2&gt;

&lt;h3 id=&#34;snapshotcontent-deletion-retain-policy&#34;&gt;SnapshotContent Deletion/Retain Policy&lt;/h3&gt;

&lt;p&gt;As described in the &lt;a href=&#34;https://kubernetes.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/&#34; target=&#34;_blank&#34;&gt;initial blog post announcing the snapshot alpha&lt;/a&gt;, the Kubernetes snapshot APIs are similar to the PV/PVC APIs: just like a volume is represented by a bound PVC and PV pair, a snapshot is represented by a bound &lt;code&gt;VolumeSnapshot&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; pair.&lt;/p&gt;

&lt;p&gt;With PV/PVC pairs, when a user is done with a volume, they can delete the PVC. And the reclaim policy on the PV determines what happens to the PV (whether it is also deleted or retained).&lt;/p&gt;

&lt;p&gt;In the initial alpha release, snapshots did not support the ability to specify a reclaim policy. Instead when a snapshot object was deleted it always resulted in the snapshot being deleted.  In Kubernetes v1.13, a snapshot content &lt;code&gt;DeletionPolicy&lt;/code&gt; was added. It enables an admin to configure what what happens to a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; after the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object it is bound to is deleted. The &lt;code&gt;DeletionPolicy&lt;/code&gt; of a volume snapshot can either be &lt;code&gt;Retain&lt;/code&gt; or &lt;code&gt;Delete&lt;/code&gt;. If the value is not specified, the default depends on whether the &lt;code&gt;SnapshotContent&lt;/code&gt; object was created via static binding or dynamic provisioning.&lt;/p&gt;

&lt;h3 id=&#34;retain&#34;&gt;Retain&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;Retain&lt;/code&gt; policy allows for manual reclamation of the resource. If a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is statically created and bound, the default &lt;code&gt;DeletionPolicy&lt;/code&gt; is &lt;code&gt;Retain&lt;/code&gt;. When the &lt;code&gt;VolumeSnapshot&lt;/code&gt; is deleted, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; continues to exist and the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is considered “released”. But it is not available for binding to other &lt;code&gt;VolumeSnapshot&lt;/code&gt; objects because it contains data. It is up to an administrator to decide how to handle the remaining API object and resource cleanup.&lt;/p&gt;

&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;

&lt;p&gt;A &lt;code&gt;Delete&lt;/code&gt; policy enables automatic deletion of the bound &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object from Kubernetes and the associated storage asset in the external infrastructure (such as an AWS EBS snapshot or GCE PD snapshot, etc.). Snapshots that are dynamically provisioned inherit the deletion policy of their &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;VolumeSnapshotClass&lt;/code&gt;&lt;/a&gt;, which defaults to &lt;code&gt;Delete&lt;/code&gt;. The administrator should configure the &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; with the desired retention policy. The policy may be changed for individual &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; after it is created by patching the object.&lt;/p&gt;

&lt;p&gt;The following example demonstrates how to check the deletion policy of a dynamically provisioned &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f ./examples/kubernetes/demo-defaultsnapshotclass.yaml
$ kubectl create -f ./examples/kubernetes/demo-snapshot.yaml
$ kubectl get volumesnapshots demo-snapshot-podpvc -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  creationTimestamp: &amp;quot;2018-11-27T23:57:09Z&amp;quot;
...
spec:
  snapshotClassName: default-snapshot-class
  snapshotContentName: snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002
  source:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: podpvc
status:
…
$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
…
spec:
  csiVolumeSnapshotSource:
    creationTime: 1546469777852000000
    driver: pd.csi.storage.gke.io
    restoreSize: 6442450944
    snapshotHandle: projects/jing-k8s-dev/global/snapshots/snapshot-26cd0db3-f2a0-11e8-8be6-42010a800002
  deletionPolicy: Delete
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
    resourceVersion: &amp;quot;21117&amp;quot;
    uid: ae400e9f-f28b-11e8-8be6-42010a800002
  snapshotClassName: default-snapshot-class
  volumeSnapshotRef:
    apiVersion: snapshot.storage.k8s.io/v1alpha1
    kind: VolumeSnapshot
    name: demo-snapshot-podpvc
    namespace: default
    resourceVersion: &amp;quot;6948065&amp;quot;
    uid: 26cd0db3-f2a0-11e8-8be6-42010a800002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;User can change the deletion policy by using patch:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl patch volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;deletionPolicy&amp;quot;:&amp;quot;Retain&amp;quot;}}&#39; --type=merge

$ kubectl get volumesnapshotcontent snapcontent-26cd0db3-f2a0-11e8-8be6-42010a800002 -o yaml
apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
...
spec:
  csiVolumeSnapshotSource:
...
  deletionPolicy: Retain
  persistentVolumeRef:
    apiVersion: v1
    kind: PersistentVolume
    name: pvc-853622a4-f28b-11e8-8be6-42010a800002
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;snapshot-object-in-use-protection&#34;&gt;Snapshot Object in Use Protection&lt;/h2&gt;

&lt;p&gt;The purpose of the Snapshot Object in Use Protection feature is to ensure that in-use snapshot API objects are not removed from the system (as this may result in data loss). There are two cases that require “in-use” protection:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If a volume snapshot is in active use by a persistent volume claim as a source to create a volume.&lt;/li&gt;
&lt;li&gt;If a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; API object is bound to a VolumeSnapshot API object, the content object is considered in use.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If a user deletes a &lt;code&gt;VolumeSnapshot&lt;/code&gt; API object in active use by a PVC, the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is not removed immediately. Instead, removal of the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is postponed until the &lt;code&gt;VolumeSnapshot&lt;/code&gt; is no longer actively used by any PVCs. Similarly, if an admin deletes a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; that is bound to a &lt;code&gt;VolumeSnapshot&lt;/code&gt;, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is not removed immediately. Instead, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; removal is postponed until the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is not bound to the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-kubernetes-snapshots&#34;&gt;Which volume plugins support Kubernetes Snapshots?&lt;/h2&gt;

&lt;p&gt;Snapshots are only supported for CSI drivers (not for in-tree or Flexvolume). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.&lt;/p&gt;

&lt;p&gt;As of the publishing of this blog post, the following CSI drivers support snapshots:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34; target=&#34;_blank&#34;&gt;GCE Persistent Disk CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/opensds/nbp/tree/master/csi/server&#34; target=&#34;_blank&#34;&gt;OpenSDS CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ceph/ceph-csi/tree/master/pkg/rbd&#34; target=&#34;_blank&#34;&gt;Ceph RBD CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/libopenstorage/openstorage/tree/master/csi&#34; target=&#34;_blank&#34;&gt;Portworx CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gluster/gluster-csi-driver&#34; target=&#34;_blank&#34;&gt;GlusterFS CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/digitalocean/csi-digitalocean&#34; target=&#34;_blank&#34;&gt;Digital Ocean CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/embercsi/ember-csi&#34; target=&#34;_blank&#34;&gt;Ember CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder&#34; target=&#34;_blank&#34;&gt;Cinder CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Datera/datera-csi&#34; target=&#34;_blank&#34;&gt;Datera CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Nexenta/nexentastor-csi-driver&#34; target=&#34;_blank&#34;&gt;NexentaStor CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Snapshot support for other &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;drivers&lt;/a&gt; is pending, and should be available soon. Read the “Container Storage Interface (CSI) for Kubernetes GA” blog post to learn more about CSI and how to deploy CSI drivers.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to beta in either 1.15 or 1.16. Some of the features we are interested in supporting include consistency groups, application consistent snapshots, workload quiescing, in-place restores, and more.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;The code repository for snapshot APIs and controller is here: &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-csi/external-snapshotter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;http://k8s.io/docs/concepts/storage/volume-snapshots&#34; target=&#34;_blank&#34;&gt;http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors that helped add CSI v1.0 support and improve the snapshot feature in this release, including Saad Ali (&lt;a href=&#34;https://github.com/saadali&#34; target=&#34;_blank&#34;&gt;saadali&lt;/a&gt;), Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;), Deep Debroy (&lt;a href=&#34;https://github.com/ddebroy&#34; target=&#34;_blank&#34;&gt;ddebroy&lt;/a&gt;), James DeFelice (&lt;a href=&#34;https://github.com/jdef&#34; target=&#34;_blank&#34;&gt;jdef&lt;/a&gt;), John Griffith (&lt;a href=&#34;https://github.com/j-griffith&#34; target=&#34;_blank&#34;&gt;j-griffith&lt;/a&gt;), Julian Hjortshoj (&lt;a href=&#34;https://github.com/julian-hj&#34; target=&#34;_blank&#34;&gt;julian-hj&lt;/a&gt;), Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;), Patrick Ohly (&lt;a href=&#34;https://github.com/pohly&#34; target=&#34;_blank&#34;&gt;pohly&lt;/a&gt;), Luis Pabon (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;), Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;), Jing Xu (&lt;a href=&#34;https://github.com/jingxu97&#34; target=&#34;_blank&#34;&gt;jingxu97&lt;/a&gt;), Shiwei Xu (&lt;a href=&#34;https://github.com/wackxu&#34; target=&#34;_blank&#34;&gt;wackxu&lt;/a&gt;), Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;), Jie Yu (&lt;a href=&#34;https://github.com/jieyu&#34; target=&#34;_blank&#34;&gt;jieyu&lt;/a&gt;), David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;We also hold regular &lt;a href=&#34;https://docs.google.com/document/d/1qdfvAj5O-tTAZzqJyz3B-yczLLxOiQd-XKpJmTEMazs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;SIG-Storage Snapshot Working Group meetings&lt;/a&gt;. New attendees are welcome to join for design and development discussions.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Container Storage Interface (CSI) for Kubernetes GA</title>
      <link>https://docstest.github.io/blog/2019/01/15/container-storage-interface-ga/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/01/15/container-storage-interface-ga/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-kubernetes.png&#34; alt=&#34;Kubernetes Logo&#34; /&gt;
&lt;img src=&#34;https://docstest.github.io/images/blog-logging/2018-04-10-container-storage-interface-beta/csi-logo.png&#34; alt=&#34;CSI Logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; Saad Ali, Senior Software Engineer, Google&lt;/p&gt;

&lt;p&gt;The Kubernetes implementation of the &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;Container Storage Interface&lt;/a&gt; (CSI) has been promoted to GA in the Kubernetes v1.13 release. Support for CSI was &lt;a href=&#34;http://blog.kubernetes.io/2018/01/introducing-container-storage-interface.html&#34; target=&#34;_blank&#34;&gt;introduced as alpha&lt;/a&gt; in Kubernetes v1.9 release, and &lt;a href=&#34;https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/&#34; target=&#34;_blank&#34;&gt;promoted to beta&lt;/a&gt; in the Kubernetes v1.10 release.&lt;/p&gt;

&lt;p&gt;The GA milestone indicates that Kubernetes users may depend on the feature and its API without fear of backwards incompatible changes in future causing regressions. GA features are protected by the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34; target=&#34;_blank&#34;&gt;Kubernetes deprecation policy&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-csi&#34;&gt;Why CSI?&lt;/h2&gt;

&lt;p&gt;Although prior to CSI Kubernetes provided a powerful volume plugin system, it was challenging to add support for new volume plugins to Kubernetes: volume plugins were “in-tree” meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries—vendors wanting to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain.&lt;/p&gt;

&lt;p&gt;CSI was developed as a standard for exposing arbitrary block and file storage storage systems to containerized workloads on Container Orchestration Systems (COs) like Kubernetes. With the adoption of the Container Storage Interface, the Kubernetes volume layer becomes truly extensible. Using CSI, third-party storage providers can write and deploy plugins exposing new storage systems in Kubernetes without ever having to touch the core Kubernetes code. This gives Kubernetes users more options for storage and makes the system more secure and reliable.&lt;/p&gt;

&lt;h2 id=&#34;what-s-new&#34;&gt;What’s new?&lt;/h2&gt;

&lt;p&gt;With the promotion to GA, the Kubernetes implementation of CSI introduces the following changes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes is now compatible with CSI spec &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v1.0.0&#34; target=&#34;_blank&#34;&gt;v1.0&lt;/a&gt; and &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.3.0&#34; target=&#34;_blank&#34;&gt;v0.3&lt;/a&gt; (instead of CSI spec &lt;a href=&#34;https://github.com/container-storage-interface/spec/releases/tag/v0.2.0&#34; target=&#34;_blank&#34;&gt;v0.2&lt;/a&gt;).

&lt;ul&gt;
&lt;li&gt;There were breaking changes between CSI spec v0.3.0 and v1.0.0, but Kubernetes v1.13 supports both versions so either version will work with Kubernetes v1.13.&lt;/li&gt;
&lt;li&gt;Please note that with the release of the CSI 1.0 API, support for CSI drivers using 0.3 and older releases of the CSI API is deprecated, and is planned to be removed in Kubernetes v1.15.&lt;/li&gt;
&lt;li&gt;There were no breaking changes between CSI spec v0.2 and v0.3, so v0.2 drivers should also work with Kubernetes v1.10.0+.&lt;/li&gt;
&lt;li&gt;There were breaking changes between the CSI spec v0.1 and v0.2, so very old drivers implementing CSI 0.1 must be updated to be at least 0.2 compatible before use with Kubernetes v1.10.0+.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; object (introduced in v1.9 in the storage v1alpha1 group, and added to the v1beta1 group in v1.10) has been added to the storage v1 group in v1.13.&lt;/li&gt;
&lt;li&gt;The Kubernetes &lt;code&gt;CSIPersistentVolumeSource&lt;/code&gt; volume type has been promoted to GA.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration&#34; target=&#34;_blank&#34;&gt;Kubelet device plugin registration mechanism&lt;/a&gt;, which is the means by which kubelet discovers new CSI drivers, has been promoted to GA in Kubernetes v1.13.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-deploy-a-csi-driver&#34;&gt;How to deploy a CSI driver?&lt;/h2&gt;

&lt;p&gt;Kubernetes users interested in how to deploy or manage an existing CSI driver on Kubernetes should look at the documentation provided by the author of the CSI driver.&lt;/p&gt;

&lt;h2 id=&#34;how-to-use-a-csi-volume&#34;&gt;How to use a CSI volume?&lt;/h2&gt;

&lt;p&gt;Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects: &lt;code&gt;PersistentVolumeClaims&lt;/code&gt;, &lt;code&gt;PersistentVolumes&lt;/code&gt;, and &lt;code&gt;StorageClasses&lt;/code&gt;. Documented &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#csi&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although the Kubernetes implementation of CSI is a GA feature in Kubernetes v1.13, it may require the following flag:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;API server binary and kubelet binaries:

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--allow-privileged=true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Most CSI plugins will require bidirectional mount propagation, which can only be enabled for privileged pods. Privileged pods are only permitted on clusters where this flag has been set to true (this is the default in some environments like GCE, GKE, and kubeadm).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dynamic-provisioning&#34;&gt;Dynamic Provisioning&lt;/h3&gt;

&lt;p&gt;You can enable automatic creation/deletion of volumes for CSI Storage plugins that support dynamic provisioning by creating a &lt;code&gt;StorageClass&lt;/code&gt; pointing to the CSI plugin.&lt;/p&gt;

&lt;p&gt;The following StorageClass, for example, enables dynamic creation of “&lt;code&gt;fast-storage&lt;/code&gt;” volumes by a CSI volume plugin called “&lt;code&gt;csi-driver.example.com&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast-storage
provisioner: csi-driver.example.com
parameters:
  type: pd-ssd
  csi.storage.k8s.io/provisioner-secret-name: mysecret
  csi.storage.k8s.io/provisioner-secret-namespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;New for GA, the &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;CSI external-provisioner&lt;/a&gt; (v1.0.1+) reserves the parameter keys prefixed with &lt;code&gt;csi.storage.k8s.io/&lt;/code&gt;. If the keys do not correspond to a set of known keys the values are simply ignored (and not passed to the CSI driver). The older secret parameter keys (&lt;code&gt;csiProvisionerSecretName&lt;/code&gt;, &lt;code&gt;csiProvisionerSecretNamespace&lt;/code&gt;, etc.) are also supported by CSI external-provisioner v1.0.1 but are deprecated and may be removed in future releases of the CSI external-provisioner.&lt;/p&gt;

&lt;p&gt;Dynamic provisioning is triggered by the creation of a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object. The following &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, for example, triggers dynamic provisioning using the &lt;code&gt;StorageClass&lt;/code&gt; above.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-request-for-storage
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: fast-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When volume provisioning is invoked, the parameter type: &lt;code&gt;pd-ssd&lt;/code&gt; and the secret any referenced secret(s) are passed to the CSI plugin &lt;code&gt;csi-driver.example.com&lt;/code&gt; via a &lt;code&gt;CreateVolume&lt;/code&gt; call. In response, the external volume plugin provisions a new volume and then automatically create a &lt;code&gt;PersistentVolume&lt;/code&gt; object to represent the new volume. Kubernetes then binds the new &lt;code&gt;PersistentVolume&lt;/code&gt; object to the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, making it ready to use.&lt;/p&gt;

&lt;p&gt;If the &lt;code&gt;fast-storage  StorageClass&lt;/code&gt; is marked as “default”, there is no need to include the &lt;code&gt;storageClassName&lt;/code&gt; in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;, it will be used by default.&lt;/p&gt;

&lt;h3 id=&#34;pre-provisioned-volumes&#34;&gt;Pre-Provisioned Volumes&lt;/h3&gt;

&lt;p&gt;You can always expose a pre-existing volume in Kubernetes by manually creating a PersistentVolume object to represent the existing volume. The following &lt;code&gt;PersistentVolume&lt;/code&gt;, for example, exposes a volume with the name “&lt;code&gt;existingVolumeName&lt;/code&gt;” belonging to a CSI storage plugin called “&lt;code&gt;csi-driver.example.com&lt;/code&gt;”.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manually-created-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  csi:
    driver: csi-driver.example.com
    volumeHandle: existingVolumeName
    readOnly: false
    fsType: ext4
    volumeAttributes:
      foo: bar
    controllerPublishSecretRef:
      name: mysecret1
      namespace: mynamespace
    nodeStageSecretRef:
      name: mysecret2
      namespace: mynamespace
    nodePublishSecretRef
      name: mysecret3
      namespace: mynamespace
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;attaching-and-mounting&#34;&gt;Attaching and Mounting&lt;/h3&gt;

&lt;p&gt;You can reference a &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; that is bound to a CSI volume in any pod or pod template.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Pod
apiVersion: v1
metadata:
  name: my-pod
spec:
  containers:
    - name: my-frontend
      image: nginx
      volumeMounts:
      - mountPath: &amp;quot;/var/www/html&amp;quot;
        name: my-csi-volume
  volumes:
    - name: my-csi-volume
      persistentVolumeClaim:
        claimName: my-request-for-storage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the pod referencing a CSI volume is scheduled, Kubernetes will trigger the appropriate operations against the external CSI plugin (&lt;code&gt;ControllerPublishVolume&lt;/code&gt;, &lt;code&gt;NodeStageVolume&lt;/code&gt;, &lt;code&gt;NodePublishVolume&lt;/code&gt;, etc.) to ensure the specified volume is attached, mounted, and ready to use by the containers in the pod.&lt;/p&gt;

&lt;p&gt;For more details please see the CSI implementation &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#csi&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-to-write-a-csi-driver&#34;&gt;How to write a CSI Driver?&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://kubernetes-csi.github.io/&#34; target=&#34;_blank&#34;&gt;kubernetes-csi&lt;/a&gt; site details how to develop, deploy, and test a CSI driver on Kubernetes. In general, CSI Drivers should be deployed on Kubernetes along with the following sidecar (helper) containers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-attacher&#34; target=&#34;_blank&#34;&gt;external-attacher&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;VolumeAttachment&lt;/code&gt; objects and triggers &lt;code&gt;ControllerPublish&lt;/code&gt; and &lt;code&gt;ControllerUnpublish&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;external-provisioner&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; objects and triggers &lt;code&gt;CreateVolume&lt;/code&gt; and &lt;code&gt;DeleteVolume&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/node-driver-registrar&#34; target=&#34;_blank&#34;&gt;node-driver-registrar&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Registers the CSI driver with kubelet using the &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-registration&#34; target=&#34;_blank&#34;&gt;Kubelet device plugin mechanism&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/cluster-driver-registrar&#34; target=&#34;_blank&#34;&gt;cluster-driver-registrar&lt;/a&gt; (Alpha)

&lt;ul&gt;
&lt;li&gt;Registers a CSI Driver with the Kubernetes cluster by creating a &lt;code&gt;CSIDriver&lt;/code&gt; object which enables the driver to customize how Kubernetes interacts with it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;external-snapshotter&lt;/a&gt; (Alpha)

&lt;ul&gt;
&lt;li&gt;Watches Kubernetes &lt;code&gt;VolumeSnapshot&lt;/code&gt; CRD objects and triggers &lt;code&gt;CreateSnapshot&lt;/code&gt; and &lt;code&gt;DeleteSnapshot&lt;/code&gt; operations against a CSI endpoint.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-csi/livenessprobe&#34; target=&#34;_blank&#34;&gt;livenessprobe&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;May be included in a CSI plugin pod to enable the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34; target=&#34;_blank&#34;&gt;Kubernetes Liveness Probe&lt;/a&gt; mechanism.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Storage vendors can build Kubernetes deployments for their plugins using these components, while leaving their CSI driver completely unaware of Kubernetes.&lt;/p&gt;

&lt;h2 id=&#34;list-of-csi-drivers&#34;&gt;List of CSI Drivers&lt;/h2&gt;

&lt;p&gt;CSI drivers are developed and maintained by third parties. You can find a non-definitive list of CSI drivers &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-about-in-tree-volume-plugins&#34;&gt;What about in-tree volume plugins?&lt;/h2&gt;

&lt;p&gt;There is a plan to migrate most of the persistent, remote in-tree volume plugins to CSI. For more details see &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/csi-migration.md&#34; target=&#34;_blank&#34;&gt;design doc&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;limitations-of-ga&#34;&gt;Limitations of GA&lt;/h2&gt;

&lt;p&gt;The GA implementation of CSI has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ephemeral local volumes must create a PVC (pod inline referencing of CSI volumes is not supported).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Work on moving Kubernetes CSI features that are still alpha to beta:

&lt;ul&gt;
&lt;li&gt;Raw block volumes&lt;/li&gt;
&lt;li&gt;Topology awareness (the ability for Kubernetes to understand and influence where a CSI volume is provisioned (zone, regions, etc.).&lt;/li&gt;
&lt;li&gt;Features depending on CSI CRDs (e.g. “Skip attach” and “Pod info on mount”).&lt;/li&gt;
&lt;li&gt;Volume Snapshots&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Work on completing support for local ephemeral volumes.&lt;/li&gt;
&lt;li&gt;Work on migrating remote persistent in-tree volume plugins to CSI.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-to-get-involved&#34;&gt;How to get involved?&lt;/h2&gt;

&lt;p&gt;The Kubernetes Slack channel &lt;a href=&#34;https://kubernetes.slack.com/messages/C8EJ01Z46/details/&#34; target=&#34;_blank&#34;&gt;wg-csi&lt;/a&gt; and the Google group &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-storage-wg-csi&#34; target=&#34;_blank&#34;&gt;kubernetes-sig-storage-wg-csi&lt;/a&gt; along with any of the standard &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact&#34; target=&#34;_blank&#34;&gt;SIG storage communication channels&lt;/a&gt; are all great mediums to reach out to the SIG Storage team.&lt;/p&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the new contributors who stepped up this quarter to help the project reach GA:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Serguei Bezverkhi (&lt;a href=&#34;https://github.com/sbezverk&#34; target=&#34;_blank&#34;&gt;sbezverk&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Masaki Kimura (&lt;a href=&#34;https://github.com/mkimuram&#34; target=&#34;_blank&#34;&gt;mkimuram&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Patrick Ohly (&lt;a href=&#34;https://github.com/pohly&#34; target=&#34;_blank&#34;&gt;pohly&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Luis Pabón (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Vladimir Vivien (&lt;a href=&#34;https://github.com/vladimirvivien&#34; target=&#34;_blank&#34;&gt;vladimirvivien&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: APIServer dry-run and kubectl diff</title>
      <link>https://docstest.github.io/blog/2019/01/14/apiserver-dry-run-and-kubectl-diff/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2019/01/14/apiserver-dry-run-and-kubectl-diff/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Antoine Pelisse (Google Cloud, @apelisse)&lt;/p&gt;

&lt;p&gt;Declarative configuration management, also known as configuration-as-code, is
one of the key strengths of Kubernetes. It allows users to commit the desired state of
the cluster, and to keep track of the different versions, improve auditing and
automation through CI/CD pipelines. The &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-wg-apply&#34; target=&#34;_blank&#34;&gt;Apply working-group&lt;/a&gt;
is working on fixing some of the gaps, and is happy to announce that Kubernetes
1.13 promoted server-side dry-run and &lt;code&gt;kubectl diff&lt;/code&gt; to beta. These
two features are big improvements for the Kubernetes declarative model.&lt;/p&gt;

&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;

&lt;p&gt;A few pieces are still missing in order to have a seamless declarative
experience with Kubernetes, and we tried to address some of these:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;While compilers and linters do a good job to detect errors in pull-requests
for code, a good validation is missing for Kubernetes configuration files.
The existing solution is to run &lt;code&gt;kubectl apply --dry-run&lt;/code&gt;, but this runs a
&lt;em&gt;local&lt;/em&gt; dry-run that doesn&amp;rsquo;t talk to the server: it doesn&amp;rsquo;t have server
validation and doesn&amp;rsquo;t go through validating admission controllers. As an
example, Custom resource names are only validated on the server so a local
dry-run won&amp;rsquo;t help.&lt;/li&gt;
&lt;li&gt;It can be difficult to know how your object is going to be applied by the
server for multiple reasons:

&lt;ul&gt;
&lt;li&gt;Defaulting will set some fields to potentially unexpected values,&lt;/li&gt;
&lt;li&gt;Mutating webhooks might set fields or clobber/change some values.&lt;/li&gt;
&lt;li&gt;Patch and merges can have surprising effects and result in unexpected
objects. For example, it can be hard to know how lists are going to be
ordered once merged.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The working group has tried to address these problems.&lt;/p&gt;

&lt;h2 id=&#34;apiserver-dry-run&#34;&gt;APIServer dry-run&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-concepts/#dry-run&#34; target=&#34;_blank&#34;&gt;APIServer dry-run&lt;/a&gt; was implemented to address these two problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it allows individual requests to the apiserver to be marked as &amp;ldquo;dry-run&amp;rdquo;,&lt;/li&gt;
&lt;li&gt;the apiserver guarantees that dry-run requests won&amp;rsquo;t be persisted to storage,&lt;/li&gt;
&lt;li&gt;the request is still processed as typical request: the fields are
defaulted, the object is validated, it goes through the validation admission
chain, and through the mutating admission chain, and then the final object is
returned to the user as it normally would, without being persisted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While dynamic admission controllers are not supposed to have side-effects on
each request, dry-run requests are only processed if all admission controllers
explicitly announce that they don&amp;rsquo;t have any dry-run side-effects.&lt;/p&gt;

&lt;h3 id=&#34;how-to-enable-it&#34;&gt;How to enable it&lt;/h3&gt;

&lt;p&gt;Server-side dry-run is enabled through a feature-gate. Now that the feature is
Beta in 1.13, it should be enabled by default, but still can be enabled/disabled
using &lt;code&gt;kube-apiserver --feature-gates DryRun=true&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have dynamic admission controllers, you might have to fix them to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Remove any side-effects when the dry-run parameter is specified on the webhook request,&lt;/li&gt;
&lt;li&gt;Specify in the &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#webhook-v1beta1-admissionregistration&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;sideEffects&lt;/code&gt;&lt;/a&gt;
field of the &lt;code&gt;admissionregistration.k8s.io/v1beta1.Webhook&lt;/code&gt; object to indicate that the object doesn&amp;rsquo;t
have side-effects on dry-run (or at all).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-to-use-it&#34;&gt;How to use it&lt;/h3&gt;

&lt;p&gt;You can trigger the feature from kubectl by using &lt;code&gt;kubectl apply
--server-dry-run&lt;/code&gt;, which will decorate the request with the dryRun flag
and return the object as it would have been applied, or an error if it would
have failed.&lt;/p&gt;

&lt;h2 id=&#34;kubectl-diff&#34;&gt;Kubectl diff&lt;/h2&gt;

&lt;p&gt;APIServer dry-run is convenient because it lets you see how the object would be
processed, but it can be hard to identify exactly what changed if the object is
big. &lt;code&gt;kubectl diff&lt;/code&gt; does exactly what you want by showing the differences between
the current &amp;ldquo;live&amp;rdquo; object and the new &amp;ldquo;dry-run&amp;rdquo; object. It makes it very
convenient to focus on only the changes that are made to the object, how the
server has merged these and how the mutating webhooks affects the output.&lt;/p&gt;

&lt;h3 id=&#34;how-to-use-it-1&#34;&gt;How to use it&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;kubectl diff&lt;/code&gt; is meant to be as similar as possible to &lt;code&gt;kubectl apply&lt;/code&gt;:
&lt;code&gt;kubectl diff -f some-resources.yaml&lt;/code&gt; will show a diff for the resources in the yaml file. One can even use the diff program of their choice by using the KUBECTL_EXTERNAL_DIFF environment variable, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;KUBECTL_EXTERNAL_DIFF=meld kubectl diff -f some-resources.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;The working group is still busy trying to improve some of these things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Server-side apply is trying to improve the apply scenario, by adding owner
semantics to fields! It&amp;rsquo;s also going to improve support for CRDs and unions!&lt;/li&gt;
&lt;li&gt;Some kubectl apply features are missing from diff and could be useful, like the ability
to filter by label, or to display pruned resources.&lt;/li&gt;
&lt;li&gt;Eventually, kubectl diff will use server-side apply!&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Federation Evolution</title>
      <link>https://docstest.github.io/blog/2018/12/12/kubernetes-federation-evolution/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/12/12/kubernetes-federation-evolution/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Irfan Ur Rehman (Huawei), Paul Morie (RedHat) and Shashidhara T D (Huawei)&lt;/p&gt;

&lt;p&gt;Kubernetes provides great primitives for deploying applications to a cluster: it can be as simple as &lt;code&gt;kubectl create -f app.yaml&lt;/code&gt;. Deploy apps across multiple clusters has never been that simple. How should app workloads be distributed? Should the app resources be replicated into all clusters, replicated into selected clusters, or partitioned into clusters? How is  access to the clusters managed? What happens if some of the resources that a user wants to distribute pre-exist, in some or all of the clusters, in some form?&lt;/p&gt;

&lt;p&gt;In SIG Multicluster, our journey has revealed that there are multiple possible models to solve these problems and there probably is no single best-fit, all-scenario solution. &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/federation/&#34; target=&#34;_blank&#34;&gt;Federation&lt;/a&gt;, however, is the single biggest Kubernetes open source sub-project, and has seen the maximum interest and contribution from the community in this problem space. The project initially reused the Kubernetes API to do away with any added usage complexity for an existing Kubernetes user. This approach was not viable, because of the problems summarised below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Difficulties in re-implementing the Kubernetes API at the cluster level, as federation-specific extensions were stored in annotations.&lt;/li&gt;
&lt;li&gt;Limited flexibility in federated types, placement and reconciliation, due to 1:1 emulation of the Kubernetes API.&lt;/li&gt;
&lt;li&gt;No settled path to GA, and general confusion on API maturity; for example, Deployments are GA in Kubernetes but not even Beta in Federation v1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ideas have evolved further with a federation-specific API architecture and a community effort which now continues as Federation v2.&lt;/p&gt;

&lt;h1 id=&#34;conceptual-overview&#34;&gt;Conceptual Overview&lt;/h1&gt;

&lt;p&gt;Because Federation attempts to address a complex set of problems, it pays to break the different parts of those problems down. Let’s take a look at the different high-level areas involved:
&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2018-12-11-Kubernetes-Federation-Evolution/concepts.png&#34;
         alt=&#34;Kubernetes Federation v2 Concepts&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Kubernetes Federation v2 Concepts&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;federating-arbitrary-resources&#34;&gt;Federating arbitrary resources&lt;/h2&gt;

&lt;p&gt;One of the main goals of Federation is to be able to define the APIs and API groups which encompass basic tenets needed to federate any given Kubernetes resource. This is crucial, due to the popularity of CustomResourceDefinitions as a way to extend Kubernetes with new APIs.&lt;/p&gt;

&lt;p&gt;The workgroup arrived at a common definition of the federation API and API groups as &lt;em&gt;&amp;lsquo;a mechanism that distributes “normal” Kubernetes API resources into different clusters&amp;rsquo;&lt;/em&gt;. The distribution in its most simple form could be imagined as &lt;strong&gt;&lt;em&gt;simple propagation&lt;/em&gt;&lt;/strong&gt; of this &lt;em&gt;&amp;lsquo;normal Kubernetes API resource&amp;rsquo;&lt;/em&gt; across the federated clusters. A thoughtful reader can certainly discern more complicated mechanisms, other than this simple propagation of the Kubernetes resources.&lt;/p&gt;

&lt;p&gt;During the journey of defining building blocks of the federation APIs, one of the near term goals also evolved as &lt;em&gt;&amp;lsquo;to be able to create a simple federation a.k.a. simple propagation of any Kubernetes resource or a CRD, writing almost zero code&amp;rsquo;&lt;/em&gt;. What ensued further was a core API group defining the building blocks as a &lt;code&gt;Template&lt;/code&gt; resource, a &lt;code&gt;Placement&lt;/code&gt; resource and an &lt;code&gt;Override&lt;/code&gt; resource per given Kubernetes resource, a &lt;code&gt;TypeConfig&lt;/code&gt; to specify sync or no sync for the given resource and associated controller(s) to carry out the sync. More details follow &lt;a href=&#34;#federating-resources-the-details&#34;&gt;in the next section&lt;/a&gt;. Further sections will also talk about being able to follow a layered behaviour with higher-level federation APIs consuming the behaviour of these core building blocks, and users being able to consume whole or part of the API and associated controllers. Lastly, this architecture also allows the users to write additional controllers or replace the available reference controllers with their own, to carry out desired behaviour.&lt;/p&gt;

&lt;p&gt;The ability to &lt;em&gt;&amp;lsquo;easily federate arbitrary Kubernetes resources&amp;rsquo;&lt;/em&gt;, and a decoupled API, divided into building blocks APIs, higher level APIs and possible user intended types, presented such that different users can consume parts and write controllers composing solutions specific to them, makes a compelling case for Federation v2.&lt;/p&gt;

&lt;h2 id=&#34;federating-resources-the-details&#34;&gt;Federating resources: the details&lt;/h2&gt;

&lt;p&gt;Fundamentally, federation must be configured with two types of information:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which API types federation should handle&lt;/li&gt;
&lt;li&gt;Which clusters federation should target for distributing those resources.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each API type that federation handles, different parts of the declared state live in different API resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;Template&lt;/code&gt; type holds the base specification of the resource - for example, a type called &lt;code&gt;FederatedReplicaSet&lt;/code&gt; holds the base specification of a &lt;code&gt;ReplicaSet&lt;/code&gt; that should be distributed to the targeted clusters&lt;/li&gt;
&lt;li&gt;A &lt;code&gt;Placement&lt;/code&gt; type holds the specification of the clusters the resource should be distributed to - for example, a type called &lt;code&gt;FederatedReplicaSetPlacement&lt;/code&gt; holds information about which clusters &lt;code&gt;FederatedReplicaSets&lt;/code&gt; should be distributed to&lt;/li&gt;
&lt;li&gt;An optional &lt;code&gt;Overrides&lt;/code&gt; type holds the specification of how the &lt;code&gt;Template&lt;/code&gt; resource should be varied in some clusters - for example, a type called &lt;code&gt;FederatedReplicaSetOverrides&lt;/code&gt; holds information about how a &lt;code&gt;FederatedReplicaSet&lt;/code&gt; should be varied in certain clusters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These types are all associated by name - meaning that for a particular Template resource with name &lt;code&gt;foo&lt;/code&gt;, the Placement and Override information for that resource are contained by the Override and Placement resources with the name &lt;code&gt;foo&lt;/code&gt; and in the same namespace as the Template.&lt;/p&gt;

&lt;h2 id=&#34;higher-level-behaviour&#34;&gt;Higher-level behaviour&lt;/h2&gt;

&lt;p&gt;The architecture of the v2 API allows higher-level APIs to be constructed using the mechanics provided by the core API types (&lt;code&gt;Template&lt;/code&gt;, &lt;code&gt;Placement&lt;/code&gt; and &lt;code&gt;Override&lt;/code&gt;), and associated controllers, for a given resource. In the community we uncovered a few use cases and implemented the higher-level APIs and associated controllers useful for those cases. Some of these types described in further sections also provide an useful reference to anybody interested in solving more complex use cases, building on top of the mechanics already available with the v2 API.&lt;/p&gt;

&lt;h3 id=&#34;replicaschedulingpreference&#34;&gt;ReplicaSchedulingPreference&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;ReplicaSchedulingPreference&lt;/code&gt; provides an automated mechanism of distributing and maintaining total number of replicas for Deployment or ReplicaSet-based federated workloads into federated clusters. This is based on high-level user preferences given by the user. These preferences include the semantics of &lt;em&gt;weighted distribution&lt;/em&gt; and &lt;em&gt;limits&lt;/em&gt; (min and max) for distributing the replicas. These also include semantics to allow redistribution of replicas dynamically in case some replica Pods remain unscheduled in some clusters, for example due to insufficient resources in that cluster.
More details can be found at the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#replicaschedulingpreference&#34; target=&#34;_blank&#34;&gt;user guide for ReplicaSchedulingPreferences&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;federated-services-cross-cluster-service-discovery&#34;&gt;Federated services &amp;amp; cross-cluster service discovery&lt;/h3&gt;

&lt;p&gt;Kubernetes Services are very useful in constructing a microservices architecture. There is a clear desire to deploy services across cluster, zone, region and cloud boundaries. Services that span clusters provide geographic distribution, enable hybrid and multi-cloud scenarios and improve the level of high availability beyond single cluster deployments. Customers who want their services to span one or more (possibly remote) clusters, need them to be reachable in a consistent manner from both within and outside their clusters.&lt;/p&gt;

&lt;p&gt;Federated &lt;code&gt;Service&lt;/code&gt;, at its core, contains a &lt;code&gt;Template&lt;/code&gt; (a definition of a Kubernetes Service), a &lt;code&gt;Placement&lt;/code&gt; (which clusters to be deployed into), an &lt;code&gt;Override&lt;/code&gt; (optional variation in particular clusters) and a &lt;code&gt;ServiceDNSRecord&lt;/code&gt; (specifying details on how to discover it).&lt;/p&gt;

&lt;p&gt;Note: The federated service has to be of type &lt;code&gt;LoadBalancer&lt;/code&gt; in order for it to be discoverable across clusters.&lt;/p&gt;

&lt;h4 id=&#34;discovering-a-federated-service-from-pods-inside-your-federated-clusters&#34;&gt;Discovering a federated service from Pods inside your federated clusters&lt;/h4&gt;

&lt;p&gt;By default, Kubernetes clusters come preconfigured with a cluster-local DNS server, as well as an intelligently constructed DNS search path, which together ensure that DNS queries like &lt;code&gt;myservice&lt;/code&gt;, &lt;code&gt;myservice.mynamespace&lt;/code&gt;, or &lt;code&gt;some-other-service.other-namespace&lt;/code&gt;, issued by software running inside Pods, are automatically expanded and resolved correctly to the appropriate IP of Services running in the local cluster.&lt;/p&gt;

&lt;p&gt;With the introduction of federated services and cross-cluster service discovery, this concept is extended to cover Kubernetes Services running in any other cluster across your cluster federation, globally. To take advantage of this extended range, you use a slightly different DNS name (e.g. &lt;code&gt;myservice.mynamespace.myfederation&lt;/code&gt;) to resolve federated services. Using a different DNS name also avoids having your existing applications accidentally traversing cross-zone or cross-region networks and you incurring perhaps unwanted network charges or latency, without you explicitly opting in to this behavior.&lt;/p&gt;

&lt;p&gt;Lets consider an example, using a service named &lt;code&gt;nginx&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A Pod in a cluster in the &lt;code&gt;us-central1-a&lt;/code&gt; availability zone needs to contact our &lt;code&gt;nginx&lt;/code&gt; service. Rather than use the service’s traditional cluster-local DNS name (&lt;code&gt;nginx.mynamespace&lt;/code&gt;, which is automatically expanded to &lt;code&gt;nginx.mynamespace.svc.cluster.local&lt;/code&gt;) it can now use the service’s federated DNS name, which is &lt;code&gt;nginx.mynamespace.myfederation&lt;/code&gt;. This will be automatically expanded and resolved to the closest healthy shard of my &lt;code&gt;nginx&lt;/code&gt; service, wherever in the world that may be. If a healthy shard exists in the local cluster, that service’s cluster-local IP address will be returned (by the cluster-local DNS). This is exactly equivalent to non-federated service resolution.&lt;/p&gt;

&lt;p&gt;If the Service does not exist in the local cluster (or it exists but has no healthy backend pods), the DNS query is automatically expanded to &lt;code&gt;nginx.mynamespace.myfederation.svc.us-central1-a.us-central1.example.com&lt;/code&gt;. Behind the scenes, this finds the external IP of one of the shards closest to my availability zone. This expansion is performed automatically by the cluster-local DNS server, which returns the associated CNAME record. This results in a traversal of the hierarchy of DNS records, and ends up at one of the external IP’s of the federated service nearby.&lt;/p&gt;

&lt;p&gt;It is also possible to target service shards in availability zones and regions other than the ones local to a Pod by specifying the appropriate DNS names explicitly, and not relying on automatic DNS expansion. For example, &lt;code&gt;nginx.mynamespace.myfederation.svc.europe-west1.example.com&lt;/code&gt;will resolve to all of the currently healthy service shards in Europe, even if the Pod issuing the lookup is located in the U.S., and irrespective of whether or not there are healthy shards of the service in the U.S. This is useful for remote monitoring and other similar applications.&lt;/p&gt;

&lt;h4 id=&#34;discovering-a-federated-service-from-other-clients-outside-your-federated-clusters&#34;&gt;Discovering a federated service from other clients outside your federated clusters&lt;/h4&gt;

&lt;p&gt;For external clients, automatic DNS expansion described is not currently possible. External clients need to specify one of the fully qualified DNS names of the federated service, be that a zonal, regional or global name. For convenience reasons, it is often a good idea to manually configure additional static CNAME records in your service, for example:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SHORT NAME&lt;/th&gt;
&lt;th&gt;CNAME&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;eu.nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.europe-west1.example.com&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;us.nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.us-central1.example.com&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;nginx.acme.com&lt;/td&gt;
&lt;td&gt;nginx.mynamespace.myfederation.svc.example.com&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;That way, your clients can always use the short form on the left, and always be automatically routed to the closest healthy shard on their home continent. All of the required failover is handled for you automatically by Kubernetes cluster federation.&lt;/p&gt;

&lt;p&gt;As further reading, a more elaborate example for users is available in the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/servicedns-with-externaldns.md&#34; target=&#34;_blank&#34;&gt;Multi-Cluster Service DNS with ExternalDNS guide&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;try-it-yourself&#34;&gt;Try it yourself&lt;/h1&gt;

&lt;p&gt;To get started with Federation v2, please refer to the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md&#34; target=&#34;_blank&#34;&gt;user guide&lt;/a&gt;. Deployment can be accomplished with a &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/charts/federation-v2/README.md&#34; target=&#34;_blank&#34;&gt;Helm chart&lt;/a&gt;, and once the control plane is available, the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#example&#34; target=&#34;_blank&#34;&gt;user guide’s example&lt;/a&gt; can be used to get some hands-on experience with using Federation V2.&lt;/p&gt;

&lt;p&gt;Federation v2 can be deployed in both &lt;em&gt;cluster-scoped&lt;/em&gt; and &lt;em&gt;namespace-scoped&lt;/em&gt; configurations.  A cluster-scoped deployment will require cluster-admin privileges to both host and member clusters, and may be a good fit for evaluating federation on clusters that are not running critical workloads. Namespace-scoped deployment requires access to only a single namespace on host and member clusters, and is a better fit for evaluating federation on clusters running workloads.  Most of the user guide refers to cluster-scoped deployment, with the &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md#namespaced-federation&#34; target=&#34;_blank&#34;&gt;namespaced federation&lt;/a&gt; section documenting how use of a namespaced deployment differs.  The same cluster can host multiple federations, and clusters can be part of multiple federations when using namespaced federation.&lt;/p&gt;

&lt;h1 id=&#34;next-steps&#34;&gt;Next Steps&lt;/h1&gt;

&lt;p&gt;As we noted in the beginning of this post, the multicluster problem space is extremely broad. It can be difficult to know exactly how to handle such broad problem spaces without concrete pieces of software to frame those conversations around. Our hope in the Federation working group is that Federation v2 can be a concrete artifact to frame discussions around. We would love to know experiences that folks have had in this problem space, how they feel about Federation v2, and what use-cases they’re interested in exploring in the future.&lt;/p&gt;

&lt;p&gt;Please feel welcome to join us at the &lt;a href=&#34;https://kubernetes.slack.com/messages/C09R1PJR3&#34; target=&#34;_blank&#34;&gt;sig-multicluster slack channel&lt;/a&gt; or at &lt;a href=&#34;https://docs.google.com/document/d/1FQx0BPlkkl1Bn0c9ocVBxYIKojpmrS1CFP5h0DI68AE/edit&#34; target=&#34;_blank&#34;&gt;Federation working group meetings&lt;/a&gt; on Wednesdays at 07:30 PST.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: etcd: Current status and future roadmap</title>
      <link>https://docstest.github.io/blog/2018/12/11/etcd-current-status-and-future-roadmap/</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/12/11/etcd-current-status-and-future-roadmap/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Gyuho Lee (Amazon Container OSS Team, @gyuho), Joe Betz (Google Cloud, @jpbetz)&lt;/p&gt;

&lt;p&gt;etcd is a distributed key value store that provides a reliable way to manage the coordination state of distributed systems. etcd was first announced in June 2013 by CoreOS (part of Red Hat as of 2018). Since its adoption in Kubernetes in 2014, etcd has become a fundamental part of the Kubernetes cluster management software design, and the etcd community has grown exponentially. etcd is now being used in production environments of multiple companies, including large cloud provider environments such as AWS, Google Cloud Platform, Azure, and other on-premises Kubernetes implementations. CNCF currently has &lt;a href=&#34;https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/&#34; target=&#34;_blank&#34;&gt;32 conformant Kubernetes platforms and distributions&lt;/a&gt;, all of which use etcd as the datastore.&lt;/p&gt;

&lt;p&gt;In this blog post, we’ll review some of the milestones achieved in latest etcd releases, and go over the future roadmap for etcd. Share your thoughts and feedback on features you consider important on the mailing list: etcd-dev@googlegroups.com.&lt;/p&gt;

&lt;h2 id=&#34;etcd-2013&#34;&gt;etcd, 2013&lt;/h2&gt;

&lt;p&gt;In June 2014, Kubernetes was released with etcd as a backing storage for all master states. Kubernetes v0.4 used etcd v0.2 API, which was in an alpha stage at the time. As Kubernetes reached the v1.0 milestone in 2015, etcd stabilized its v2.0 API. The widespread adoption of Kubernetes led to a dramatic increase in the scalability requirements for etcd. To handle large number of workloads and the growing requirements on scale, etcd released v3.0 API in June 2016. Kubernetes v1.13 finally &lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/622&#34; target=&#34;_blank&#34;&gt;dropped support for etcd v2.0 API&lt;/a&gt; and adopted the etcd v3.0 API. The table below gives a visual snapshot of the release cycles of etcd and Kubernetes.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;etcd&lt;/th&gt;
&lt;th&gt;Kubernetes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Initial Commit&lt;/td&gt;
&lt;td&gt;June 2, 2013&lt;/td&gt;
&lt;td&gt;June 1, 2014&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;First Stable Release&lt;/td&gt;
&lt;td&gt;January 28, 2015 (v2.0.0)&lt;/td&gt;
&lt;td&gt;July 13, 2015 (v1.0.0)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Latest Release&lt;/td&gt;
&lt;td&gt;October 10, 2018 (v3.3.10)&lt;/td&gt;
&lt;td&gt;December 3, 2018 (v1.13.0)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;etcd-v3-1-early-2017&#34;&gt;etcd v3.1, early 2017&lt;/h2&gt;

&lt;p&gt;etcd v3.1 features provide better read performance and better availability during version upgrades. Given the high use of etcd in production even to this day, these features were very useful for users. It implements Raft read index, which bypasses &lt;a href=&#34;https://godoc.org/github.com/etcd-io/etcd/wal&#34; target=&#34;_blank&#34;&gt;Raft WAL&lt;/a&gt; disk writes for linearizable reads. The follower requests read index from the leader. Responses from the leader indicate whether a follower has advanced as much as the leader. When the follower&amp;rsquo;s logs are up-to-date, quorum read is served locally without going through the full Raft protocol. Thus, no disk write is required for read requests. etcd v3.1 introduces automatic leadership transfer. When etcd leader receives an interrupt signal, it automatically transfers its leadership to a follower. This provides higher availability when the cluster adds or loses a member.&lt;/p&gt;

&lt;h2 id=&#34;etcd-v3-2-summer-2017&#34;&gt;etcd v3.2 (summer 2017)&lt;/h2&gt;

&lt;p&gt;etcd v3.2 focuses on stability. Its client was shipped in Kubernetes v1.10, v1.11, and v1.12. The etcd team still actively maintains the branch by backporting all the bug fixes. This release introduces gRPC proxy to support, watch, and coalesce all watch event broadcasts into one gRPC stream. These event broadcasts can go up to one million events per second.&lt;/p&gt;

&lt;p&gt;etcd v3.2 also introduces changes such as &lt;code&gt;“snapshot-count”&lt;/code&gt; default value from 10,000 to 100,000. With higher snapshot count, etcd server holds Raft entries in-memory for longer periods before compacting the old ones. etcd v3.2 default configuration shows higher memory usage, while giving more time for slow followers to catch up. It is a trade-off between less frequent snapshot sends and higher memory usage. Users can employ lower &lt;code&gt;etcd --snapshot-count&lt;/code&gt; value to reduce the memory usage or higher &lt;code&gt;“snapshot-count”&lt;/code&gt; value to increase the availability of slow followers.&lt;/p&gt;

&lt;p&gt;Another new feature backported to etcd v3.2.19 was &lt;code&gt;etcd --initial-election-tick-advance&lt;/code&gt; flag. By default, a rejoining follower fast-forwards election ticks to speed up its initial cluster bootstrap. For example, the starting follower node only waits 200ms instead of full election timeout 1-second before starting an election. Ideally, within the 200ms, it receives a leader heartbeat and immediately joins the cluster as a follower. However, if network partition happens, heartbeat may drop and thus leadership election will be triggered. A vote request from a partitioned node is quite disruptive. If it contains a higher Raft term, current leader is forced to step down. With “initial-election-tick-advance” set to false, a rejoining node has &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9591&#34; target=&#34;_blank&#34;&gt;more chance to receive leader heartbeats&lt;/a&gt; before disrupting the cluster.&lt;/p&gt;

&lt;h2 id=&#34;etcd-v3-3-early-2018&#34;&gt;etcd v3.3 (early 2018)&lt;/h2&gt;

&lt;p&gt;etcd v3.3 continues the theme of stability. Its client is included in &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/69322&#34; target=&#34;_blank&#34;&gt;Kubernetes v1.13&lt;/a&gt;. Previously, etcd client carelessly retried on network disconnects without any backoff or failover logic. The client was often stuck with a partitioned node, &lt;a href=&#34;https://github.com/etcd-io/etcd/issues/7321&#34; target=&#34;_blank&#34;&gt;affecting several production users&lt;/a&gt;. v3.3 client balancer now maintains a list of unhealthy endpoints using gRPC health checking protocol, making more efficient retries and failover in the face of transient disconnects and &lt;a href=&#34;https://github.com/etcd-io/etcd/issues/8711&#34; target=&#34;_blank&#34;&gt;network partitions&lt;/a&gt;. This was backported to etcd v3.2 and also &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/57480&#34; target=&#34;_blank&#34;&gt;included in Kubernetes v1.10 API server&lt;/a&gt;. etcd v3.3 also provides more predictable database size. etcd used to maintain a separate freelist DB to track pages that were no longer in use and freed after transactions, so that following transactions can reuse them. However, it turns out persisting freelist demands high disk space and introduces high latency for Kubernetes workloads. Especially when there were frequent snapshots with lots of read transactions, etcd database size quickly grew from 16 MB to 4 GB. etcd v3.3 disables freelist sync and rebuilds the freelist on restart. The overhead is so small that it is unnoticeable to most users. See &lt;a href=&#34;https://github.com/etcd-io/etcd/issues/8009&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;database space exceeded&amp;rdquo; issue&lt;/a&gt; for more information on this.&lt;/p&gt;

&lt;h2 id=&#34;etcd-v3-4-and-beyond&#34;&gt;etcd v3.4 and beyond&lt;/h2&gt;

&lt;p&gt;etcd v3.4 focuses on improving the operational experience. It adds &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9352&#34; target=&#34;_blank&#34;&gt;Raft pre-vote feature&lt;/a&gt; to improve the robustness of leadership election. When a node becomes isolated (e.g. network partition), this member will start an election requesting votes with increased Raft terms. When a leader receives a vote request with a higher term, it steps down to a follower. With pre-vote, Raft runs an additional election phase to check if the candidate can get enough votes to win an election. The isolated follower&amp;rsquo;s vote request is rejected because it does not contain the latest log entries.&lt;/p&gt;

&lt;p&gt;etcd v3.4 adds a &lt;a href=&#34;https://etcd.readthedocs.io/en/latest/server-learner.html#server-learner&#34; target=&#34;_blank&#34;&gt;Raft learner&lt;/a&gt; that joins the cluster as a non-voting member that still receives all the updates from leader. Adding a learner node does not increase the size of quorum and hence improves the cluster availability during membership reconfiguration. It only serves as a standby node until it gets promoted to a voting member. Moreover, to handle unexpected upgrade failures, v3.4 introduces &lt;a href=&#34;https://groups.google.com/forum/?hl=en#!topic/etcd-dev/Hq6zru44L74&#34; target=&#34;_blank&#34;&gt;etcd downgrade&lt;/a&gt; feature.&lt;/p&gt;

&lt;p&gt;etcd v3 storage uses multi-version concurrency control model to preserve key updates as event history. Kubernetes runs compaction to discard the event history that is no longer needed, and reclaims the storage space. etcd v3.4 will improve this storage compact operation, boost backend &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/9384&#34; target=&#34;_blank&#34;&gt;concurrency for large read transactions&lt;/a&gt;, and &lt;a href=&#34;https://github.com/etcd-io/etcd/pull/10283&#34; target=&#34;_blank&#34;&gt;optimize storage commit interval&lt;/a&gt; for Kubernetes use-case.&lt;/p&gt;

&lt;p&gt;To further improve etcd client load balancer, the v3.4 balancer was rewritten to leverage the newly introduced gRPC load balancing API. By leveraging gPRC, the etcd client load balancer codebase was substantially simplified while retaining feature parity with the v3.3 implementation and improving overall load balancing by round-robining requests across healthy endpoints. See &lt;a href=&#34;https://etcd.readthedocs.io/en/latest/client-architecture.html#client-architecture&#34; target=&#34;_blank&#34;&gt;Client Architecture&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Additionally, etcd maintainers will continue to make improvements to Kubernetes test frameworks: kubemark integration for scalability tests, Kubernetes API server conformance tests with etcd to provide release recommends and version skew policy, specifying conformance testing requirements for each cloud provider, etc.&lt;/p&gt;

&lt;h2 id=&#34;etcd-joins-cncf&#34;&gt;etcd Joins CNCF&lt;/h2&gt;

&lt;p&gt;etcd now has a new home at &lt;a href=&#34;https://github.com/etcd-io&#34; target=&#34;_blank&#34;&gt;etcd-io&lt;/a&gt; and &lt;a href=&#34;https://www.cncf.io/blog/2018/12/11/cncf-to-host-etcd/&#34; target=&#34;_blank&#34;&gt;joined CNCF as an incubating project&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The synergistic efforts with Kubernetes have driven the evolution of etcd. Without community feedback and contribution, etcd could not have achieved its maturity and reliability. We’re looking forward to continuing the growth of etcd as an open source project and are excited to work with the Kubernetes and the wider CNCF community.&lt;/p&gt;

&lt;p&gt;Finally, we’d like to thank all contributors with special thanks to &lt;a href=&#34;https://github.com/xiang90&#34; target=&#34;_blank&#34;&gt;Xiang Li&lt;/a&gt; for his leadership in etcd and Kubernetes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: New Contributor Workshop Shanghai</title>
      <link>https://docstest.github.io/blog/2018/12/05/new-contributor-workshop-shanghai/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/12/05/new-contributor-workshop-shanghai/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png&#34;
         alt=&#34;Kubecon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Kubecon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.&lt;/p&gt;

&lt;p&gt;This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png&#34;
         alt=&#34;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have &amp;ldquo;guest speakers&amp;rdquo; from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.&lt;/p&gt;

&lt;p&gt;Those hands-on exercises use a repository known as &lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;the contributor playground&lt;/a&gt;, created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png&#34;
         alt=&#34;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;Both the &amp;ldquo;Great Firewall&amp;rdquo; and the language barrier prevent contributing Kubernetes from China from being straightforward. What&amp;rsquo;s more, because open source business models are not mature in China, the time for employees work on open source projects is limited.&lt;/p&gt;

&lt;p&gt;Chinese engineers are eager to participate in the development of Kubernetes, but many of them don&amp;rsquo;t know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;I have been participating in the Kubernetes community for about three years,&amp;rdquo; said XiangPeng Zhao. &amp;ldquo;In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it&amp;rsquo;s not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago.&amp;rdquo;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://docstest.github.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png&#34;
         alt=&#34;Panel of contributors. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Panel of contributors. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;p&gt;The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor&amp;rsquo;s journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.&lt;/p&gt;

&lt;p&gt;After the workshop, Xiang Peng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, &amp;ldquo;I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor.&amp;rdquo; Another attendee, Jie Jia, said, &amp;ldquo;The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!&amp;rdquo;&lt;/p&gt;

&lt;p&gt;SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming Kubecon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future Kubecon. And, when you meet an NCW attendee, make sure to welcome them to the community.&lt;/p&gt;

&lt;p&gt;Links:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;English versions of the slides: &lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; or &lt;a href=&#34;https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Google Docs with speaker notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chinese version of the slides: &lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;Contributor playground&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Production-Ready Kubernetes Cluster Creation with kubeadm</title>
      <link>https://docstest.github.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/12/04/production-ready-kubernetes-cluster-creation-with-kubeadm/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Lucas Käldström (CNCF Ambassador) and Luc Perkins (CNCF Developer Advocate)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt; is a tool that enables Kubernetes administrators to quickly and easily bootstrap minimum viable clusters that are fully compliant with &lt;a href=&#34;https://github.com/cncf/k8s-conformance/blob/master/terms-conditions/Certified_Kubernetes_Terms.md&#34; target=&#34;_blank&#34;&gt;Certified Kubernetes&lt;/a&gt; guidelines. It&amp;rsquo;s been under active development by &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle&#34; target=&#34;_blank&#34;&gt;SIG Cluster Lifecycle&lt;/a&gt; since 2016 and we&amp;rsquo;re excited to announce that it has now graduated from beta to stable and generally available (GA)!&lt;/p&gt;

&lt;p&gt;This GA release of kubeadm is an important event in the progression of the Kubernetes ecosystem, bringing stability to an area where stability is paramount.&lt;/p&gt;

&lt;p&gt;The goal of kubeadm is to provide a foundational implementation for Kubernetes cluster setup and administration. kubeadm ships with best-practice defaults but can also be customized to support other ecosystem requirements or vendor-specific approaches. kubeadm is designed to be easy to integrate into larger deployment systems and tools.&lt;/p&gt;

&lt;h3 id=&#34;the-scope-of-kubeadm&#34;&gt;The scope of kubeadm&lt;/h3&gt;

&lt;p&gt;kubeadm is focused on bootstrapping Kubernetes clusters on existing infrastructure and performing an essential set of maintenance tasks. The core of the kubeadm interface is quite simple: new control plane nodes are created by running &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubeadm init&lt;/code&gt;&lt;/a&gt; and worker nodes are joined to the control plane by running &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubeadm join&lt;/code&gt;&lt;/a&gt;. Also included are utilities for managing already bootstrapped clusters, such as control plane upgrades and token and certificate renewal.&lt;/p&gt;

&lt;p&gt;To keep kubeadm lean, focused, and vendor/infrastructure agnostic, the following tasks are out of its scope:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Infrastructure provisioning&lt;/li&gt;
&lt;li&gt;Third-party networking&lt;/li&gt;
&lt;li&gt;Non-critical add-ons, e.g. for monitoring, logging, and visualization&lt;/li&gt;
&lt;li&gt;Specific cloud provider integrations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Infrastructure provisioning, for example, is left to other SIG Cluster Lifecycle projects, such as the &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API&lt;/a&gt;. Instead, kubeadm covers only the common denominator in every Kubernetes cluster: the &lt;a href=&#34;https://kubernetes.io/docs/concepts/#kubernetes-control-plane&#34; target=&#34;_blank&#34;&gt;control plane&lt;/a&gt;. The user may install their preferred networking solution and other add-ons on top of Kubernetes &lt;em&gt;after&lt;/em&gt; cluster creation.&lt;/p&gt;

&lt;h3 id=&#34;what-kubeadm-s-ga-release-means&#34;&gt;What kubeadm&amp;rsquo;s GA release means&lt;/h3&gt;

&lt;p&gt;General Availability means different things for different projects. For kubeadm, going GA means not only that the process of creating a conformant Kubernetes cluster is now stable, but also that kubeadm is flexible enough to support a wide variety of deployment options.&lt;/p&gt;

&lt;p&gt;We now consider kubeadm to have achieved GA-level maturity in each of these important domains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stable command-line UX&lt;/strong&gt; &amp;mdash; The kubeadm CLI conforms to &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-a-flag-or-cli&#34; target=&#34;_blank&#34;&gt;#5a GA rule of the Kubernetes Deprecation Policy&lt;/a&gt;, which states that a command or flag that exists in a GA version must be kept for at least 12 months after deprecation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable underlying implementation&lt;/strong&gt; &amp;mdash; kubeadm now creates a new Kubernetes cluster using methods that shouldn&amp;rsquo;t change any time soon. The control plane, for example, is run as a set of static Pods, bootstrap tokens are used for the &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubeadm join&lt;/code&gt;&lt;/a&gt; flow, and &lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/0014-20180707-componentconfig-api-types-to-staging.md&#34; target=&#34;_blank&#34;&gt;ComponentConfig&lt;/a&gt; is used for configuring the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&#34; target=&#34;_blank&#34;&gt;kubelet&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configuration file schema&lt;/strong&gt; &amp;mdash; With the new &lt;strong&gt;v1beta1&lt;/strong&gt; API version, you can now tune almost every part of the cluster declaratively and thus build a &amp;ldquo;GitOps&amp;rdquo; flow around kubeadm-built clusters. In future versions, we plan to graduate the API to version &lt;strong&gt;v1&lt;/strong&gt; with minimal changes (and perhaps none).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The &amp;ldquo;toolbox&amp;rdquo; interface of kubeadm&lt;/strong&gt; &amp;mdash; Also known as &lt;strong&gt;phases&lt;/strong&gt;. If you don&amp;rsquo;t want to perform all &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubeadm init&lt;/code&gt;&lt;/a&gt; tasks, you can instead apply more fine-grained actions using the &lt;code&gt;kubeadm init phase&lt;/code&gt; command (for example generating certificates or control plane &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/static-pod/&#34; target=&#34;_blank&#34;&gt;Static Pod&lt;/a&gt; manifests).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upgrades between minor versions&lt;/strong&gt; &amp;mdash; The &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;kubeadm upgrade&lt;/code&gt;&lt;/a&gt; command is now fully GA. It handles control plane upgrades for you, which includes upgrades to &lt;a href=&#34;https://etcd.io&#34; target=&#34;_blank&#34;&gt;etcd&lt;/a&gt;, the &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-overview/&#34; target=&#34;_blank&#34;&gt;API Server&lt;/a&gt;, the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&#34; target=&#34;_blank&#34;&gt;Controller Manager&lt;/a&gt;, and the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&#34; target=&#34;_blank&#34;&gt;Scheduler&lt;/a&gt;. You can seamlessly upgrade your cluster between minor or patch versions (e.g. v1.12.2 -&amp;gt; v1.13.1 or v1.13.1 -&amp;gt; v1.13.3).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;etcd setup&lt;/strong&gt; &amp;mdash; &lt;a href=&#34;https://etcd.io&#34; target=&#34;_blank&#34;&gt;etcd&lt;/a&gt; is now set up in a way that is secure by default, with TLS communication everywhere, and allows for expanding to a highly available cluster when needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;who-will-benefit-from-a-stable-kubeadm&#34;&gt;Who will benefit from a stable kubeadm&lt;/h3&gt;

&lt;p&gt;SIG Cluster Lifecycle has identified a handful of likely kubeadm user profiles, although we expect that kubeadm at GA can satisfy many other scenarios as well.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s our list:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;re a &lt;strong&gt;new user&lt;/strong&gt; who wants to take Kubernetes for a spin. kubeadm is the fastest way to get up and running on &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;Linux machines&lt;/a&gt;. If you&amp;rsquo;re using &lt;a href=&#34;https://github.com/kubernetes/minikube&#34; target=&#34;_blank&#34;&gt;Minikube&lt;/a&gt; on a Mac or Windows workstation, you&amp;rsquo;re actually already running kubeadm inside the Minikube VM!&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;re a &lt;strong&gt;system administrator&lt;/strong&gt; responsible for setting up Kubernetes on bare metal machines and you want to quickly create Kubernetes clusters that are secure and in conformance with best practices but also highly configurable.&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;re a &lt;strong&gt;cloud provider&lt;/strong&gt; who wants to add a Kubernetes offering to your suite of cloud services. kubeadm is the go-to tool for creating clusters at a low level.&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;re an &lt;strong&gt;organization that requires highly customized Kubernetes clusters&lt;/strong&gt;. Existing public cloud offerings like &lt;a href=&#34;https://aws.amazon.com/eks/&#34; target=&#34;_blank&#34;&gt;Amazon EKS&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/&#34; target=&#34;_blank&#34;&gt;Google Kubernetes Engine&lt;/a&gt; won&amp;rsquo;t cut it for you; you need customized Kubernetes clusters tailored to your hardware, security, policy, and other needs.&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;re creating a &lt;strong&gt;higher-level cluster creation tool&lt;/strong&gt; than kubeadm, building the cluster experience from the ground up, but you don&amp;rsquo;t want to reinvent the wheel. You can &amp;ldquo;rebase&amp;rdquo; on top of kubeadm and utilize the common bootstrapping tools kubeadm provides for you. Several community tools have adopted kubeadm, and it&amp;rsquo;s a perfect match for &lt;a href=&#34;https://github.com/kubernetes-sigs/cluster-api&#34; target=&#34;_blank&#34;&gt;Cluster API&lt;/a&gt; implementations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All these users can benefit from kubeadm graduating to a stable GA state.&lt;/p&gt;

&lt;h3 id=&#34;kubeadm-survey&#34;&gt;kubeadm survey&lt;/h3&gt;

&lt;p&gt;Although kubeadm is GA, the SIG Cluster Lifecycle will continue to be committed to improving the user experience in managing Kubernetes clusters. We&amp;rsquo;re launching a survey to collect community feedback about kubeadm for the sake of future improvement.&lt;/p&gt;

&lt;p&gt;The survey is available at &lt;a href=&#34;https://bit.ly/2FPfRiZ&#34; target=&#34;_blank&#34;&gt;https://bit.ly/2FPfRiZ&lt;/a&gt;. Your participation would be highly valued!&lt;/p&gt;

&lt;h3 id=&#34;thanks-to-the-community&#34;&gt;Thanks to the community!&lt;/h3&gt;

&lt;p&gt;This release wouldn&amp;rsquo;t have been possible without the help of the great people that have been contributing to the SIG. SIG Cluster Lifecycle would like to thank a few key kubeadm contributors:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Organization&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Role&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;Tim St. Clair&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Heptio&lt;/td&gt;
&lt;td&gt;SIG co-chair&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/roberthbailey&#34; target=&#34;_blank&#34;&gt;Robert Bailey&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;SIG co-chair&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/fabriziopandini&#34; target=&#34;_blank&#34;&gt;Fabrizio Pandini&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Independent&lt;/td&gt;
&lt;td&gt;Approver&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/neolit123&#34; target=&#34;_blank&#34;&gt;Lubomir Ivanov&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;VMware&lt;/td&gt;
&lt;td&gt;Approver&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/mikedanese&#34; target=&#34;_blank&#34;&gt;Mike Danese&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;Emeritus approver&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/errordeveloper&#34; target=&#34;_blank&#34;&gt;Ilya Dmitrichenko&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Weaveworks&lt;/td&gt;
&lt;td&gt;Emeritus  approver&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/xiangpengzhao&#34; target=&#34;_blank&#34;&gt;Peter Zhao&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;ZTE&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/dixudx&#34; target=&#34;_blank&#34;&gt;Di Xu&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Ant Financial&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/chuckha&#34; target=&#34;_blank&#34;&gt;Chuck Ha&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Heptio&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/liztio&#34; target=&#34;_blank&#34;&gt;Liz Frost&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Heptio&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/detiber&#34; target=&#34;_blank&#34;&gt;Jason DeTiberus&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Heptio&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kad&#34; target=&#34;_blank&#34;&gt;Alexander Kanievsky&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intel&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/rosti&#34; target=&#34;_blank&#34;&gt;Ross Georgiev&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;VMware&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/yagonobre&#34; target=&#34;_blank&#34;&gt;Yago Nobre&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Nubank&lt;/td&gt;
&lt;td&gt;Reviewer&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We also want to thank all the companies making it possible for their developers to work on Kubernetes, and all the other people that have contributed in various ways towards making kubeadm as stable as it is today!&lt;/p&gt;

&lt;h3 id=&#34;about-the-authors&#34;&gt;About the authors&lt;/h3&gt;

&lt;h4 id=&#34;lucas-käldström&#34;&gt;Lucas Käldström&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;kubeadm subproject owner and SIG Cluster Lifecycle co-chair&lt;/li&gt;
&lt;li&gt;Kubernetes upstream contractor, last two years contracting for &lt;a href=&#34;https://weave.works&#34; target=&#34;_blank&#34;&gt;Weaveworks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CNCF Ambassador&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/luxas&#34; target=&#34;_blank&#34;&gt;luxas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;luc-perkins&#34;&gt;Luc Perkins&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cncf.io&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt; Developer Advocate&lt;/li&gt;
&lt;li&gt;Kubernetes SIG Docs contributor and SIG Docs tooling WG chair&lt;/li&gt;
&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/lucperkins&#34; target=&#34;_blank&#34;&gt;lucperkins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.13: Simplified Cluster Management with Kubeadm, Container Storage Interface (CSI), and CoreDNS as Default DNS are Now Generally Available</title>
      <link>https://docstest.github.io/blog/2018/12/03/kubernetes-1-13-release-announcement/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/12/03/kubernetes-1-13-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: The 1.13 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.13/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.13, our fourth and final release of 2018!&lt;/p&gt;

&lt;p&gt;Kubernetes 1.13 has been one of the shortest releases to date at 10 weeks. This release continues to focus on stability and extensibility of Kubernetes with three major features graduating to general availability this cycle in the areas of Storage and Cluster Lifecycle. Notable features graduating in this release include: simplified cluster management with kubeadm, Container Storage Interface (CSI), and CoreDNS as the default DNS.&lt;/p&gt;

&lt;p&gt;These stable graduations are an important milestone for users and operators in terms of setting support expectations. In addition, there’s a continual and steady stream of internal improvements and new alpha features that are made available to the community in this release. These features are discussed in the “additional notable features” section below.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;simplified-kubernetes-cluster-management-with-kubeadm-in-ga&#34;&gt;Simplified Kubernetes Cluster Management with kubeadm in GA&lt;/h2&gt;

&lt;p&gt;Most people who have gotten hands-on with Kubernetes have at some point been hands-on with kubeadm. It&amp;rsquo;s an essential tool for managing the cluster lifecycle, from creation to configuration to upgrade; and now kubeadm is officially GA. &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt; handles the bootstrapping of production clusters on existing hardware and configuring the core Kubernetes components in a best-practice-manner to providing a secure yet easy joining flow for new nodes and supporting easy upgrades. What’s notable about this GA release are the now graduated advanced features, specifically around pluggability and configurability. The scope of kubeadm is to be a toolbox for both admins and automated, higher-level system and this release is a significant step in that direction.&lt;/p&gt;

&lt;h2 id=&#34;container-storage-interface-csi-goes-ga&#34;&gt;Container Storage Interface (CSI) Goes GA&lt;/h2&gt;

&lt;p&gt;The Container Storage Interface (&lt;a href=&#34;https://github.com/container-storage-interface&#34; target=&#34;_blank&#34;&gt;CSI&lt;/a&gt;) is now GA after being introduced as alpha in v1.9 and beta in v1.10. With CSI, the Kubernetes volume layer becomes truly extensible. This provides an opportunity for third party storage providers to write plugins that interoperate with Kubernetes without having to touch the core code. The &lt;a href=&#34;https://github.com/container-storage-interface/spec&#34; target=&#34;_blank&#34;&gt;specification itself&lt;/a&gt; has also reached a 1.0 status.&lt;/p&gt;

&lt;p&gt;With CSI now stable, plugin authors are developing storage plugins out of core, at their own pace. You can find a list of sample and production drivers in the &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;CSI Documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;coredns-is-now-the-default-dns-server-for-kubernetes&#34;&gt;CoreDNS is Now the Default DNS Server for Kubernetes&lt;/h2&gt;

&lt;p&gt;In 1.11, we announced CoreDNS had reached General Availability for DNS-based service discovery. In 1.13, &lt;a href=&#34;https://github.com/kubernetes/features/issues/566&#34; target=&#34;_blank&#34;&gt;CoreDNS is now replacing kube-dns as the default DNS server&lt;/a&gt; for Kubernetes. CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. CoreDNS has fewer moving parts than the previous DNS server, since it’s a single executable and a single process, and supports flexible use cases by creating custom DNS entries. It’s also written in Go making it memory-safe.&lt;/p&gt;

&lt;p&gt;CoreDNS is now the recommended DNS solution for Kubernetes 1.13+. The project has switched the common test infrastructure to use CoreDNS by default and we recommend users switching as well. KubeDNS will still be supported for at least one more release, but it&amp;rsquo;s time to start planning your migration. Many OSS installer tools have already made the switch, including &lt;a href=&#34;https://kubernetes.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/&#34; target=&#34;_blank&#34;&gt;Kubeadm in 1.11&lt;/a&gt;. If you use a hosted solution, please work with your vendor to understand how this will impact you.&lt;/p&gt;

&lt;h2 id=&#34;additional-notable-feature-updates&#34;&gt;Additional Notable Feature Updates&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/606&#34; target=&#34;_blank&#34;&gt;Support for 3rd party device monitoring plugins&lt;/a&gt; has been introduced as an alpha feature. This removes current device-specific knowledge from the kubelet to enable future use-cases requiring device-specific knowledge to be out-of-tree.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/595&#34; target=&#34;_blank&#34;&gt;Kubelet Device Plugin Registration&lt;/a&gt; is graduating to stable. This creates a common Kubelet plugin discovery model that can be used by different types of node-level plugins, such as device plugins, CSI and CNI, to establish communication channels with Kubelet.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/490&#34; target=&#34;_blank&#34;&gt;Topology Aware Volume Scheduling&lt;/a&gt; is now stable. This make the scheduler aware of a Pod&amp;rsquo;s volume&amp;rsquo;s topology constraints, such as zone or node.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/576&#34; target=&#34;_blank&#34;&gt;APIServer DryRun&lt;/a&gt; is graduating to beta. This moves &amp;ldquo;apply&amp;rdquo; and declarative object management from &lt;code&gt;kubectl&lt;/code&gt; to the &lt;code&gt;apiserver&lt;/code&gt; in order to fix many of the existing bugs that can&amp;rsquo;t be fixed today.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/491&#34; target=&#34;_blank&#34;&gt;Kubectl Diff&lt;/a&gt; is graduating to beta. This allows users to run a &lt;code&gt;kubectl&lt;/code&gt; command to view the difference between a locally declared object configuration and the current state of a live object.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/351&#34; target=&#34;_blank&#34;&gt;Raw block device using persistent volume source&lt;/a&gt; is graduating to beta. This makes raw block devices (non-networked) available for consumption via a Persistent Volume Source.&lt;/p&gt;

&lt;p&gt;Each Special Interest Group (SIG) within the community continues to deliver the most-requested enhancements, fixes, and functionality for their respective specialty areas. For a complete list of inclusions by SIG, please visit the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.13.md#113-release-notes&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.13 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.13.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also easily install 1.13 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;features-blog-series&#34;&gt;Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back tomorrow for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Day 1 - Simplified Kubernetes Cluster Creation with Kubeadm&lt;/li&gt;
&lt;li&gt;Day 2 - Out-of-tree CSI Volume Plugins&lt;/li&gt;
&lt;li&gt;Day 3 - Switch default DNS plugin to CoreDNS&lt;/li&gt;
&lt;li&gt;Day 4 - New CLI Tips and Tricks (Kubectl Diff and APIServer Dry run)&lt;/li&gt;
&lt;li&gt;Day 5 - Raw Block Volume&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.13/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Aishwarya Sundar, Software Engineer at Google. The 39 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 25,000 individual contributors to date and an active community of more than 51,000 people.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average over the past year, 347 different companies and over 2,372 individuals contribute to Kubernetes each month. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;IBM Cloud&lt;/strong&gt;, a leading provider of public, private, and hybrid cloud functionality, is using &lt;a href=&#34;https://kubernetes.io/case-studies/ibm/&#34; target=&#34;_blank&#34;&gt;cloud native technology for high-availability deployments&lt;/a&gt; with three instances across two zones in each of the five regions, load balanced with failover support.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The National Association of Insurance Commissioners (NAIC)&lt;/strong&gt;, the U.S. standard-setting and regulatory support organization, leverages Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/naic/&#34; target=&#34;_blank&#34;&gt;create rapid prototypes in two days&lt;/a&gt; that would have previously taken at least a month.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ocado&lt;/strong&gt;, the world’s largest online-only grocery retailer, &lt;a href=&#34;https://kubernetes.io/case-studies/ocado/&#34; target=&#34;_blank&#34;&gt;use 15-25% less hardware resources&lt;/a&gt; to host the same applications in Kubernetes in their test environments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adform&lt;/strong&gt;, a provider of advertising technology to enable digital ads across devices, &lt;a href=&#34;https://kubernetes.io/case-studies/adform/&#34; target=&#34;_blank&#34;&gt;uses Kubernetes to reduce utilization of hardware resources&lt;/a&gt;, with containers notching 2-3 times more efficiency over virtual machines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;CNCF recently released the findings of their &lt;a href=&#34;https://www.cncf.io/blog/2018/11/13/cncf-survey-china-november-2018/&#34; target=&#34;_blank&#34;&gt;bi-annual CNCF survey&lt;/a&gt; in Mandarin, finding that cloud usage in Asia has grown 135% since March 2018.&lt;/li&gt;
&lt;li&gt;CNCF expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual&amp;rsquo;s ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found &lt;a href=&#34;https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online training&lt;/a&gt; that teaches the skills needed to create and configure a real-world Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Kubernetes documentation now features &lt;a href=&#34;https://k8s.io/docs/home/&#34; target=&#34;_blank&#34;&gt;user journeys&lt;/a&gt;: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;Seattle&lt;/a&gt; from December 10-13, 2018 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-europe-2019/&#34; target=&#34;_blank&#34;&gt;Barcelona&lt;/a&gt; from May 20-23, 2019. This conference will feature technical sessions, case studies, developer deep dives, salons, and more. &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Registration &lt;/a&gt; will open up in early 2019.&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.13 release team on January 10th at 9am PDT to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/WN_A2FZovz-TIWn_Xvrb5uERQ&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community discussion on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Docs Updates, International Edition</title>
      <link>https://docstest.github.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Zach Corleissen (Linux Foundation)&lt;/p&gt;

&lt;p&gt;As a co-chair of SIG Docs, I&amp;rsquo;m excited to share that Kubernetes docs have a fully mature workflow for localization (l10n).&lt;/p&gt;

&lt;h2 id=&#34;abbreviations-galore&#34;&gt;Abbreviations galore&lt;/h2&gt;

&lt;p&gt;L10n is an abbreviation for &lt;em&gt;localization&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I18n is an abbreviation for &lt;em&gt;internationalization&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I18n is &lt;a href=&#34;https://www.w3.org/International/questions/qa-i18n&#34; target=&#34;_blank&#34;&gt;what you do&lt;/a&gt; to make l10n easier. L10n is a fuller, more comprehensive process than translation (&lt;em&gt;t9n&lt;/em&gt;).&lt;/p&gt;

&lt;h2 id=&#34;why-localization-matters&#34;&gt;Why localization matters&lt;/h2&gt;

&lt;p&gt;The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible.&lt;/p&gt;

&lt;p&gt;One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), &lt;a href=&#34;https://kubernetes.io/blog/2018/05/05/hugo-migration/&#34; target=&#34;_blank&#34;&gt;much transformation&lt;/a&gt;, and &lt;a href=&#34;https://github.com/kubernetes/website/pull/10485&#34; target=&#34;_blank&#34;&gt;renewed commitment to easier localization&lt;/a&gt;, we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what&amp;rsquo;s possible.&lt;/p&gt;

&lt;p&gt;Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we&amp;rsquo;ve paid off a significant amount of technical debt and streamlined l10n in a single workflow. That&amp;rsquo;s great for the future as well as the present.&lt;/p&gt;

&lt;h2 id=&#34;consolidated-workflow&#34;&gt;Consolidated workflow&lt;/h2&gt;

&lt;p&gt;Localization is now consolidated in the &lt;a href=&#34;https://github.com/kubernetes/website&#34; target=&#34;_blank&#34;&gt;kubernetes/website&lt;/a&gt; repository. We&amp;rsquo;ve configured the Kubernetes CI/CD system, &lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt;, to handle automatic language label assignment as well as team-level PR review and approval.&lt;/p&gt;

&lt;h3 id=&#34;language-labels&#34;&gt;Language labels&lt;/h3&gt;

&lt;p&gt;Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor &lt;a href=&#34;https://github.com/kubernetes/test-infra/pull/9835&#34; target=&#34;_blank&#34;&gt;June Yi&lt;/a&gt;, folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label &lt;code&gt;language/ko&lt;/code&gt; (Korean).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/language ko
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for &lt;a href=&#34;https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh&#34; target=&#34;_blank&#34;&gt;PRs with Chinese content&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;team-review&#34;&gt;Team review&lt;/h3&gt;

&lt;p&gt;L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are &lt;a href=&#34;https://github.com/kubernetes/website/blob/master/content/en/OWNERS&#34; target=&#34;_blank&#34;&gt;assigned in an OWNERS file&lt;/a&gt; in the top subfolder for English content.&lt;/p&gt;

&lt;p&gt;Adding &lt;code&gt;OWNERS&lt;/code&gt; files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re looking forward to the &lt;a href=&#34;https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required&#34; target=&#34;_blank&#34;&gt;doc sprint in Shanghai&lt;/a&gt; to serve as a resource for the Chinese l10n team.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested in localizing Kubernetes for your own language or region, check out our &lt;a href=&#34;https://kubernetes.io/docs/contribute/localization/&#34; target=&#34;_blank&#34;&gt;guide to localizing Kubernetes docs&lt;/a&gt; and reach out to a &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs#leadership&#34; target=&#34;_blank&#34;&gt;SIG Docs chair&lt;/a&gt; for support.&lt;/p&gt;

&lt;h3 id=&#34;get-involved-with-sig-docs&#34;&gt;Get involved with SIG Docs&lt;/h3&gt;

&lt;p&gt;If you&amp;rsquo;re interested in Kubernetes documentation, come to a SIG Docs &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs#meetings&#34; target=&#34;_blank&#34;&gt;weekly meeting&lt;/a&gt;, or join &lt;a href=&#34;https://kubernetes.slack.com/messages/C1J0BPD2M/details/&#34; target=&#34;_blank&#34;&gt;#sig-docs in Kubernetes Slack&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: gRPC Load Balancing on Kubernetes without Tears</title>
      <link>https://docstest.github.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: William Morgan (Buoyant)&lt;/p&gt;

&lt;p&gt;Many new gRPC users are surprised to find that Kubernetes&amp;rsquo;s default load
balancing often doesn&amp;rsquo;t work out of the box with gRPC. For example, here&amp;rsquo;s what
happens when you take a &lt;a href=&#34;https://github.com/sourishkrout/nodevoto&#34; target=&#34;_blank&#34;&gt;simple gRPC Node.js microservices
app&lt;/a&gt; and deploy it on Kubernetes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0116-c4d86100-afc1-4a08-a01c-16da391756dd.34.36.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While the &lt;code&gt;voting&lt;/code&gt; service displayed here has several pods, it&amp;rsquo;s clear from
Kubernetes&amp;rsquo;s CPU graphs that only one of the pods is actually doing any
work&amp;mdash;because only one of the pods is receiving any traffic. Why?&lt;/p&gt;

&lt;p&gt;In this blog post, we describe why this happens, and how you can easily fix it
by adding gRPC load balancing to any Kubernetes app with
&lt;a href=&#34;https://linkerd.io&#34; target=&#34;_blank&#34;&gt;Linkerd&lt;/a&gt;, a &lt;a href=&#34;https://cncf.io&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt; service mesh and service sidecar.&lt;/p&gt;

&lt;h1 id=&#34;why-does-grpc-need-special-load-balancing&#34;&gt;Why does gRPC need special load balancing?&lt;/h1&gt;

&lt;p&gt;First, let&amp;rsquo;s understand why we need to do something special for gRPC.&lt;/p&gt;

&lt;p&gt;gRPC is an increasingly common choice for application developers. Compared to
alternative protocols such as JSON-over-HTTP, gRPC can provide some significant
benefits, including dramatically lower (de)serialization costs, automatic type
checking, formalized APIs, and less TCP management overhead.&lt;/p&gt;

&lt;p&gt;However, gRPC also breaks the standard connection-level load balancing,
including what&amp;rsquo;s provided by Kubernetes. This is because gRPC is built on
HTTP/2, and HTTP/2 is designed to have a single long-lived TCP connection,
across which all requests are &lt;em&gt;multiplexed&lt;/em&gt;&amp;mdash;meaning multiple requests can be
active on the same connection at any point in time. Normally, this is great, as
it reduces the overhead of connection management. However, it also means that
(as you might imagine) connection-level balancing isn&amp;rsquo;t very useful. Once the
connection is established, there&amp;rsquo;s no more balancing to be done. All requests
will get pinned to a single destination pod, as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Mono-8d2e53ef-b133-4aa0-9551-7e36a880c553.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;why-doesn-t-this-affect-http-1-1&#34;&gt;Why doesn&amp;rsquo;t this affect HTTP/1.1?&lt;/h1&gt;

&lt;p&gt;The reason why this problem doesn&amp;rsquo;t occur in HTTP/1.1, which also has the
concept of long-lived connections, is because HTTP/1.1 has several features
that naturally result in cycling of TCP connections. Because of this,
connection-level balancing is &amp;ldquo;good enough&amp;rdquo;, and for most HTTP/1.1 apps we
don&amp;rsquo;t need to do anything more.&lt;/p&gt;

&lt;p&gt;To understand why, let&amp;rsquo;s take a deeper look at HTTP/1.1. In contrast to HTTP/2,
HTTP/1.1 cannot multiplex requests. Only one HTTP request can be active at a
time per TCP connection. The client makes a request, e.g. &lt;code&gt;GET /foo&lt;/code&gt;, and then
waits until the server responds. While that request-response cycle is
happening, no other requests can be issued on that connection.&lt;/p&gt;

&lt;p&gt;Usually, we want lots of requests happening in parallel. Therefore, to have
concurrent HTTP/1.1 requests, we need to make multiple HTTP/1.1 connections,
and issue our requests across all of them. Additionally, long-lived HTTP/1.1
connections typically expire after some time, and are torn down by the client
(or server). These two factors combined mean that HTTP/1.1 requests typically
cycle across multiple TCP connections, and so connection-level balancing works.&lt;/p&gt;

&lt;h1 id=&#34;so-how-do-we-load-balance-grpc&#34;&gt;So how do we load balance gRPC?&lt;/h1&gt;

&lt;p&gt;Now back to gRPC. Since we can&amp;rsquo;t balance at the connection level, in order to
do gRPC load balancing, we need to shift from connection balancing to &lt;em&gt;request&lt;/em&gt;
balancing. In other words, we need to open an HTTP/2 connection to each
destination, and balance &lt;em&gt;requests&lt;/em&gt; across these connections, as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Stereo-09aff9d7-1c98-4a0a-9184-9998ed83a531.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In network terms, this means we need to make decisions at L5/L7 rather than
L3/L4, i.e. we need to understand the protocol sent over the TCP connections.&lt;/p&gt;

&lt;p&gt;How do we accomplish this? There are a couple options. First, our application
code could manually maintain its own load balancing pool of destinations, and
we could configure our gRPC client to &lt;a href=&#34;https://godoc.org/google.golang.org/grpc/balancer&#34; target=&#34;_blank&#34;&gt;use this load balancing
pool&lt;/a&gt;. This approach gives
us the most control, but it can be very complex in environments like Kubernetes
where the pool changes over time as Kubernetes reschedules pods. Our
application would have to watch the Kubernetes API and keep itself up to date
with the pods.&lt;/p&gt;

&lt;p&gt;Alternatively, in Kubernetes, we could deploy our app as &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#headless-services&#34; target=&#34;_blank&#34;&gt;headless
services&lt;/a&gt;.
In this case, Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#headless-services&#34; target=&#34;_blank&#34;&gt;will create multiple A
records&lt;/a&gt;
in the DNS entry for the service. If our gRPC client is sufficiently advanced,
it can automatically maintain the load balancing pool from those DNS entries.
But this approach restricts us to certain gRPC clients, and it&amp;rsquo;s rarely
possible to only use headless services.&lt;/p&gt;

&lt;p&gt;Finally, we can take a third approach: use a lightweight proxy.&lt;/p&gt;

&lt;h1 id=&#34;grpc-load-balancing-on-kubernetes-with-linkerd&#34;&gt;gRPC load balancing on Kubernetes with Linkerd&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://linkerd.io&#34; target=&#34;_blank&#34;&gt;Linkerd&lt;/a&gt; is a &lt;a href=&#34;https://cncf.io&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt;-hosted &lt;em&gt;service
mesh&lt;/em&gt; for Kubernetes. Most relevant to our purposes, Linkerd also functions as
a &lt;em&gt;service sidecar&lt;/em&gt;, where it can be applied to a single service&amp;mdash;even without
cluster-wide permissions. What this means is that when we add Linkerd to our
service, it adds a tiny, ultra-fast proxy to each pod, and these proxies watch
the Kubernetes API and do gRPC load balancing automatically. Our deployment
then looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Linkerd-8df1031c-cdd1-4164-8e91-00f2d941e93f.io.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using Linkerd has a couple advantages. First, it works with services written in
any language, with any gRPC client, and any deployment model (headless or not).
Because Linkerd&amp;rsquo;s proxies are completely transparent, they auto-detect HTTP/2
and HTTP/1.x and do L7 load balancing, and they pass through all other traffic
as pure TCP. This means that everything will &lt;em&gt;just work.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Second, Linkerd&amp;rsquo;s load balancing is very sophisticated. Not only does Linkerd
maintain a watch on the Kubernetes API and automatically update the load
balancing pool as pods get rescheduled, Linkerd uses an &lt;em&gt;exponentially-weighted
moving average&lt;/em&gt; of response latencies to automatically send requests to the
fastest pods. If one pod is slowing down, even momentarily, Linkerd will shift
traffic away from it. This can reduce end-to-end tail latencies.&lt;/p&gt;

&lt;p&gt;Finally, Linkerd&amp;rsquo;s Rust-based proxies are incredibly fast and small. They
introduce &amp;lt;1ms of p99 latency and require &amp;lt;10mb of RSS per pod, meaning that
the impact on system performance will be negligible.&lt;/p&gt;

&lt;h1 id=&#34;grpc-load-balancing-in-60-seconds&#34;&gt;gRPC Load Balancing in 60 seconds&lt;/h1&gt;

&lt;p&gt;Linkerd is very easy to try. Just follow the steps in the &lt;a href=&#34;https://linkerd.io/2/getting-started/&#34; target=&#34;_blank&#34;&gt;Linkerd Getting
Started Instructions&lt;/a&gt;&amp;mdash;install the
CLI on your laptop, install the control plane on your cluster, and &amp;ldquo;mesh&amp;rdquo; your
service (inject the proxies into each pod). You&amp;rsquo;ll have Linkerd running on your
service in no time, and should see proper gRPC balancing immediately.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s take a look at our sample &lt;code&gt;voting&lt;/code&gt; service again, this time after
installing Linkerd:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0116-24b8ee81-144c-4eac-b73d-871bbf0ea22e.57.42.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see, the CPU graphs for all pods are active, indicating that all pods
are now taking traffic&amp;mdash;without having to change a line of code. Voila,
gRPC load balancing as if by magic!&lt;/p&gt;

&lt;p&gt;Linkerd also gives us built-in traffic-level dashboards, so we don&amp;rsquo;t even need
to guess what&amp;rsquo;s happening from CPU charts any more. Here&amp;rsquo;s a Linkerd graph
that&amp;rsquo;s showing the success rate, request volume, and latency percentiles of
each pod:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/grpc-load-balancing-with-linkerd/Screenshot2018-11-0212-15ed0448-5424-4e47-9828-20032de868b5.08.38.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that each pod is getting around 5 RPS. We can also see that, while
we&amp;rsquo;ve solved our load balancing problem, we still have some work to do on our
success rate for this service. (The demo app is built with an intentional
failure&amp;mdash;as an exercise to the reader, see if you can figure it out by
using the Linkerd dashboard!)&lt;/p&gt;

&lt;h1 id=&#34;wrapping-it-up&#34;&gt;Wrapping it up&lt;/h1&gt;

&lt;p&gt;If you&amp;rsquo;re interested in a dead simple way to add gRPC load balancing to your
Kubernetes services, regardless of what language it&amp;rsquo;s written in, what gRPC
client you&amp;rsquo;re using, or how it&amp;rsquo;s deployed, you can use Linkerd to add gRPC load
balancing in a few commands.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to Linkerd, including security, reliability, and debugging
and diagnostics features, but those are topics for future blog posts.&lt;/p&gt;

&lt;p&gt;Want to learn more? We’d love to have you join our rapidly-growing community!
Linkerd is a &lt;a href=&#34;https://cncf.io&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt; project, &lt;a href=&#34;https://github.com/linkerd/linkerd2&#34; target=&#34;_blank&#34;&gt;hosted on
GitHub&lt;/a&gt;, and has a thriving community
on &lt;a href=&#34;https://slack.linkerd.io&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/linkerd&#34; target=&#34;_blank&#34;&gt;Twitter&lt;/a&gt;,
and the &lt;a href=&#34;https://lists.cncf.io/g/cncf-linkerd-users&#34; target=&#34;_blank&#34;&gt;mailing lists&lt;/a&gt;. Come and
join the fun!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Tips for Your First Kubecon Presentation - Part 2</title>
      <link>https://docstest.github.io/blog/2018/10/26/tips-for-your-first-kubecon-presentation-part-2/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/26/tips-for-your-first-kubecon-presentation-part-2/</guid>
      <description>
        
        
        
      </description>
    </item>
    
    <item>
      <title>Blog: Tips for Your First Kubecon Presentation - Part 1</title>
      <link>https://docstest.github.io/blog/2018/10/18/tips-for-your-first-kubecon-presentation-part-1/</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/18/tips-for-your-first-kubecon-presentation-part-1/</guid>
      <description>
        
        
        
      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 2018 North American Contributor Summit</title>
      <link>https://docstest.github.io/blog/2018/10/16/kubernetes-2018-north-american-contributor-summit/</link>
      <pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/16/kubernetes-2018-north-american-contributor-summit/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;
&lt;a href=&#34;https://twitter.com/mrbobbytables&#34; target=&#34;_blank&#34;&gt;Bob Killen&lt;/a&gt; (University of Michigan)
&lt;a href=&#34;https://twitter.com/sp_zala&#34; target=&#34;_blank&#34;&gt;Sahdev Zala&lt;/a&gt; (IBM),
&lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt; (CNCF)&lt;/p&gt;

&lt;p&gt;The 2018 North American Kubernetes Contributor Summit to be hosted right before
&lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;KubeCon + CloudNativeCon&lt;/a&gt; Seattle is shaping up to be the largest yet.
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.&lt;/p&gt;

&lt;p&gt;Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed ‘hallway’ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the &lt;a href=&#34;https://www.garagebilliards.com/&#34; target=&#34;_blank&#34;&gt;Garage Lounge and Gaming Hall&lt;/a&gt;, just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.&lt;/p&gt;

&lt;p&gt;Things pick up the following day, Monday the 10th with three separate tracks:&lt;/p&gt;

&lt;h3 id=&#34;new-contributor-workshop&#34;&gt;New Contributor Workshop:&lt;/h3&gt;

&lt;p&gt;A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.&lt;/p&gt;

&lt;h3 id=&#34;current-contributor-track&#34;&gt;Current Contributor Track:&lt;/h3&gt;

&lt;p&gt;Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the &lt;a href=&#34;https://git.k8s.io/community/events/2018/12-contributor-summit#agenda&#34; target=&#34;_blank&#34;&gt;schedule in GitHub&lt;/a&gt; as content is frequently being updated.&lt;/p&gt;

&lt;h3 id=&#34;docs-sprint&#34;&gt;Docs Sprint:&lt;/h3&gt;

&lt;p&gt;SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.&lt;/p&gt;

&lt;h2 id=&#34;to-register&#34;&gt;To Register:&lt;/h2&gt;

&lt;p&gt;To register for the Contributor Summit, see the &lt;a href=&#34;https://git.k8s.io/community/events/2018/12-contributor-summit#registration&#34; target=&#34;_blank&#34;&gt;Registration section of the
Event Details in GitHub&lt;/a&gt;. Please note that registrations are being
reviewed. If you select the “Current Contributor Track” and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.&lt;/p&gt;

&lt;p&gt;If you have any questions or concerns, please don’t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.&lt;/p&gt;

&lt;p&gt;Look forward to seeing everyone there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2018 Steering Committee Election Results</title>
      <link>https://docstest.github.io/blog/2018/10/15/2018-steering-committee-election-results/</link>
      <pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/15/2018-steering-committee-election-results/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/&#34; target=&#34;_blank&#34;&gt;Kubernetes Steering Committee Election&lt;/a&gt; is now complete and the following candidates came ahead to secure two year terms that start immediately:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Aaron Crickenberger, Google, &lt;a href=&#34;https://github.com/spiffxp&#34; target=&#34;_blank&#34;&gt;@spiffxp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Davanum Srinivas, Huawei, &lt;a href=&#34;https://github.com/dims&#34; target=&#34;_blank&#34;&gt;@dims&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tim St. Clair, Heptio, &lt;a href=&#34;https://github.com/timothysc&#34; target=&#34;_blank&#34;&gt;@timothysc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;big-thanks&#34;&gt;Big Thanks!&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Steering Committee Member Emeritus &lt;a href=&#34;https://github.com/quinton-hoole&#34; target=&#34;_blank&#34;&gt;Quinton Hoole&lt;/a&gt; for his service to the community over the past year. We look forward to&lt;/li&gt;
&lt;li&gt;The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.&lt;/li&gt;
&lt;li&gt;All 307 voters who cast a ballot.&lt;/li&gt;
&lt;li&gt;And last but not least&amp;hellip;Cornell University for hosting &lt;a href=&#34;https://civs.cs.cornell.edu/&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;get-involved-with-the-steering-committee&#34;&gt;Get Involved with the Steering Committee&lt;/h2&gt;

&lt;p&gt;You can follow along to Steering Committee &lt;a href=&#34;https://git.k8s.io/steering/backlog.md&#34; target=&#34;_blank&#34;&gt;backlog items&lt;/a&gt; and weigh in by filing an issue or creating a PR against their &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt;. They meet bi-weekly on &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Wednesdays at 8pm UTC&lt;/a&gt; and regularly attend Meet Our Contributors.&lt;/p&gt;

&lt;p&gt;Steering Committee Meetings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM&#34; target=&#34;_blank&#34;&gt;YouTube Playlist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Meet Our Contributors Steering AMA’s:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/x6Jm8p0K-IQ&#34; target=&#34;_blank&#34;&gt;Oct  3 2018&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/UbxWV12Or58&#34; target=&#34;_blank&#34;&gt;Sept 5 2018&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Topology-Aware Volume Provisioning in Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/10/11/topology-aware-volume-provisioning-in-kubernetes/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/11/topology-aware-volume-provisioning-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Michelle Au (Google)&lt;/p&gt;

&lt;p&gt;The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod.  In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.&lt;/p&gt;

&lt;h2 id=&#34;previous-challenges&#34;&gt;Previous challenges&lt;/h2&gt;

&lt;p&gt;Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.&lt;/p&gt;

&lt;p&gt;This resulted in unschedulable pods because volumes were provisioned in zones that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;did not have enough CPU or memory resources to run the pod&lt;/li&gt;
&lt;li&gt;conflicted with node selectors, pod affinity or anti-affinity policies&lt;/li&gt;
&lt;li&gt;could not run the pod due to taints&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.&lt;/p&gt;

&lt;p&gt;Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.&lt;/p&gt;

&lt;p&gt;The topology-aware dynamic provisioning feature addresses all of the above issues.&lt;/p&gt;

&lt;h2 id=&#34;supported-volume-types&#34;&gt;Supported Volume Types&lt;/h2&gt;

&lt;p&gt;In 1.12, the following drivers support topology-aware dynamic provisioning:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AWS EBS&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;GCE PD (including Regional PD)&lt;/li&gt;
&lt;li&gt;CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;design-principles&#34;&gt;Design Principles&lt;/h2&gt;

&lt;p&gt;While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.&lt;/p&gt;

&lt;p&gt;In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage system’s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.&lt;/p&gt;

&lt;h2 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;To enable this feature, all you need to do is to create a StorageClass with &lt;code&gt;volumeBindingMode&lt;/code&gt; set to &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: pd-standard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;zones&lt;/code&gt; parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.&lt;/p&gt;

&lt;p&gt;Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;multiple PVCs in a pod&lt;/li&gt;
&lt;li&gt;nodeAffinity across a subset of zones&lt;/li&gt;
&lt;li&gt;pod anti-affinity on zones&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:   
  serviceName: &amp;quot;nginx&amp;quot;
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: failure-domain.beta.kubernetes.io/zone
                operator: In
                values:
                - us-central1-a
                - us-central1-f
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - nginx
            topologyKey: failure-domain.beta.kubernetes.io/zone
      containers:
      - name: nginx
        image: gcr.io/google_containers/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
        - name: logs
          mountPath: /logs
 volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: topology-aware-standard
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: logs
    spec:
      accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
      storageClassName: topology-aware-standard
      resources:
        requests:
          storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pv -o=jsonpath=&#39;{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}&#39;
www-web-0       us-central1-f
logs-web-0      us-central1-f
www-web-1       us-central1-a
logs-web-1      us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Official documentation on the topology-aware dynamic provisioning feature is available here:
&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Documentation for CSI drivers is available at &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;We are actively working on improving this feature to support:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;more volume types, including dynamic provisioning for local volumes&lt;/li&gt;
&lt;li&gt;dynamic volume attachable count and capacity limits per node&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;If you have feedback for this feature or are interested in getting involved with the design and development, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special-Interest-Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

&lt;p&gt;Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing (&lt;a href=&#34;https://github.com/verult&#34; target=&#34;_blank&#34;&gt;verult&lt;/a&gt;), Chuqiang Li (&lt;a href=&#34;https://github.com/lichuqiang&#34; target=&#34;_blank&#34;&gt;lichuqiang&lt;/a&gt;), David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;), Deep Debroy (&lt;a href=&#34;https://github.com/ddebroy&#34; target=&#34;_blank&#34;&gt;ddebroy&lt;/a&gt;), Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;), Jordan Liggitt (&lt;a href=&#34;https://github.com/liggitt&#34; target=&#34;_blank&#34;&gt;liggitt&lt;/a&gt;), Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;), Pengfei Ni (&lt;a href=&#34;https://github.com/feiskyer&#34; target=&#34;_blank&#34;&gt;feiskyer&lt;/a&gt;), Saad Ali (&lt;a href=&#34;https://github.com/saad-ali&#34; target=&#34;_blank&#34;&gt;saad-ali&lt;/a&gt;), Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;), and Yecheng Fu (&lt;a href=&#34;https://github.com/cofyc&#34; target=&#34;_blank&#34;&gt;cofyc&lt;/a&gt;).&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes v1.12: Introducing RuntimeClass</title>
      <link>https://docstest.github.io/blog/2018/10/10/kubernetes-v1.12-introducing-runtimeclass/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/10/kubernetes-v1.12-introducing-runtimeclass/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Tim Allclair (Google)&lt;/p&gt;

&lt;p&gt;Kubernetes originally launched with support for Docker containers running native applications on a Linux host. Starting with &lt;a href=&#34;https://kubernetes.io/blog/2016/07/rktnetes-brings-rkt-container-engine-to-kubernetes/&#34; target=&#34;_blank&#34;&gt;rkt&lt;/a&gt; in Kubernetes 1.3 more runtimes were coming, which lead to the development of the &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34; target=&#34;_blank&#34;&gt;Container Runtime Interface&lt;/a&gt; (CRI). Since then, the set of alternative runtimes has only expanded: projects like &lt;a href=&#34;https://katacontainers.io/&#34; target=&#34;_blank&#34;&gt;Kata Containers&lt;/a&gt; and &lt;a href=&#34;https://github.com/google/gvisor&#34; target=&#34;_blank&#34;&gt;gVisor&lt;/a&gt; were announced for stronger workload isolation, and Kubernetes&amp;rsquo; Windows support has been &lt;a href=&#34;https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/&#34; target=&#34;_blank&#34;&gt;steadily progressing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With runtimes targeting so many different use cases, a clear need for mixed runtimes in a cluster arose. But all these different ways of running containers have brought a new set of problems to deal with:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How do users know which runtimes are available, and select the runtime for their workloads?&lt;/li&gt;
&lt;li&gt;How do we ensure pods are scheduled to the nodes that support the desired runtime?&lt;/li&gt;
&lt;li&gt;Which runtimes support which features, and how can we surface incompatibilities to the user?&lt;/li&gt;
&lt;li&gt;How do we account for the varying resource overheads of the runtimes?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;RuntimeClass&lt;/strong&gt; aims to solve these issues.&lt;/p&gt;

&lt;h2 id=&#34;runtimeclass-in-kubernetes-1-12&#34;&gt;RuntimeClass in Kubernetes 1.12&lt;/h2&gt;

&lt;p&gt;RuntimeClass was recently introduced as an alpha feature in Kubernetes 1.12. The initial implementation focuses on providing a runtime selection API, and paves the way to address the other open problems.&lt;/p&gt;

&lt;p&gt;The RuntimeClass resource represents a container runtime supported in a Kubernetes cluster. The cluster provisioner sets up, configures, and defines the concrete runtimes backing the RuntimeClass. In its current form, a RuntimeClassSpec holds a single field, the &lt;strong&gt;RuntimeHandler&lt;/strong&gt;. The RuntimeHandler is interpreted by the CRI implementation running on a node, and mapped to the actual runtime configuration. Meanwhile the PodSpec has been expanded with a new field, &lt;strong&gt;RuntimeClassName&lt;/strong&gt;, which names the RuntimeClass that should be used to run the pod.&lt;/p&gt;

&lt;p&gt;Why is RuntimeClass a pod level concept? The Kubernetes resource model expects certain resources to be shareable between containers in the pod. If the pod is made up of different containers with potentially different resource models, supporting the necessary level of resource sharing becomes very challenging. For example, it is extremely difficult to support a loopback (localhost) interface across a VM boundary, but this is a common model for communication between two containers in a pod.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;

&lt;p&gt;The RuntimeClass resource is an important foundation for surfacing runtime properties to the control plane. For example, to implement scheduler support for clusters with heterogeneous nodes supporting different runtimes, we might add &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity&#34; target=&#34;_blank&#34;&gt;NodeAffinity&lt;/a&gt; terms to the RuntimeClass definition. Another area to address is managing the variable resource requirements to run pods of different runtimes. The &lt;a href=&#34;https://docs.google.com/document/d/1EJKT4gyl58-kzt2bnwkv08MIUZ6lkDpXcxkHqCvvAp4/preview&#34; target=&#34;_blank&#34;&gt;Pod Overhead proposal&lt;/a&gt; was an early take on this that aligns nicely with the RuntimeClass design, and may be pursued further.&lt;/p&gt;

&lt;p&gt;Many other RuntimeClass extensions have also been proposed, and will be revisited as the feature continues to develop and mature. A few more extensions that are being considered include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Surfacing optional features supported by runtimes, and better visibility into errors caused by incompatible features.&lt;/li&gt;
&lt;li&gt;Automatic runtime or feature discovery, to support scheduling decisions without manual configuration.&lt;/li&gt;
&lt;li&gt;Standardized or conformant RuntimeClass names that define a set of properties that should be supported across clusters with RuntimeClasses of the same name.&lt;/li&gt;
&lt;li&gt;Dynamic registration of additional runtimes, so users can install new runtimes on existing clusters with no downtime.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Fitting&amp;rdquo; a RuntimeClass to a pod&amp;rsquo;s requirements. For instance, specifying runtime properties and letting the system match an appropriate RuntimeClass, rather than explicitly assigning a RuntimeClass by name.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;RuntimeClass will be under active development at least through 2019, and we’re excited to see the feature take shape, starting with the RuntimeClass alpha in Kubernetes 1.12.&lt;/p&gt;

&lt;h2 id=&#34;learn-more&#34;&gt;Learn More&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Take it for a spin! As an alpha feature, there are some additional setup steps to use RuntimeClass. Refer to the &lt;a href=&#34;https://kubernetes.io/docs/concepts/containers/runtime-class/#runtime-class&#34; target=&#34;_blank&#34;&gt;RuntimeClass documentation&lt;/a&gt; for how to get it running.&lt;/li&gt;
&lt;li&gt;Check out the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/keps/sig-node/0014-runtime-class.md&#34; target=&#34;_blank&#34;&gt;RuntimeClass Kubernetes Enhancement Proposal&lt;/a&gt; for more nitty-gritty design details.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview&#34; target=&#34;_blank&#34;&gt;Sandbox Isolation Level Decision&lt;/a&gt; documents the thought process that initially went into making RuntimeClass a pod-level choice.&lt;/li&gt;
&lt;li&gt;Join the discussions and help shape the future of RuntimeClass with the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-node&#34; target=&#34;_blank&#34;&gt;SIG-Node community&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Volume Snapshot Alpha for Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/</link>
      <pubDate>Tue, 09 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/09/introducing-volume-snapshot-alpha-for-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jing Xu (Google) Xing Yang (Huawei), Saad Ali (Google)&lt;/p&gt;

&lt;p&gt;Kubernetes v1.12 introduces alpha support for volume snapshotting. This feature allows creating/deleting volume snapshots, and the ability to create new volumes from a snapshot natively using the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;what-is-a-snapshot&#34;&gt;What is a Snapshot?&lt;/h2&gt;

&lt;p&gt;Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a &amp;ldquo;snapshot&amp;rdquo; of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore the existing volume to a previous state (represented by the snapshot).&lt;/p&gt;

&lt;h2 id=&#34;why-add-snapshots-to-kubernetes&#34;&gt;Why add Snapshots to Kubernetes?&lt;/h2&gt;

&lt;p&gt;The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.&lt;/p&gt;

&lt;p&gt;Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no “cluster specific” knowledge.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage SIG&lt;/a&gt; identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.&lt;/p&gt;

&lt;p&gt;By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).&lt;/p&gt;

&lt;p&gt;Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.&lt;/p&gt;

&lt;p&gt;Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: such as data protection, data replication, and data migration.&lt;/p&gt;

&lt;h2 id=&#34;which-volume-plugins-support-kubernetes-snapshots&#34;&gt;Which volume plugins support Kubernetes Snapshots?&lt;/h2&gt;

&lt;p&gt;Kubernetes supports three types of volume plugins: in-tree, Flex, and CSI. See &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34; target=&#34;_blank&#34;&gt;Kubernetes Volume Plugin FAQ&lt;/a&gt; for details.&lt;/p&gt;

&lt;p&gt;Snapshots are only supported for CSI drivers (not for in-tree or Flex). To use the Kubernetes snapshots feature, ensure that a CSI Driver that implements snapshots is deployed on your cluster.&lt;/p&gt;

&lt;p&gt;As of the publishing of this blog, the following CSI drivers support snapshots:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34; target=&#34;_blank&#34;&gt;GCE Persistent Disk CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/opensds/nbp/tree/master/csi/server&#34; target=&#34;_blank&#34;&gt;OpenSDS CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ceph/ceph-csi/tree/master/pkg/rbd&#34; target=&#34;_blank&#34;&gt;Ceph RBD CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/libopenstorage/openstorage/tree/master/csi&#34; target=&#34;_blank&#34;&gt;Portworx CSI Driver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Snapshot support for other &lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;drivers&lt;/a&gt; is pending, and should be available soon. Read the “&lt;a href=&#34;https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/&#34; target=&#34;_blank&#34;&gt;Container Storage Interface (CSI) for Kubernetes Goes Beta&lt;/a&gt;” blog post to learn more about CSI and how to deploy CSI drivers.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-snapshots-api&#34;&gt;Kubernetes Snapshots API&lt;/h2&gt;

&lt;p&gt;Similar to the API for managing Kubernetes Persistent Volumes, Kubernetes Volume Snapshots introduce three new API objects for managing snapshots:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Created by a Kubernetes user to request creation of a snapshot for a specified volume. It contains information about the snapshot operation such as the timestamp when the snapshot was taken and whether the snapshot is ready to use.&lt;/li&gt;
&lt;li&gt;Similar to the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object, the creation and deletion of this object represents a user desire to create or delete a cluster resource (a snapshot).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotContent&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Created by the CSI volume driver once a snapshot has been successfully created. It contains information about the snapshot including snapshot ID.&lt;/li&gt;
&lt;li&gt;Similar to the &lt;code&gt;PersistentVolume&lt;/code&gt; object, this object represents a provisioned resource on the cluster (a snapshot).&lt;/li&gt;
&lt;li&gt;Like &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; and &lt;code&gt;PersistentVolume&lt;/code&gt; objects, once a snapshot is created, the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object binds to the VolumeSnapshot for which it was created (with a one-to-one mapping).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshotClass&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Created by cluster administrators to describe how snapshots should be created. including the driver information, the secrets to access the snapshot, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to note that unlike the core Kubernetes Persistent Volume objects, these Snapshot objects are defined as &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions&#34; target=&#34;_blank&#34;&gt;CustomResourceDefinitions (CRDs)&lt;/a&gt;. The Kubernetes project is moving away from having resource types pre-defined in the API server, and is moving towards a model where the API server is independent of the API objects. This allows the API server to be reused for projects other than Kubernetes, and consumers (like Kubernetes) can simply install the resource types they require as CRDs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes-csi.github.io/docs/Drivers.html&#34; target=&#34;_blank&#34;&gt;CSI Drivers&lt;/a&gt; that support snapshots will automatically install the required CRDs. Kubernetes end users only need to verify that a CSI driver that supports snapshots is deployed on their Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;In addition to these new objects, a new, DataSource field has been added to the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type PersistentVolumeClaimSpec struct {
    AccessModes []PersistentVolumeAccessMode
    Selector *metav1.LabelSelector
    Resources ResourceRequirements
    VolumeName string
    StorageClassName *string
    VolumeMode *PersistentVolumeMode
    DataSource *TypedLocalObjectReference
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This new alpha field enables a new volume to be created and automatically pre-populated with data from an existing snapshot.&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-snapshots-requirements&#34;&gt;Kubernetes Snapshots Requirements&lt;/h2&gt;

&lt;p&gt;Before using Kubernetes Volume Snapshotting, you must:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ensure a CSI driver implementing snapshots is deployed and running on your Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Enable the Kubernetes Volume Snapshotting feature via new Kubernetes feature gate (disabled by default for alpha):

&lt;ul&gt;
&lt;li&gt;Set the following flag on the API server binary: &lt;code&gt;--feature-gates=VolumeSnapshotDataSource=true&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before creating a snapshot, you also need to specify CSI driver information for snapshots by creating a &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; object and setting the &lt;code&gt;snapshotter&lt;/code&gt; field to point to your CSI driver. In the example of &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; below, the CSI driver is &lt;code&gt;com.example.csi-driver&lt;/code&gt;. You need at least one &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; object per snapshot provisioner. You can also set a default &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; for each individual CSI driver by putting an annotation &lt;code&gt;snapshot.storage.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&lt;/code&gt; in the class definition.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotClass
metadata:
  name: default-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;
snapshotter: com.example.csi-driver


apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotClass
metadata:
  name: csi-snapclass
snapshotter: com.example.csi-driver
parameters:
  fakeSnapshotOption: foo
  csiSnapshotterSecretName: csi-secret
  csiSnapshotterSecretNamespace: csi-namespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You must set any required opaque parameters based on the documentation for your CSI driver. As the example above shows,  the parameter &lt;code&gt;fakeSnapshotOption: foo&lt;/code&gt; and any referenced secret(s) will be passed to CSI driver during snapshot creation and deletion. The &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;default CSI external-snapshotter&lt;/a&gt; reserves the parameter keys &lt;code&gt;csiSnapshotterSecretName&lt;/code&gt; and &lt;code&gt;csiSnapshotterSecretNamespace&lt;/code&gt;. If specified, it fetches the secret and passes it to the CSI driver when creating and deleting a snapshot.&lt;/p&gt;

&lt;p&gt;And finally, before creating a snapshot, you must provision a volume using your CSI driver and populate it with some data that you want to snapshot (see the &lt;a href=&#34;https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/&#34; target=&#34;_blank&#34;&gt;CSI blog post&lt;/a&gt; on how to create and use CSI volumes).&lt;/p&gt;

&lt;h2 id=&#34;creating-a-new-snapshot-with-kubernetes&#34;&gt;Creating a new Snapshot with Kubernetes&lt;/h2&gt;

&lt;p&gt;Once a &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; object is defined and you have a volume you want to snapshot, you may create a new snapshot by creating a &lt;code&gt;VolumeSnapshot&lt;/code&gt; object.&lt;/p&gt;

&lt;p&gt;The source of the snapshot specifies the volume to create a snapshot from. It has two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt; - must be &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; - the PVC API object name&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The namespace of the volume to snapshot is assumed to be the same as the namespace of the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-demo
  namespace: demo-namespace
spec:
  snapshotClassName: csi-snapclass
  source:
    name: mypvc
    kind: PersistentVolumeClaim
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the &lt;code&gt;VolumeSnapshot&lt;/code&gt; spec, user can specify the &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; which has the information about which CSI driver should be used for creating the snapshot . When the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is created, the parameter &lt;code&gt;fakeSnapshotOption: foo&lt;/code&gt; and any referenced secret(s) from the &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; are passed to the CSI plugin &lt;code&gt;com.example.csi-driver&lt;/code&gt; via a &lt;code&gt;CreateSnapshot&lt;/code&gt; call.&lt;/p&gt;

&lt;p&gt;In response, the CSI driver triggers a snapshot of the volume and then automatically creates a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object to represent the new snapshot, and binds the new &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object to the &lt;code&gt;VolumeSnapshot&lt;/code&gt;, making it ready to use. If the  CSI driver fails to create the snapshot and returns error, the snapshot controller reports the error in the status of &lt;code&gt;VolumeSnapshot&lt;/code&gt; object and does not retry (this is different from other controllers in Kubernetes, and is to prevent snapshots from being taken at an unexpected time).&lt;/p&gt;

&lt;p&gt;If a snapshot class is not specified, the external snapshotter will try to find and set a default snapshot class for the snapshot. The &lt;code&gt;CSI driver&lt;/code&gt; specified by &lt;code&gt;snapshotter&lt;/code&gt; in the default snapshot class must match the &lt;code&gt;CSI driver&lt;/code&gt; specified by the &lt;code&gt;provisioner&lt;/code&gt; in the storage class of the PVC.&lt;/p&gt;

&lt;p&gt;Please note that the alpha release of Kubernetes Snapshot does not provide any consistency guarantees. You have to prepare your application (pause application, freeze filesystem etc.) before taking the snapshot for data consistency.&lt;/p&gt;

&lt;p&gt;You can verify that the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is created and bound with &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; by running &lt;code&gt;kubectl describe volumesnapshot&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Ready&lt;/code&gt; should be set to true under &lt;code&gt;Status&lt;/code&gt; to indicate this volume snapshot is ready for use.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Creation Time&lt;/code&gt; field indicates when the snapshot is actually created (cut).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Restore Size&lt;/code&gt; field indicates the minimum volume size when restoring a volume from the snapshot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Snapshot Content Name&lt;/code&gt; field in the &lt;code&gt;spec&lt;/code&gt; points to the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object created for this snapshot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;importing-an-existing-snapshot-with-kubernetes&#34;&gt;Importing an existing snapshot with Kubernetes&lt;/h2&gt;

&lt;p&gt;You can always import an existing snapshot to Kubernetes by manually creating a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object to represent the existing snapshot. Because &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; is a non-namespace API object, only a system admin may have the permission to create it. Once a &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object is created, the user can create a &lt;code&gt;VolumeSnapshot&lt;/code&gt; object pointing to the &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object. The external-snapshotter controller will mark snapshot as ready after verifying the snapshot exists and the binding between &lt;code&gt;VolumeSnapshot&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; objects is correct. Once bound, the snapshot is ready to use in Kubernetes.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; object should be created with the following fields to represent a pre-provisioned snapshot:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;csiVolumeSnapshotSource&lt;/code&gt; - Snapshot identifying information.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;snapshotHandle&lt;/code&gt; - name/identifier of the snapshot. This field is required.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;driver&lt;/code&gt; - CSI driver used to handle this volume. This field is required. It must match the snapshotter name in the snapshot controller.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;creationTime&lt;/code&gt; and &lt;code&gt;restoreSize&lt;/code&gt; - these fields are not required for pre-provisioned volumes. The external-snapshotter controller will automatically update them after creation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;volumeSnapshotRef&lt;/code&gt; - Pointer to the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object this object should bind to.

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; and &lt;code&gt;namespace&lt;/code&gt; -  It specifies the name and namespace of the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object which the content is bound to.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UID&lt;/code&gt; - these fields are not required for pre-provisioned volumes.The external-snapshotter controller will update the field automatically after binding. If user specifies UID field, he/she must make sure that it matches with the binding snapshot’s UID.  If the specified UID does not match the binding snapshot’s UID, the content is considered an orphan object and the controller will delete it and its associated snapshot.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snapshotClassName&lt;/code&gt; - This field is optional. The external-snapshotter controller will update the field automatically after binding.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshotContent
metadata:
  name: static-snapshot-content
spec:
  csiVolumeSnapshotSource:
    driver: com.example.csi-driver
    snapshotHandle: snapshotcontent-example-id
  volumeSnapshotRef:
    kind: VolumeSnapshot
    name: static-snapshot-demo
    namespace: demo-namespace
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A &lt;code&gt;VolumeSnapshot&lt;/code&gt; object should be created to allow a user to use the snapshot:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;snapshotClassName&lt;/code&gt; - name of the volume snapshot class. This field is optional. If set, the snapshotter field in the snapshot class must match the snapshotter name of the snapshot controller. If not set, the snapshot controller will try to find a default snapshot class.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snapshotContentName&lt;/code&gt; - name of the volume snapshot content. This field is required for pre-provisioned volumes.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: snapshot.storage.k8s.io/v1alpha1
kind: VolumeSnapshot
metadata:
  name: static-snapshot-demo
  namespace: demo-namespace
spec:
  snapshotClassName: csi-snapclass
  snapshotContentName: static-snapshot-content
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once these objects are created, the snapshot controller will bind them together, and set the field Ready (under &lt;code&gt;Status&lt;/code&gt;) to True to indicate the snapshot is ready to use.&lt;/p&gt;

&lt;h2 id=&#34;provision-a-new-volume-from-a-snapshot-with-kubernetes&#34;&gt;Provision a new volume from a snapshot with Kubernetes&lt;/h2&gt;

&lt;p&gt;To provision a new volume pre-populated with data from a snapshot object, use the new dataSource field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;. It has three parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;name - name of the &lt;code&gt;VolumeSnapshot&lt;/code&gt; object representing the snapshot to use as source&lt;/li&gt;
&lt;li&gt;kind - must be &lt;code&gt;VolumeSnapshot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;apiGroup - must be &lt;code&gt;snapshot.storage.k8s.io&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The namespace of the source &lt;code&gt;VolumeSnapshot&lt;/code&gt; object is assumed to be the same as the namespace of the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-restore
  Namespace: demo-namespace
spec:
  storageClassName: csi-storageclass
  dataSource:
    name: new-snapshot-demo
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object is created, it will trigger provisioning of a new volume that is pre-populated with data from the specified snapshot.&lt;/p&gt;

&lt;h2 id=&#34;as-a-storage-vendor-how-do-i-add-support-for-snapshots-to-my-csi-driver&#34;&gt;As a storage vendor, how do I add support for snapshots to my CSI driver?&lt;/h2&gt;

&lt;p&gt;To implement the snapshot feature, a CSI driver MUST add support for additional controller capabilities &lt;code&gt;CREATE_DELETE_SNAPSHOT&lt;/code&gt; and &lt;code&gt;LIST_SNAPSHOTS&lt;/code&gt;, and implement additional controller RPCs: &lt;code&gt;CreateSnapshot&lt;/code&gt;, &lt;code&gt;DeleteSnapshot&lt;/code&gt;, and &lt;code&gt;ListSnapshots&lt;/code&gt;. For details, see &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;the CSI spec&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although Kubernetes is as &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#third-party-csi-volume-drivers&#34; target=&#34;_blank&#34;&gt;minimally prescriptive&lt;/a&gt; on the packaging and deployment of a CSI Volume Driver as possible, it provides a &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes&#34; target=&#34;_blank&#34;&gt;suggested mechanism&lt;/a&gt; for deploying an arbitrary containerized CSI driver on Kubernetes to simplify deployment of containerized CSI compatible volume drivers.&lt;/p&gt;

&lt;p&gt;As part of this recommended deployment process, the Kubernetes team provides a number of sidecar (helper) containers, including a new &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter&#34; target=&#34;_blank&#34;&gt;external-snapshotter&lt;/a&gt; sidecar container.&lt;/p&gt;

&lt;p&gt;The external-snapshotter watches the Kubernetes API server for &lt;code&gt;VolumeSnapshot&lt;/code&gt; and &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; objects and triggers CreateSnapshot and DeleteSnapshot operations against a CSI endpoint. The CSI &lt;a href=&#34;https://github.com/kubernetes-csi/external-provisioner&#34; target=&#34;_blank&#34;&gt;external-provisioner&lt;/a&gt; sidecar container has also been updated to support restoring volume from snapshot using the new &lt;code&gt;dataSource&lt;/code&gt; PVC field.&lt;/p&gt;

&lt;p&gt;In order to support snapshot feature, it is recommended that storage vendors deploy the external-snapshotter sidecar containers in addition to the external provisioner the external attacher, along with their CSI driver in a statefulset as shown in the following diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-09-volume-snapshot-alpha/snapshot.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/blob/e011fe31df548813d2eb6dacb278c0ca58533b34/deploy/kubernetes/setup-csi-snapshotter.yaml&#34; target=&#34;_blank&#34;&gt;example deployment yaml&lt;/a&gt; file, two sidecar containers, the external provisioner and the external snapshotter, and CSI drivers are deployed together with the hostpath CSI plugin in the statefulset pod. Hostpath CSI plugin is a sample plugin, not for production.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-limitations-of-alpha&#34;&gt;What are the limitations of alpha?&lt;/h2&gt;

&lt;p&gt;The alpha implementation of snapshots for Kubernetes has the following limitations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does not support reverting an existing volume to an earlier state represented by a snapshot (alpha only supports provisioning a new volume from a snapshot).&lt;/li&gt;
&lt;li&gt;Does not support “in-place restore” of an existing PersistentVolumeClaim from a snapshot: i.e. provisioning a new volume from a snapshot, but updating an existing PersistentVolumeClaim to point to the new volume and effectively making the PVC appear to revert to an earlier state (alpha only supports using a new volume provisioned from a snapshot via a new PV/PVC).&lt;/li&gt;
&lt;li&gt;No snapshot consistency guarantees beyond any guarantees provided by storage system (e.g. crash consistency).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h2&gt;

&lt;p&gt;Depending on feedback and adoption, the Kubernetes team plans to push the CSI Snapshot implementation to beta in either 1.13 or 1.14.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Check out additional documentation on the snapshot feature here: &lt;a href=&#34;http://k8s.io/docs/concepts/storage/volume-snapshots&#34; target=&#34;_blank&#34;&gt;http://k8s.io/docs/concepts/storage/volume-snapshots&lt;/a&gt; and &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34; target=&#34;_blank&#34;&gt;https://kubernetes-csi.github.io/docs/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-get-involved&#34;&gt;How do I get involved?&lt;/h2&gt;

&lt;p&gt;This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p&gt;

&lt;p&gt;In addition to the contributors who have been working on the Snapshot feature:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Xing Yang (&lt;a href=&#34;https://github.com/xing-yang&#34; target=&#34;_blank&#34;&gt;xing-yang&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jing Xu (&lt;a href=&#34;https://github.com/jingxu97&#34; target=&#34;_blank&#34;&gt;jingxu97&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Huamin Chen (&lt;a href=&#34;https://github.com/rootfs&#34; target=&#34;_blank&#34;&gt;rootfs&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Tomas Smetana (&lt;a href=&#34;https://github.com/tsmetana&#34; target=&#34;_blank&#34;&gt;tsmetana&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Shiwei Xu (&lt;a href=&#34;https://github.com/wackxu&#34; target=&#34;_blank&#34;&gt;wackxu&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We offer a huge thank you to all the contributors in Kubernetes Storage SIG and CSI community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Saad Ali (&lt;a href=&#34;https://github.com/saadali&#34; target=&#34;_blank&#34;&gt;saadali&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Tim Hockin (&lt;a href=&#34;https://github.com/thockin&#34; target=&#34;_blank&#34;&gt;thockin&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jan Šafránek (&lt;a href=&#34;https://github.com/jsafrane&#34; target=&#34;_blank&#34;&gt;jsafrane&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Luis Pabon (&lt;a href=&#34;https://github.com/lpabon&#34; target=&#34;_blank&#34;&gt;lpabon&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Jordan Liggitt (&lt;a href=&#34;https://github.com/liggitt&#34; target=&#34;_blank&#34;&gt;liggitt&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;David Zhu (&lt;a href=&#34;https://github.com/davidz627&#34; target=&#34;_blank&#34;&gt;davidz627&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Garth Bushell (&lt;a href=&#34;https://github.com/garthy&#34; target=&#34;_blank&#34;&gt;garthy&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Ardalan Kangarlou (&lt;a href=&#34;https://github.com/kangarlou&#34; target=&#34;_blank&#34;&gt;kangarlou&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Seungcheol Ko (&lt;a href=&#34;https://github.com/sngchlko&#34; target=&#34;_blank&#34;&gt;sngchlko&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Michelle Au (&lt;a href=&#34;https://github.com/msau42&#34; target=&#34;_blank&#34;&gt;msau42&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Humble Devassy Chirammal (&lt;a href=&#34;https://github.com/humblec&#34; target=&#34;_blank&#34;&gt;humblec&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Vladimir Vivien (&lt;a href=&#34;https://github.com/vladimirvivien&#34; target=&#34;_blank&#34;&gt;vladimirvivien&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;John Griffith (&lt;a href=&#34;https://github.com/j-griffith&#34; target=&#34;_blank&#34;&gt;j-griffith&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Bradley Childs (&lt;a href=&#34;https://github.com/childsb&#34; target=&#34;_blank&#34;&gt;childsb&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Ben Swartzlander (&lt;a href=&#34;https://github.com/bswartz&#34; target=&#34;_blank&#34;&gt;bswartz&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you’re interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-storage&#34; target=&#34;_blank&#34;&gt;Kubernetes Storage Special Interest Group&lt;/a&gt; (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Support for Azure VMSS,  Cluster-Autoscaler and User Assigned Identity</title>
      <link>https://docstest.github.io/blog/2018/10/08/support-for-azure-vmss-cluster-autoscaler-and-user-assigned-identity/</link>
      <pubDate>Mon, 08 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/08/support-for-azure-vmss-cluster-autoscaler-and-user-assigned-identity/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/kkwriting&#34; target=&#34;_blank&#34;&gt;Krishnakumar R (KK)&lt;/a&gt; (Microsoft), &lt;a href=&#34;https://twitter.com/feisky&#34; target=&#34;_blank&#34;&gt;Pengfei Ni&lt;/a&gt; (Microsoft)&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With Kubernetes v1.12, Azure virtual machine scale sets (VMSS) and cluster-autoscaler have reached their General Availability (GA) and User Assigned Identity is available as a preview feature.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Azure VMSS allow you to create and manage identical, load balanced VMs that automatically increase or decrease based on demand or a set schedule. This enables you to easily manage and scale multiple VMs to provide high availability and application resiliency, ideal for large-scale applications like container workloads &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cluster autoscaler allows you to adjust the size of the Kubernetes clusters based on the load conditions automatically.&lt;/p&gt;

&lt;p&gt;Another exciting feature which v1.12 brings to the table is the ability to use User Assigned Identities with Kubernetes clusters &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&#34; target=&#34;_blank&#34;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this article, we will do a brief overview of VMSS, cluster autoscaler and user assigned identity features on Azure.&lt;/p&gt;

&lt;h2 id=&#34;vmss&#34;&gt;VMSS&lt;/h2&gt;

&lt;p&gt;Azure’s Virtual Machine Scale sets (VMSS) feature offers users an ability to automatically create VMs from a single central configuration, provide load balancing via L4 and L7 load balancing, provide a path to use availability zones for high availability, provides large-scale VM instances et. al.&lt;/p&gt;

&lt;p&gt;VMSS consists of a group of virtual machines, which are identical and can be managed and configured at a group level. More details of this feature in Azure itself can be found at the following link &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview&#34; target=&#34;_blank&#34;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With Kubernetes v1.12 customers can create k8s cluster out of VMSS instances and utilize VMSS features.&lt;/p&gt;

&lt;h2 id=&#34;cluster-components-on-azure&#34;&gt;Cluster components on Azure&lt;/h2&gt;

&lt;p&gt;Generally, standalone Kubernetes cluster in Azure consists of the following parts&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute - the VM itself and its properties.&lt;/li&gt;
&lt;li&gt;Networking - this includes the IPs and load balancers.&lt;/li&gt;
&lt;li&gt;Storage - the disks which are associated with the VMs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;compute&#34;&gt;Compute&lt;/h2&gt;

&lt;p&gt;Compute in cloud k8s cluster consists of the VMs. These VMs are created by provisioning tools such as acs-engine or AKS (in case of managed service). Eventually, they run various system daemons such as kubelet, kube-api server etc. either as a process (in some versions) or as a docker container.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-08-support-for-azure-vmss/sample-azure-cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;networking&#34;&gt;Networking&lt;/h2&gt;

&lt;p&gt;In Azure Kubernetes cluster various networking components are brought together to provide features required for users. Typically they consist of the network interfaces, network security groups, public IP resource, VNET (virtual networks), load balancers etc.&lt;/p&gt;

&lt;h2 id=&#34;storage&#34;&gt;Storage&lt;/h2&gt;

&lt;p&gt;Kubernetes clusters are built on top of disks created in Azure. In a typical configuration, we have managed disks which are used to hold the regular OS images and a separate disk is used for etcd.&lt;/p&gt;

&lt;h2 id=&#34;cloud-provider-components&#34;&gt;Cloud provider components&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-08-support-for-azure-vmss/cloud-provider-components.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes cloud provider interface provides interactions with clouds for managing cloud-specific resources, e.g. public IPs and routes. A good overview of these components is given in &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&#34; target=&#34;_blank&#34;&gt;[2]&lt;/a&gt;. In case of Azure Kubernetes cluster, the Kubernetes interactions go through the Azure cloud provider layer and contact the various services running in the cloud.&lt;/p&gt;

&lt;p&gt;The cloud provider implementation of K8s can be largely divided into the following component interfaces which we need to implement:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Load Balancer&lt;/li&gt;
&lt;li&gt;Instances&lt;/li&gt;
&lt;li&gt;Zones&lt;/li&gt;
&lt;li&gt;Routes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In addition to the above interfaces, the storage services from the cloud provider is linked via the volume plugin layer.&lt;/p&gt;

&lt;h2 id=&#34;azure-cloud-provider-implementation-and-vmss&#34;&gt;Azure cloud provider implementation and VMSS&lt;/h2&gt;

&lt;p&gt;In the Azure cloud provider, for every type of cluster we implement, there is a VMType option which we specify. In case of VMSS, the VM type is “vmss”.  The provisioning software (acs-engine, in future AKS etc.) would setup these values in /etc/kubernetes/azure.json file. Based on this type, various implementations would get instantiated &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/azure_vmss.go&#34; target=&#34;_blank&#34;&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The load balancer interface provides access to the underlying cloud provider load balancer service. The information about the load balancers and the control operations on them are required for Kubernetes to handle the services which gets hosted on the Kubernetes cluster. For VMSS support the changes ensure that the VMSS instances are part of the load balancer pool as required.&lt;/p&gt;

&lt;p&gt;The instances interfaces help the cloud controller to get various details about a node from the cloud provider layer. For example, the details of a node like the IP address, the instance id etc, is obtained by the controller by means of the instances interfaces which the cloud provider layer registers with it. In case of VMSS support, we talk to VMSS service to gather information regarding the instances.&lt;/p&gt;

&lt;p&gt;The zones interfaces help the cloud controller to get zone information for each node. Scheduler could spread pods to different availability zones with such information. It is also required for supporting topology aware dynamic provisioning features, e.g. AzureDisk. Each VMSS instances will be labeled with its current zone and region.&lt;/p&gt;

&lt;p&gt;The routes interfaces help the cloud controller to setup advanced routes for Pod network. For example, a route with prefix node’s podCIDR and next hop node’s internal IP will be set for each node. In case of VMSS support, the next hops are VMSS virtual machines’ internal IP address.&lt;/p&gt;

&lt;p&gt;The Azure volume plugin interfaces have been modified for VMSS to work properly. For example, the attach/detach to the AzureDisk have been modified to perform these operations at VMSS instance level.&lt;/p&gt;

&lt;h2 id=&#34;setting-up-a-vmss-cluster-on-azure&#34;&gt;Setting up a VMSS cluster on Azure&lt;/h2&gt;

&lt;p&gt;The following link &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md&#34; target=&#34;_blank&#34;&gt;[4]&lt;/a&gt; provides an example of acs-engine to create a Kubernetes cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acs-engine deploy --subscription-id &amp;lt;subscription id&amp;gt; \
    --dns-prefix &amp;lt;dns&amp;gt; --location &amp;lt;location&amp;gt; \
    --api-model examples/kubernetes.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;API model file provides various configurations which acs-engine uses to create a cluster. The API model here &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/master/examples/kubernetes-vmss/kubernetes.json&#34; target=&#34;_blank&#34;&gt;[5]&lt;/a&gt; gives a good starting configuration to setup the VMSS cluster.&lt;/p&gt;

&lt;p&gt;Once a VMSS cluster is created, here are some of the steps you can run to understand more about the cluster setup. Here is the output of kubectl get nodes from a cluster created using the above command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
NAME                                 STATUS    ROLES     AGE       VERSION
k8s-agentpool1-92998111-vmss000000   Ready     agent     1h        v1.12.0-rc.2
k8s-agentpool1-92998111-vmss000001   Ready     agent     1h        v1.12.0-rc.2
k8s-master-92998111-0                Ready     master    1h        v1.12.0-rc.2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This cluster consists of two worker nodes and one master. Now how do we check which node is which in Azure parlance? In VMSS listing, we can see a single VMSS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vmss list -o table -g k8sblogkk1
Name                          ResourceGroup    Location    Zones      Capacity  Overprovision    UpgradePolicy
----------------------------  ---------------  ----------  -------  ----------  ---------------  ---------------
k8s-agentpool1-92998111-vmss  k8sblogkk1       westus2                       2  False            Manual
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nodes which we see as agents (in the kubectl get nodes command) are part of this vmss. We can use the following command to list the instances which are part of the VM scale set:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ az vmss list-instances -g k8sblogkk1 -n k8s-agentpool1-92998111-vmss -o table
  InstanceId  LatestModelApplied    Location    Name                            ProvisioningState    ResourceGroup    VmId
------------  --------------------  ----------  ------------------------------  -------------------  ---------------  ------------------------------------
           0  True                  westus2     k8s-agentpool1-92998111-vmss_0  Succeeded            K8SBLOGKK1       21c57d6c-9c8f-4a62-970f-63ed0fcba53f
           1  True                  westus2     k8s-agentpool1-92998111-vmss_1  Succeeded            K8SBLOGKK1       840743b9-0076-4a2e-920e-5ba9da296665
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The node name does not match the name in the vm scale set, but if we run the following command to list the providerID we can find the matching node which resembles the instance name:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$  kubectl describe nodes k8s-agentpool1-92998111-vmss000000| grep ProviderID
ProviderID:                  azure:///subscriptions/&amp;lt;subscription id&amp;gt;/resourceGroups/k8sblogkk1/providers/Microsoft.Compute/virtualMachineScaleSets/k8s-agentpool1-92998111-vmss/virtualMachines/0
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;current-status-and-future&#34;&gt;Current Status and Future&lt;/h2&gt;

&lt;p&gt;Currently the following is supported:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;VMSS master nodes and worker nodes&lt;/li&gt;
&lt;li&gt;VMSS on worker nodes and Availability set on master nodes combination.&lt;/li&gt;
&lt;li&gt;Per vm disk attach&lt;/li&gt;
&lt;li&gt;Azure Disk &amp;amp; Azure File support&lt;/li&gt;
&lt;li&gt;Availability zones (Alpha)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In future there will be support for the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;AKS with VMSS support&lt;/li&gt;
&lt;li&gt;Per VM instance public IP&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;cluster-autoscaler&#34;&gt;Cluster Autoscaler&lt;/h2&gt;

&lt;p&gt;A Kubernetes cluster consists of nodes. These nodes can be virtual machines, bare metal servers or could be even virtual node (virtual kubelet). To avoid getting lost in permutations and combinations of Kubernetes ecosystem ;-), let&amp;rsquo;s consider that the cluster we are discussing consists of virtual machines, which are hosted in a cloud (eg: Azure, Google or AWS). What this effectively means is that you have access to virtual machines which run Kubernetes agents and a master node which runs k8s services like API server. A detailed version of k8s architecture can be found here &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/&#34; target=&#34;_blank&#34;&gt;[11]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The number of nodes which are required on a cluster depends on the workload on the cluster. When the load goes up there is a need to increase the nodes and when it subsides, there is a need to reduce the nodes and clean up the resources which are no longer in use. One way this can be taken care of is to manually scale up the nodes which are part of the Kubernetes cluster and manually scale down when the demand reduces. But shouldn’t this be done automatically ? Answer to this question is the Cluster Autoscaler (CA).&lt;/p&gt;

&lt;p&gt;The cluster autoscaler itself runs as a pod within the kubernetes cluster. The following figure illustrates the high level view of the setup with respect to the k8s cluster:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-08-support-for-azure-vmss/cluster-autoscaler.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Since Cluster Autoscaler is a pod within the k8s cluster, it can use the in-cluster config and the Kubernetes go client &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;[10]&lt;/a&gt; to contact the API server.&lt;/p&gt;

&lt;h2 id=&#34;internals&#34;&gt;Internals&lt;/h2&gt;

&lt;p&gt;The API server is the central service which manages the state of the k8s cluster utilizing a backing store (an etcd database), runs on the management node or runs within the cloud (in case of managed service such as AKS). For any component within the Kubernetes cluster to figure out the state of the cluster, like for example the nodes registered in the cluster, contacting the API server is the way to go.&lt;/p&gt;

&lt;p&gt;In order to simplify our discussion let’s divide the CA functionality into 3 parts as given below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-08-support-for-azure-vmss/ca-functionality.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The main portion of the CA is a control loop which keeps running at every scan interval. This loop is responsible for updating the autoscaler metrics and health probes. Before this loop is entered auto scaler performs various operations such as claiming the leader state after performing a Kubernetes leader election. The main loop initializes static autoscaler component. This component initializes the underlying cloud provider based on the parameters passed onto the CA.&lt;/p&gt;

&lt;p&gt;Various operations performed by the CA to manage the state of the cluster is passed onto the cloud provider component. Some examples like - increase target size, decrease target size etc, results in the cloud provider component talking to the cloud services internally and performing operations such as adding a node or deleting a node. These operations are performed on group of nodes in the cluster. The static autoscaler also keeps tab on the state of the system by querying the API server - operations such as list pods and list nodes are used to get hold of such information.&lt;/p&gt;

&lt;p&gt;The decision to make a scale up is based on pods which remain unscheduled and a variety of checks and balances. The nodes which are free to be scaled down are deleted from the cluster and deleted from the cloud itself. The cluster autoscaler applies checks and balances before scaling up and scaling down - for example the nodes which have been recently added are given special consideration. During the deletion the nodes are drained to ensure that no disruption happens to the running pods.&lt;/p&gt;

&lt;h2 id=&#34;setting-up-ca-on-azure&#34;&gt;Setting up CA on Azure:&lt;/h2&gt;

&lt;p&gt;Cluster Autoscaler is available as an add-on with acs-engine. The following link &lt;a href=&#34;https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler&#34; target=&#34;_blank&#34;&gt;[15]&lt;/a&gt; has an example configuration file used to deploy autoscaler with acs-engine.  The following link &lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md&#34; target=&#34;_blank&#34;&gt;[8]&lt;/a&gt; provides details on manual step by step way to do the same.&lt;/p&gt;

&lt;p&gt;In acs-engine case we use the regular command line to deploy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acs-engine deploy --subscription-id &amp;lt;subscription id&amp;gt; \
    --dns-prefix &amp;lt;dns&amp;gt; --location &amp;lt;location&amp;gt; \
    --api-model examples/kubernetes.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main difference are the following lines in the config file at &lt;a href=&#34;https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler&#34; target=&#34;_blank&#34;&gt;[15]&lt;/a&gt; makes sure that CA is deployed as an addon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;addons&amp;quot;: [
          {
            &amp;quot;name&amp;quot;: &amp;quot;cluster-autoscaler&amp;quot;,
            &amp;quot;enabled&amp;quot;: true,
            &amp;quot;config&amp;quot;: {
              &amp;quot;minNodes&amp;quot;: &amp;quot;1&amp;quot;,
              &amp;quot;maxNodes&amp;quot;: &amp;quot;5&amp;quot;
            }
          }
        ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The config section in the json above can be used to provide the configuration to the cluster autoscaler pod, eg: min and max nodes as above.&lt;/p&gt;

&lt;p&gt;Once the setup completes we can see that the cluster-autoscaler pod is deployed in the system namespace:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl get pods -n kube-system  | grep autoscaler
cluster-autoscaler-7bdc74d54c-qvbjs             1/1       Running             1          6m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the output from the CA configmap and events from a sample cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$kubectl -n kube-system describe configmap cluster-autoscaler-status
Name:         cluster-autoscaler-status
Namespace:    kube-system
Labels:       &amp;lt;none&amp;gt;
Annotations:  cluster-autoscaler.kubernetes.io/last-updated=2018-10-02 01:21:17.850010508 +0000 UTC

Data
====
status:
----
Cluster-autoscaler status at 2018-10-02 01:21:17.850010508 +0000 UTC:
Cluster-wide:
  Health:      Healthy (ready=3 unready=0 notStarted=0 longNotStarted=0 registered=3 longUnregistered=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleUp:     NoActivity (ready=3 registered=3)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:39:50.493307405 +0000 UTC m=+674.133759650

NodeGroups:
  Name:        k8s-agentpool1-92998111-vmss
  Health:      Healthy (ready=2 unready=0 notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=5))
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleUp:     NoActivity (ready=2 cloudProviderTarget=2)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:28:49.944222739 +0000 UTC m=+13.584675084
  ScaleDown:   NoCandidates (candidates=0)
               LastProbeTime:      2018-10-02 01:21:17.772229859 +0000 UTC m=+3161.412682204
               LastTransitionTime: 2018-10-02 00:39:50.493307405 +0000 UTC m=+674.133759650


Events:
  Type    Reason          Age   From                Message
  ----    ------          ----  ----                -------
  Normal  ScaleDownEmpty  42m   cluster-autoscaler  Scale-down: removing empty node k8s-agentpool1-92998111-vmss000002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As can be seen the events, the cluster autoscaler scaled down and deleted a node as there was no load on this cluster. The rest of the configmap in this case indicates that there are no further actions which the autoscaler is taking at this moment.&lt;/p&gt;

&lt;h2 id=&#34;current-status-and-future-1&#34;&gt;Current status and future:&lt;/h2&gt;

&lt;p&gt;Cluster Autoscaler currently supports four VM types: standard (VMAS), VMSS, ACS and AKS. In the future, Cluster Autoscaler will be integrated within AKS product, so that users can enable it by one-click.&lt;/p&gt;

&lt;h2 id=&#34;user-assigned-identity&#34;&gt;User Assigned Identity&lt;/h2&gt;

&lt;p&gt;Inorder for the Kubernetes cluster components to securely talk to the cloud services, it needs to authenticate with the cloud provider. In Azure Kubernetes clusters, up until now this was done using two ways - Service Principals or Managed Identities. In case of service principal the credentials are stored within the cluster and there are password rotation and other challenges which user needs to incur to accommodate this model. Managed service identities takes out this burden from the user and manages the service instances directly &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&#34; target=&#34;_blank&#34;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are two kinds of managed identities possible - one is system assigned and another is user assigned. In case of system assigned identity each vm in the Kubernetes cluster is assigned a managed identity during creation. This identity is used by various Kubernetes components needing access to Azure resources. Examples to these operations are getting/updating load balancer configuration, getting/updating vm information etc. With the system assigned managed identity, user has no control over the identity which is assigned to the underlying vm. The system automatically assigns it and this reduces the flexibility for the user.&lt;/p&gt;

&lt;p&gt;With v1.12 we bring user assigned managed identity support for Kubernetes. With this support user does not have to manage any passwords but at the same time has the flexibility to manage the identity which is used by the cluster. For example if the user needs to allow access to a cluster for a specific storage account or a Azure key vault, the user assigned identity can be created in advance and key vault access provided.&lt;/p&gt;

&lt;h2 id=&#34;internals-1&#34;&gt;Internals&lt;/h2&gt;

&lt;p&gt;To understand the internals, we will focus on a cluster created using acs-engine. This can be configured in other ways, but the basic interactions are of the same pattern.&lt;/p&gt;

&lt;p&gt;The acs-engine sets up the cluster with the required configuration. The /etc/kubernetes/azure.json file provides a way for the cluster components (eg: kube-apiserver) to gather configuration on how to access the cloud resources. In a user managed identity cluster there is a value filled with the key as &lt;code&gt;UserAssignedIdentityID&lt;/code&gt;. This value is filled with the client id of the user assigned identity created by acs-engine or provided by the user, however the case may be. The code which does the authentication for Kubernetes on azure can be found here &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/auth/azure_auth.go&#34; target=&#34;_blank&#34;&gt;[14]&lt;/a&gt;. This code uses Azure adal packages to get authenticated to access various resources in the cloud. In case of user assigned identity the following API call is made to get new token:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adal.NewServicePrincipalTokenFromMSIWithUserAssignedID(msiEndpoint,
env.ServiceManagementEndpoint,
config.UserAssignedIdentityID)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This calls hits either the instance metadata service or the vm extension &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&#34; target=&#34;_blank&#34;&gt;[12]&lt;/a&gt; to gather the token which is then used to access various resources.&lt;/p&gt;

&lt;h2 id=&#34;setting-up-a-cluster-with-user-assigned-identity&#34;&gt;Setting up a cluster with user assigned identity&lt;/h2&gt;

&lt;p&gt;With the upstream support for user assigned identity in v1.12, it is now supported in the acs-engine to create a cluster with the user assigned identity. The json config files present here &lt;a href=&#34;https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned&#34; target=&#34;_blank&#34;&gt;[13]&lt;/a&gt; can be used to create a cluster with user assigned identity. The same step used to create a vmss cluster can be used to create a cluster which has user assigned identity assigned.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;acs-engine deploy --subscription-id &amp;lt;subscription id&amp;gt; \
    --dns-prefix &amp;lt;dns&amp;gt; --location &amp;lt;location&amp;gt; \
    --api-model examples/kubernetes-msi-userassigned/kube-vmss.json
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main config values here are the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;useManagedIdentity&amp;quot;: true
&amp;quot;userAssignedID&amp;quot;: &amp;quot;acsenginetestid&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first one &lt;code&gt;useManagedIdentity&lt;/code&gt; indicates to acs-engine that we are going to use the managed identity extension. This sets up the necessary packages and extensions required for the managed identities to work. The next one &lt;code&gt;userAssignedID&lt;/code&gt; provides the information on the user identity which is to be used with the cluster.&lt;/p&gt;

&lt;h2 id=&#34;current-status-and-future-2&#34;&gt;Current status and future&lt;/h2&gt;

&lt;p&gt;Currently we support the user assigned identity creation with the cluster using deploy of the acs-engine. In future this will become part of AKS.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get involved&lt;/h2&gt;

&lt;p&gt;For azure specific discussions - please checkout the Azure SIG page at &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-azure&#34; target=&#34;_blank&#34;&gt;[6]&lt;/a&gt; and come and join the &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-azure&#34; target=&#34;_blank&#34;&gt;#sig-azure&lt;/a&gt; slack channel for more.&lt;/p&gt;

&lt;p&gt;For CA, please checkout the Autoscaler project here &lt;a href=&#34;http://www.github.com/kubernetes/autoscaler&#34; target=&#34;_blank&#34;&gt;[7]&lt;/a&gt; and join the &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-autoscaling&#34; target=&#34;_blank&#34;&gt;#sig-autoscaling&lt;/a&gt; Slack for more discussions.&lt;/p&gt;

&lt;p&gt;For the acs-engine (the unmanaged variety) on Azure docs can be found here: &lt;a href=&#34;https://github.com/Azure/acs-engine&#34; target=&#34;_blank&#34;&gt;[9]&lt;/a&gt;. More details about the managed service from Azure Kubernetes Service (AKS) here &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/&#34; target=&#34;_blank&#34;&gt;[5]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;p&gt;1) &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview&#34; target=&#34;_blank&#34;&gt;https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/concepts/architecture/cloud-controller/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3)  &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/azure_vmss.go&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/azure_vmss.go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;4) &lt;a href=&#34;https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md&#34; target=&#34;_blank&#34;&gt;https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/deploy.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;5) &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/&#34; target=&#34;_blank&#34;&gt;https://docs.microsoft.com/en-us/azure/aks/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;6) &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-azure&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/community/tree/master/sig-azure&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;7) &lt;a href=&#34;https://github.com/kubernetes/autoscaler&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/autoscaler&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;8) &lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;9) &lt;a href=&#34;https://github.com/Azure/acs-engine&#34; target=&#34;_blank&#34;&gt;https://github.com/Azure/acs-engine&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;10) &lt;a href=&#34;https://github.com/kubernetes/client-go&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/client-go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;11) &lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/concepts/architecture/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;12) &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&#34; target=&#34;_blank&#34;&gt;https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;13) &lt;a href=&#34;https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned&#34; target=&#34;_blank&#34;&gt;https://github.com/Azure/acs-engine/tree/master/examples/kubernetes-msi-userassigned&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;14) &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/auth/azure_auth.go&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/azure/auth/azure_auth.go&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;15) &lt;a href=&#34;https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler&#34; target=&#34;_blank&#34;&gt;https://github.com/Azure/acs-engine/tree/master/examples/addons/cluster-autoscaler&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing the Non-Code Contributor’s Guide</title>
      <link>https://docstest.github.io/blog/2018/10/04/introducing-the-non-code-contributors-guide/</link>
      <pubDate>Thu, 04 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/04/introducing-the-non-code-contributors-guide/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/noah_abrahams&#34; target=&#34;_blank&#34;&gt;Noah Abrahams&lt;/a&gt; (InfoSiftr), &lt;a href=&#34;https://twitter.com/jonasrosland&#34; target=&#34;_blank&#34;&gt;Jonas Rosland&lt;/a&gt; (VMware), &lt;a href=&#34;https://twitter.com/idvoretskyi&#34; target=&#34;_blank&#34;&gt;Ihor Dvoretskyi&lt;/a&gt; (CNCF)&lt;/p&gt;

&lt;p&gt;It was May 2018 in Copenhagen, and the Kubernetes community was enjoying the contributor summit at KubeCon/CloudNativeCon, complete with the first run of the New Contributor Workshop. As a time of tremendous collaboration between contributors, the topics covered ranged from signing the CLA to deep technical conversations. Along with the vast exchange of information and ideas, however, came continued scrutiny of the topics at hand to ensure that the community was being as inclusive and accommodating as possible. Over that spring week, some of the pieces under the microscope included the many themes being covered, and how they were being presented, but also the overarching characteristics of the people contributing and the skill sets involved. From the discussions and analysis that followed grew the idea that the community was not benefiting as much as it could from the many people who wanted to contribute, but whose strengths were in areas other than writing code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This all led to an effort called the &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/non-code-contributions.md&#34; target=&#34;_blank&#34;&gt;Non-Code Contributor’s Guide&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now, it’s important to note that Kubernetes is rare, if not unique, in the open source world, in that it was defined very early on as both a project and a community. While the project itself is focused on the codebase, it is the community of people driving it forward that makes the project successful. The community works together with an explicit set of &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/values.md&#34; target=&#34;_blank&#34;&gt;community values&lt;/a&gt;, guiding the day-to-day behavior of contributors whether on GitHub, Slack, Discourse, or sitting together over tea or coffee.&lt;/p&gt;

&lt;p&gt;By having a community that values people first, and explicitly values a diversity of people, the Kubernetes project is building a product to serve people with diverse needs. The different backgrounds of the contributors bring different approaches to the problem solving, with different methods of collaboration, and all those different viewpoints ultimately create a better project.&lt;/p&gt;

&lt;p&gt;The Non-Code Contributor’s Guide aims to make it easy for anyone to contribute to the Kubernetes project in a way that makes sense for them. This can be in many forms, technical and non-technical, based on the person&amp;rsquo;s knowledge of the project and their available time. Most individuals are not developers, and most of the world’s developers are not paid to fully work on open source projects. Based on this we have started an ever-growing list of possible ways to contribute to the Kubernetes project in a Non-Code way!&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;Some of the ways that you can contribute to the Kubernetes community without writing a single line of code include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Community education, answering questions on &lt;a href=&#34;https://discuss.kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Discuss&lt;/a&gt;, &lt;a href=&#34;https://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;StackOverflow&lt;/a&gt;, and &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Outward facing community work such as &lt;a href=&#34;https://www.meetup.com/pro/cncf/&#34; target=&#34;_blank&#34;&gt;hosting meetups&lt;/a&gt; and events&lt;/li&gt;
&lt;li&gt;Writing &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-docs&#34; target=&#34;_blank&#34;&gt;project documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Writing operational manuals, helping users understand how to run Kubernetes&lt;/li&gt;
&lt;li&gt;Helping deliver Kubernetes, as a part of the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/release-team/README.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Project, program, and &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-pm/README.md&#34; target=&#34;_blank&#34;&gt;product management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;And many more!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The guide to get started with Kubernetes project contribution is &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/contributors/guide&#34; target=&#34;_blank&#34;&gt;documented on Github&lt;/a&gt;, and as the Non-Code Contributors Guide is a part of that Kubernetes Contributors Guide, it can be found &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/guide/non-code-contributions.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. As stated earlier, this list is not exhaustive and will continue to be a work in progress.&lt;/p&gt;

&lt;p&gt;To date, the typical Non-Code contributions fall into the following categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Roles that are based on skill sets other than “software developer”&lt;/li&gt;
&lt;li&gt;Non-Code contributions in primarily code-based roles&lt;/li&gt;
&lt;li&gt;“Post-Code” roles, that are not code-based, but require knowledge of either the code base or management of the code base&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you, dear reader, have any additional ideas for a Non-Code way to contribute, whether or not it fits in an existing category, the team will always appreciate if you could help us expand the list.&lt;/p&gt;

&lt;p&gt;If a contribution of the Non-Code nature appeals to you, please read the Non-Code Contributions document, and then check the &lt;a href=&#34;https://discuss.kubernetes.io/c/contributors/role-board&#34; target=&#34;_blank&#34;&gt;Contributor Role Board&lt;/a&gt; to see if there are any open positions where your expertise could be best used! If there are no listed open positions that match your skill set, drop on by the &lt;a href=&#34;https://kubernetes.slack.com/messages/sig-contribex&#34; target=&#34;_blank&#34;&gt;#sig-contribex&lt;/a&gt; channel on Slack, and we’ll point you in the right direction.&lt;/p&gt;

&lt;p&gt;We hope to see you contributing to the Kubernetes community soon!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: KubeDirector: The easy way to run complex stateful applications on Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/10/03/kubedirector-the-easy-way-to-run-complex-stateful-applications-on-kubernetes/</link>
      <pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/03/kubedirector-the-easy-way-to-run-complex-stateful-applications-on-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Thomas Phelan (BlueData)&lt;/p&gt;

&lt;p&gt;KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.&lt;/p&gt;

&lt;p&gt;We recently &lt;a href=&#34;https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/&#34; target=&#34;_blank&#34;&gt;introduced the KubeDirector project&lt;/a&gt;, as part of a broader open source Kubernetes initiative we call BlueK8s. I’m happy to announce that the pre-alpha
code for &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;KubeDirector&lt;/a&gt; is now available. And in this blog post, I’ll show how it works.&lt;/p&gt;

&lt;p&gt;KubeDirector provides the following capabilities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, it’s not necessary to decompose these existing applications to fit a microservices design pattern.&lt;/li&gt;
&lt;li&gt;Native support for preserving application-specific configuration and state.&lt;/li&gt;
&lt;li&gt;An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes &amp;ndash; with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts.  The application metadata is referred to as a KubeDirectorApp resource.&lt;/p&gt;

&lt;p&gt;To understand the components of KubeDirector, clone the repository on &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; using a command similar to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file &lt;code&gt;kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
 {
    &amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
    &amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
    &amp;quot;metadata&amp;quot;: {
        &amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
    },
    &amp;quot;spec&amp;quot; : {
        &amp;quot;systemctlMounts&amp;quot;: true,
        &amp;quot;config&amp;quot;: {
            &amp;quot;node_services&amp;quot;: [
                {
                    &amp;quot;service_ids&amp;quot;: [
                        &amp;quot;ssh&amp;quot;,
                        &amp;quot;spark&amp;quot;,
                        &amp;quot;spark_master&amp;quot;,
                        &amp;quot;spark_worker&amp;quot;
                    ],
…
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
&lt;code&gt;kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
  name: &amp;quot;spark221e2&amp;quot;
spec:
  app: spark221e2
  roles:
  - name: controller
    replicas: 1
    resources:
      requests:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
      limits:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
  - name: worker
    replicas: 2
    resources:
      requests:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
      limits:
        memory: &amp;quot;4Gi&amp;quot;
        cpu: &amp;quot;2&amp;quot;
  - name: jupyter
…
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;running-spark-on-kubernetes-with-kubedirector&#34;&gt;Running Spark on Kubernetes with KubeDirector&lt;/h2&gt;

&lt;p&gt;With KubeDirector, it’s easy to run Spark clusters on Kubernetes.&lt;/p&gt;

&lt;p&gt;First, verify that Kubernetes (version 1.9 or later) is running, using the command &lt;code&gt;kubectl version&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}                                    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd kubedirector
make deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These will start the KubeDirector pod:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                           READY     STATUS     RESTARTS     AGE
kubedirector-58cf59869-qd9hb   1/1       Running    0            1m     
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;List the installed KubeDirector applications with &lt;code&gt;kubectl get KubeDirectorApp&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get KubeDirectorApp
NAME           AGE
cassandra311   30m
spark211up     30m
spark221e2     30m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
&lt;code&gt;kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code&gt; command.
Verify that the Spark cluster has been started:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
kubedirector-58cf59869-djdwl     1/1       Running   0          19m
spark221e2-controller-zbg4d-0    1/1       Running   0          23m
spark221e2-jupyter-2km7q-0       1/1       Running   0          23m
spark221e2-worker-4gzbz-0        1/1       Running   0          23m
spark221e2-worker-4gzbz-1        1/1       Running   0          23m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The running services now include the Spark services:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get service
NAME                                TYPE         CLUSTER-IP        EXTERNAL-IP    PORT(S)                                                    AGE
kubedirector                        ClusterIP    10.98.234.194     &amp;lt;none&amp;gt;         60000/TCP                                                  1d
kubernetes                          ClusterIP    10.96.0.1         &amp;lt;none&amp;gt;         443/TCP                                                    1d
svc-spark221e2-5tg48                ClusterIP    None              &amp;lt;none&amp;gt;         8888/TCP                                                   21s
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123    &amp;lt;none&amp;gt;         22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP  20s
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249    &amp;lt;none&amp;gt;         22:30632/TCP,8888:30355/TCP                                20s
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165    &amp;lt;none&amp;gt;         22:30358/TCP,8081:32144/TCP                                20s
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221     &amp;lt;none&amp;gt;         22:30294/TCP,8081:31436/TCP                                20s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pointing the browser at port 31533 connects to the Spark Master UI:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-03-kubedirector/kubedirector.png&#34; alt=&#34;kubedirector&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That’s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.&lt;/p&gt;

&lt;p&gt;To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See the running Cassandra cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pods
NAME                              READY     STATUS    RESTARTS   AGE
cassandra311-seed-v24r6-0         1/1       Running   0          1m
cassandra311-seed-v24r6-1         1/1       Running   0          1m
cassandra311-worker-rqrhl-0       1/1       Running   0          1m
cassandra311-worker-rqrhl-1       1/1       Running   0          1m
kubedirector-58cf59869-djdwl      1/1       Running   0          1d
spark221e2-controller-tq8d6-0     1/1       Running   0          22m
spark221e2-jupyter-6989v-0        1/1       Running   0          22m
spark221e2-worker-d9892-0         1/1       Running   0          22m
spark221e2-worker-d9892-1         1/1       Running   0          22m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use &lt;code&gt;kubectl get service&lt;/code&gt; to see the set of services.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get service
NAME                                TYPE         CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                   AGE
kubedirector                        ClusterIP    10.98.234.194    &amp;lt;none&amp;gt;        60000/TCP                                                 1d
kubernetes                          ClusterIP    10.96.0.1        &amp;lt;none&amp;gt;        443/TCP                                                   1d
svc-cassandra311-seed-v24r6-0       NodePort     10.96.94.204     &amp;lt;none&amp;gt;        22:31131/TCP,9042:30739/TCP                               3m
svc-cassandra311-seed-v24r6-1       NodePort     10.106.144.52    &amp;lt;none&amp;gt;        22:30373/TCP,9042:32662/TCP                               3m
svc-cassandra311-vhh29              ClusterIP    None             &amp;lt;none&amp;gt;        8888/TCP                                                  3m
svc-cassandra311-worker-rqrhl-0     NodePort     10.109.61.194    &amp;lt;none&amp;gt;        22:31832/TCP,9042:31962/TCP                               3m
svc-cassandra311-worker-rqrhl-1     NodePort     10.97.147.131    &amp;lt;none&amp;gt;        22:31454/TCP,9042:31170/TCP                               3m
svc-spark221e2-5tg48                ClusterIP    None             &amp;lt;none&amp;gt;        8888/TCP                                                  24m
svc-spark221e2-controller-tq8d6-0   NodePort     10.104.181.123   &amp;lt;none&amp;gt;        22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0      NodePort     10.105.227.249   &amp;lt;none&amp;gt;        22:30632/TCP,8888:30355/TCP                               24m
svc-spark221e2-worker-d9892-0       NodePort     10.107.131.165   &amp;lt;none&amp;gt;        22:30358/TCP,8081:32144/TCP                               24m
svc-spark221e2-worker-d9892-1       NodePort     10.110.88.221    &amp;lt;none&amp;gt;        22:30294/TCP,8081:31436/TCP                               24m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;KubeDirector is a fully open source, Apache v2 licensed, project – the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow &lt;a href=&#34;https://twitter.com/BlueK8s/&#34; target=&#34;_blank&#34;&gt;@BlueK8s&lt;/a&gt; on Twitter and get involved through these channels:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;KubeDirector &lt;a href=&#34;https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/&#34; target=&#34;_blank&#34;&gt;chat room on Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KubeDirector &lt;a href=&#34;https://github.com/bluek8s/kubedirector/&#34; target=&#34;_blank&#34;&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Building a Network Bootable Server Farm for Kubernetes with LTSP</title>
      <link>https://docstest.github.io/blog/2018/10/02/building-a-network-bootable-server-farm-for-kubernetes-with-ltsp/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/02/building-a-network-bootable-server-farm-for-kubernetes-with-ltsp/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Andrei Kvapil (WEDOS)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-10-01-network-bootable-farm-with-ltsp/k8s+ltsp.svg&#34; alt=&#34;k8s+ltsp&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;m going to introduce you to a cool technology for Kubernetes, LTSP. It is useful for large baremetal Kubernetes deployments.&lt;/p&gt;

&lt;p&gt;You don&amp;rsquo;t need to think about installing an OS and binaries on each node anymore. Why? You can do that automatically through Dockerfile!&lt;/p&gt;

&lt;p&gt;You can buy and put 100 new servers into a production environment and get them working immediately - it&amp;rsquo;s really amazing!&lt;/p&gt;

&lt;p&gt;Intrigued? Let me walk you through how it works.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Please note:&lt;/strong&gt; this is a cool hack, but is not officially supported in Kubernetes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First, we need to understand how exactly it works.&lt;/p&gt;

&lt;p&gt;In short, for all nodes we have prepared the image with the OS, Docker, Kubelet and everything else that you need there. This image with the kernel is building automatically by CI using Dockerfile. End nodes are booting the kernel and OS from this image via the network.&lt;/p&gt;

&lt;p&gt;Nodes are using overlays as the root filesystem and after reboot any changes will be lost (like in Docker containers). You have a config-file where you can describe mounts and some initial commands which should be executed during node boot (Example: set root user ssh-key and kubeadm join commands)&lt;/p&gt;

&lt;h2 id=&#34;image-preparation-process&#34;&gt;Image Preparation Process&lt;/h2&gt;

&lt;p&gt;We will use LTSP project because it&amp;rsquo;s gives us everything we need to organize the network booting environment. Basically, LTSP is a pack of shell-scripts which makes our life much easier.&lt;/p&gt;

&lt;p&gt;LTSP provides a initramfs module, a few helper-scripts, and the configuration system which prepare the system during the early state of boot, before the main init process call.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This is what the image preparation procedure looks like:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You&amp;rsquo;re deploying the basesystem in the chroot environment.&lt;/li&gt;
&lt;li&gt;Make any needed changes there, install software.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;ltsp-build-image&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After that, you will get the squashed image from the chroot with all the software inside. Each node will download this image during the boot and use it as the rootfs. For the update node, you can just reboot it. The new squashed image will be downloaded and mounted into the rootfs.&lt;/p&gt;

&lt;h2 id=&#34;server-components&#34;&gt;Server Components&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The server part of LTSP includes two components in our case:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TFTP-server&lt;/strong&gt; - TFTP is the initial protocol, it is used the download the kernel, initramfs and main config - lts.conf.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NBD-server&lt;/strong&gt; - NBD protocol is used to distribute the squashed rootfs image to the clients. It is the fastest way, but if you want, it can be replaced by the NFS or AoE protocol.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You should also have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DHCP-server&lt;/strong&gt; - it will distribute the IP-settings and a few specific options to the clients to make it possible for them to boot from our LTSP-server.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;node-booting-process&#34;&gt;Node Booting Process&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;This is how the node is booting up&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first time, the node will ask DHCP for IP-settings and &lt;code&gt;next-server&lt;/code&gt;, &lt;code&gt;filename&lt;/code&gt; options.&lt;/li&gt;
&lt;li&gt;Next, the node will apply settings and download bootloader (pxelinux or grub)&lt;/li&gt;
&lt;li&gt;Bootloader will download and read config with the kernel and initramfs image.&lt;/li&gt;
&lt;li&gt;Then bootloader will download the kernel and initramfs and execute it with specific cmdline options.&lt;/li&gt;
&lt;li&gt;During the boot, initramfs modules will handle options from cmdline and do some actions like connect NBD-device, prepare overlay rootfs, etc.&lt;/li&gt;
&lt;li&gt;Afterwards it will call the ltsp-init system instead of the normal init.&lt;/li&gt;
&lt;li&gt;ltsp-init scripts will prepare the system on the earlier stage, before the main init will be called. Basically it applies the setting from lts.conf (main config): write fstab and rc.local entries etc.&lt;/li&gt;
&lt;li&gt;Call the main init (systemd) which is booting the configured system as usual, mounts shares from fstab, start targets and services, executes commands from rc.local file.&lt;/li&gt;
&lt;li&gt;In the end you have a fully configured and booted system ready for further operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;preparing-the-server&#34;&gt;Preparing the Server&lt;/h1&gt;

&lt;p&gt;As I said before, I&amp;rsquo;m preparing the LTSP-server with the squashed image automatically using Dockerfile. This method is quite good because you have all steps described in your git repository.
You have versioning, branches, CI and everything that you used to use for preparing your usual Docker projects.&lt;/p&gt;

&lt;p&gt;Otherwise, you can deploy the LTSP server manually by executing all steps by hand. This is a good practice for learning and understanding the basic principles.&lt;/p&gt;

&lt;p&gt;Just repeat all the steps listed here by hand, just to try to install LTSP without Dockerfile.&lt;/p&gt;

&lt;h2 id=&#34;used-patches-list&#34;&gt;Used Patches List&lt;/h2&gt;

&lt;p&gt;LTSP still has some issues which authors don’t want to apply, yet. However LTSP is easy customizable so I prepared a few patches for myself and will share them here.&lt;/p&gt;

&lt;p&gt;I’ll create a fork if the community will warmly accept my solution.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kvaps/ltsp/compare/feature-grub.diff&#34; target=&#34;_blank&#34;&gt;feature-grub.diff&lt;/a&gt;
  LTSP does not support EFI by default, so I&amp;rsquo;ve prepared a patch which adds GRUB2 with EFI support.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kvaps/ltsp/compare/feature_preinit.diff&#34; target=&#34;_blank&#34;&gt;feature_preinit.diff&lt;/a&gt;
  This patch adds a PREINIT option to lts.conf, which allows you to run custom commands before the main init call. It may be useful to modify the systemd units and configure the network. It&amp;rsquo;s remarkable that all environment variables from the boot environment are saved and you can use them in your scripts.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kvaps/ltsp/compare/feature_initramfs_params_from_lts_conf.diff&#34; target=&#34;_blank&#34;&gt;feature_initramfs_params_from_lts_conf.diff&lt;/a&gt;
  Solves a problem with NBD_TO_RAM option, after this patch you can specify it on lts.conf inside chroot. (not in tftp directory)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.githubusercontent.com/kvaps/1a6a7d8b73bf7444f0f99b22379c9e4e/raw/eb0d60c638ef72b7e28438b7f4d2beda89c41f75/nbd-server-wrapper.sh&#34; target=&#34;_blank&#34;&gt;nbd-server-wrapper.sh&lt;/a&gt;
  This is not a patch but a special wrapper script which allows you to run NBD-server in the foreground. It is useful if you want to run it inside a Docker container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;dockerfile-stages&#34;&gt;Dockerfile Stages&lt;/h2&gt;

&lt;p&gt;We will use &lt;a href=&#34;https://docs.docker.com/develop/develop-images/multistage-build/&#34; target=&#34;_blank&#34;&gt;stage building&lt;/a&gt; in our Dockerfile to leave only the needed parts in our Docker image. The unused parts will be removed from the final image.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ltsp-base
(install basic LTSP server software)
   |
   |---basesystem
   |   (prepare chroot with main software and kernel)
   |     |
   |     |---builder
   |     |   (build additional software from sources, if needed)
   |     |
   |     &#39;---ltsp-image
   |         (install additional software, docker, kubelet and build squashed image)
   |
   &#39;---final-stage
       (copy squashed image, kernel and initramfs into first stage)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stage-1-ltsp-base&#34;&gt;Stage 1: ltsp-base&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s start writing our Dockerfile. This is the first part:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; ubuntu:16.04 as ltsp-base&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;ADD&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; nbd-server-wrapper.sh /bin/&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;ADD&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; /patches/feature-grub.diff /patches/feature-grub.diff&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; apt-get -y update &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get -y install &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      ltsp-server &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      tftpd-hpa &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      nbd-server &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      grub-common &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      grub-pc-bin &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      grub-efi-amd64-bin &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      curl &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      patch &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sed -i &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;s|in_target mount|in_target_nofail mount|&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      /usr/share/debootstrap/functions &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Add EFI support and Grub bootloader (#1745251)&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; patch -p2 -d /usr/sbin &amp;lt; /patches/feature-grub.diff &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; rm -rf /var/lib/apt/lists &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get clean&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this stage our Docker image has already been installed:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;NBD-server&lt;/li&gt;
&lt;li&gt;TFTP-server&lt;/li&gt;
&lt;li&gt;LTSP-scripts with grub bootloader support (for EFI)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;stage-2-basesystem&#34;&gt;Stage 2: basesystem&lt;/h3&gt;

&lt;p&gt;In this stage we will prepare a chroot environment with basesystem, and install basic software with the kernel.&lt;/p&gt;

&lt;p&gt;We will use the classic &lt;strong&gt;debootstrap&lt;/strong&gt; instead of &lt;strong&gt;ltsp-build-client&lt;/strong&gt; to prepare the base image, because &lt;strong&gt;ltsp-build-client&lt;/strong&gt; will install GUI and few other things which we don&amp;rsquo;t need for the server deployment.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; ltsp-base as basesystem&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;ARG &lt;span style=&#34;color:#b8860b&#34;&gt;DEBIAN_FRONTEND&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;noninteractive&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Prepare base system&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; debootstrap --arch amd64 xenial /opt/ltsp/amd64&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install updates&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;\
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      deb http://archive.ubuntu.com/ubuntu xenial main restricted universe multiverse\n\
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      deb http://archive.ubuntu.com/ubuntu xenial-updates main restricted universe multiverse\n\
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      deb http://archive.ubuntu.com/ubuntu xenial-security main restricted universe multiverse&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &amp;gt; /opt/ltsp/amd64/etc/apt/sources.list &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ltsp-chroot apt-get -y update &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ltsp-chroot apt-get -y upgrade&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Installing LTSP-packages&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot apt-get -y install ltsp-client-core&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Apply initramfs patches&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# 1: Read params from /etc/lts.conf during the boot (#1680490)&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# 2: Add support for PREINIT variables in lts.conf&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;ADD&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; /patches /patches&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; patch -p4 -d /opt/ltsp/amd64/usr/share &amp;lt; /patches/feature_initramfs_params_from_lts_conf.diff &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; patch -p3 -d /opt/ltsp/amd64/usr/share &amp;lt; /patches/feature_preinit.diff&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Write new local client config for boot NBD image to ram:&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;[Default]\nLTSP_NBD_TO_RAM = true&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &amp;gt; /opt/ltsp/amd64/etc/lts.conf&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install packages&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; &lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;APT::Install-Recommends &amp;#34;0&amp;#34;;\nAPT::Install-Suggests &amp;#34;0&amp;#34;;&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &amp;gt;&amp;gt; /opt/ltsp/amd64/etc/apt/apt.conf.d/01norecommend &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ltsp-chroot apt-get -y install &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      software-properties-common &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      apt-transport-https &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      ca-certificates &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      ssh &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      bridge-utils &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      pv &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      jq &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      vlan &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      bash-completion &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      screen &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      vim &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      mc &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      lm-sensors &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      htop &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      jnettop &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      rsync &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      curl &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      wget &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      tcpdump &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      arping &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      apparmor-utils &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      nfs-common &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      telnet &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      sysstat &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      ipvsadm &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      ipset &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      make&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install kernel&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot apt-get -y install linux-generic-hwe-16.04&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that you may encounter problems with some packages, such as &lt;code&gt;lvm2&lt;/code&gt;.
They have not fully optimized for installing in an unprivileged chroot.
Their postinstall scripts try to call some privileged commands which can fail with errors and block the package installation.&lt;/p&gt;

&lt;p&gt;Solution:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Some of them can be installed before the kernel without any problems (like &lt;code&gt;lvm2&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;But for some of them you will need to use &lt;a href=&#34;https://askubuntu.com/a/482936/327437&#34; target=&#34;_blank&#34;&gt;this workaround&lt;/a&gt; to install without the postinstall script.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;stage-3-builder&#34;&gt;Stage 3: builder&lt;/h3&gt;

&lt;p&gt;Now we can build all the necessary software and kernel modules. It&amp;rsquo;s really cool that you can do that automatically in this stage.
You can skip this stage if you have nothing to do here.&lt;/p&gt;

&lt;p&gt;Here is example for install latest MLNX_EN driver:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; basesystem as builder&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Set cpuinfo (for building from sources)&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; cp /proc/cpuinfo /opt/ltsp/amd64/proc/cpuinfo&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Compile Mellanox driver&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot sh -cx &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;   &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;  VERSION=4.3-1.0.1.0-ubuntu16.04-x86_64 \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; curl -L http://www.mellanox.com/downloads/ofed/MLNX_EN-${VERSION%%-ubuntu*}/mlnx-en-${VERSION}.tgz \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      | tar xzf - \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; export \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;        DRIVER_DIR=&amp;#34;$(ls -1 | grep &amp;#34;MLNX_OFED_LINUX-\|mlnx-en-&amp;#34;)&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;        KERNEL=&amp;#34;$(ls -1t /lib/modules/ | head -n1)&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; cd &amp;#34;$DRIVER_DIR&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; ./*install --kernel &amp;#34;$KERNEL&amp;#34; --without-dkms --add-kernel-support \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; cd - \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; rm -rf &amp;#34;$DRIVER_DIR&amp;#34; /tmp/mlnx-en* /tmp/ofed*&amp;#39;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Save kernel modules&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot sh -c &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39; export KERNEL=&amp;#34;$(ls -1t /usr/src/ | grep -m1 &amp;#34;^linux-headers&amp;#34; | sed &amp;#34;s/^linux-headers-//g&amp;#34;)&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; tar cpzf /modules.tar.gz /lib/modules/${KERNEL}/updates&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;stage-4-ltsp-image&#34;&gt;Stage 4: ltsp-image&lt;/h3&gt;

&lt;p&gt;In this stage we will install what we built in the previous step:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; basesystem as ltsp-image&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Retrieve kernel modules&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;COPY --from&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;builder /opt/ltsp/amd64/modules.tar.gz /opt/ltsp/amd64/modules.tar.gz&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install kernel modules&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot sh -c &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39; export KERNEL=&amp;#34;$(ls -1t /usr/src/ | grep -m1 &amp;#34;^linux-headers&amp;#34; | sed &amp;#34;s/^linux-headers-//g&amp;#34;)&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; tar xpzf /modules.tar.gz \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; depmod -a &amp;#34;${KERNEL}&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; rm -f /modules.tar.gz&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then do some additional changes to finalize our ltsp-image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install docker&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot sh -c &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;   &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; echo &amp;#34;deb https://download.docker.com/linux/ubuntu xenial stable&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;        &amp;gt; /etc/apt/sources.list.d/docker.list \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; apt-get -y update \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;   &amp;amp;&amp;amp; apt-get -y install \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;        docker-ce=$(apt-cache madison docker-ce | grep 18.06 | head -1 | awk &amp;#34;{print $ 3}&amp;#34;)&amp;#39;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Configure docker options&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; &lt;span style=&#34;color:#b8860b&#34;&gt;DOCKER_OPTS&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --storage-driver&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;overlay2 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --iptables&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;false&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --ip-masq&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;false&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --log-driver&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;json-file &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --log-opt&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;max-size&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;10m &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      --log-opt&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;max-file&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sed &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/^ExecStart=/ s|&lt;/span&gt;$&lt;span style=&#34;color:#b44&#34;&gt;| &lt;/span&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;$DOCKER_OPTS&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;|g&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      /opt/ltsp/amd64/lib/systemd/system/docker.service &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &amp;gt; /opt/ltsp/amd64/etc/systemd/system/docker.service&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Install kubeadm, kubelet and kubectl&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot sh -c &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      &amp;amp;&amp;amp; echo &amp;#34;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;#34; \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;           &amp;gt; /etc/apt/sources.list.d/kubernetes.list \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      &amp;amp;&amp;amp; apt-get -y update \
&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;      &amp;amp;&amp;amp; apt-get -y install kubelet kubeadm kubectl cri-tools&amp;#39;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Disable automatic updates&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; rm -f /opt/ltsp/amd64/etc/apt/apt.conf.d/20auto-upgrades&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Disable apparmor profiles&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-chroot find /etc/apparmor.d &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -maxdepth &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -type f &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -name &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;sbin.*&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -o -name &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;usr.*&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      -exec ln -sf &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;{}&amp;#34;&lt;/span&gt; /etc/apparmor.d/disable/ &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\;&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Write kernel cmdline options&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; &lt;span style=&#34;color:#b8860b&#34;&gt;KERNEL_OPTIONS&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;$(&lt;/span&gt;&lt;span style=&#34;color:#a2f&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#b8860b&#34;&gt;init&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;/sbin/init-ltsp &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      forcepae &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#b8860b&#34;&gt;console&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;tty1 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#b8860b&#34;&gt;console&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ttyS0,9600n8 &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      nvme_core.default_ps_max_latency_us&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;)&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sed -i &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/^CMDLINE_LINUX_DEFAULT=/ s|=.*|=\&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;${&lt;/span&gt;&lt;span style=&#34;color:#b8860b&#34;&gt;KERNEL_OPTIONS&lt;/span&gt;&lt;span style=&#34;color:#b68;font-weight:bold&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;\&amp;#34;|&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt;      &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/opt/ltsp/amd64/etc/ltsp/update-kernels.conf&amp;#34;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then we will make the squashed image from our chroot:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Cleanup caches&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; rm -rf /opt/ltsp/amd64/var/lib/apt/lists &lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#b62;font-weight:bold&#34;&gt;&lt;/span&gt; &lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ltsp-chroot apt-get clean&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Build squashed image&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;RUN&lt;/span&gt; ltsp-update-image&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;stage-5-final-stage&#34;&gt;Stage 5: Final Stage&lt;/h3&gt;

&lt;p&gt;In the final stage we will save only our squashed image and kernels with initramfs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Dockerfile&#34; data-lang=&#34;Dockerfile&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt; ltsp-base&lt;/span&gt;&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;COPY --from&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ltsp-image /opt/ltsp/images /opt/ltsp/images&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;COPY --from&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ltsp-image /etc/nbd-server/conf.d /etc/nbd-server/conf.d&lt;span style=&#34;&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;&#34;&gt;&lt;/span&gt;COPY --from&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;ltsp-image /var/lib/tftpboot /var/lib/tftpboot&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ok, now we have docker image which includes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TFTP-server&lt;/li&gt;
&lt;li&gt;NBD-server&lt;/li&gt;
&lt;li&gt;configured bootloader&lt;/li&gt;
&lt;li&gt;kernel with initramfs&lt;/li&gt;
&lt;li&gt;squashed rootfs image&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;usage&#34;&gt;Usage&lt;/h1&gt;

&lt;p&gt;OK, now when our docker-image with LTSP-server, kernel, initramfs and squashed rootfs fully prepared we can run the deployment with it.&lt;/p&gt;

&lt;p&gt;We can do that as usual, but one more thing is networking.
Unfortunately, we can&amp;rsquo;t use the standard Kubernetes service abstraction for our deployment, because TFTP can&amp;rsquo;t work behind the NAT. During the boot, our nodes are not part of Kubernetes cluster and they requires ExternalIP, but Kubernetes always enables NAT for ExternalIPs, and there is no way to override this behavior.&lt;/p&gt;

&lt;p&gt;For now I have two ways for avoid this: use &lt;code&gt;hostNetwork: true&lt;/code&gt; or use &lt;a href=&#34;https://github.com/dreamcat4/docker-images/blob/master/pipework/3.%20Examples.md#kubernetes&#34; target=&#34;_blank&#34;&gt;pipework&lt;/a&gt;. The second option will also provide you redundancy because, in case of failure, the IP will be moved with the Pod to another node. Unfortunately, pipework is not native and a less secure method.
If you have some better option for that please let me know.&lt;/p&gt;

&lt;p&gt;Here is example for deployment with hostNetwork:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;extensions/v1beta1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Deployment&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ltsp-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ltsp-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ltsp-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;replicas:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;template:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;labels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ltsp-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;hostNetwork:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;containers:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;tftpd&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;registry.example.org/example/ltsp:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/usr/sbin/in.tftpd&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-L&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-u&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;tftp&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-a&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;:69&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-s&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/var/lib/tftpboot&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;lifecycle:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;postStart:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;            &lt;/span&gt;exec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;              &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;cd /var/lib/tftpboot/ltsp/amd64; ln -sf config/lts.conf .&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;volumeMounts:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;mountPath:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/var/lib/tftpboot/ltsp/amd64/config&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;nbd-server&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;image:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;registry.example.org/example/ltsp:latest&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;command:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;[&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;/bin/nbd-server-wrapper.sh&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;]&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;volumes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;config&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;        &lt;/span&gt;configMap:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;          &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ltsp-config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you can see it also requires configmap with &lt;strong&gt;lts.conf&lt;/strong&gt; file.
Here is example part from mine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: ltsp-config
data:
  lts.conf: |
    [default]
    KEEP_SYSTEM_SERVICES           = &amp;quot;ssh ureadahead dbus-org.freedesktop.login1 systemd-logind polkitd cgmanager ufw rpcbind nfs-kernel-server&amp;quot;

    PREINIT_00_TIME                = &amp;quot;ln -sf /usr/share/zoneinfo/Europe/Prague /etc/localtime&amp;quot;
    PREINIT_01_FIX_HOSTNAME        = &amp;quot;sed -i &#39;/^127.0.0.2/d&#39; /etc/hosts&amp;quot;
    PREINIT_02_DOCKER_OPTIONS      = &amp;quot;sed -i &#39;s|^ExecStart=.*|ExecStart=/usr/bin/dockerd -H fd:// --storage-driver overlay2 --iptables=false --ip-masq=false --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5|&#39; /etc/systemd/system/docker.service&amp;quot;

    FSTAB_01_SSH                   = &amp;quot;/dev/data/ssh     /etc/ssh          ext4 nofail,noatime,nodiratime 0 0&amp;quot;
    FSTAB_02_JOURNALD              = &amp;quot;/dev/data/journal /var/log/journal  ext4 nofail,noatime,nodiratime 0 0&amp;quot;
    FSTAB_03_DOCKER                = &amp;quot;/dev/data/docker  /var/lib/docker   ext4 nofail,noatime,nodiratime 0 0&amp;quot;

    # Each command will stop script execution when fail
    RCFILE_01_SSH_SERVER           = &amp;quot;cp /rofs/etc/ssh/*_config /etc/ssh; ssh-keygen -A&amp;quot;
    RCFILE_02_SSH_CLIENT           = &amp;quot;mkdir -p /root/.ssh/; echo &#39;ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDBSLYRaORL2znr1V4a3rjDn3HDHn2CsvUNK1nv8+CctoICtJOPXl6zQycI9KXNhANfJpc6iQG1ZPZUR74IiNhNIKvOpnNRPyLZ5opm01MVIDIZgi9g0DUks1g5gLV5LKzED8xYKMBmAfXMxh/nsP9KEvxGvTJB3OD+/bBxpliTl5xY3Eu41+VmZqVOz3Yl98+X8cZTgqx2dmsHUk7VKN9OZuCjIZL9MtJCZyOSRbjuo4HFEssotR1mvANyz+BUXkjqv2pEa0I2vGQPk1VDul5TpzGaN3nOfu83URZLJgCrX+8whS1fzMepUYrbEuIWq95esjn0gR6G4J7qlxyguAb9 admin@kubernetes&#39; &amp;gt;&amp;gt; /root/.ssh/authorized_keys&amp;quot;
    RCFILE_03_KERNEL_DEBUG         = &amp;quot;sysctl -w kernel.unknown_nmi_panic=1 kernel.softlockup_panic=1; modprobe netconsole netconsole=@/vmbr0,@10.9.0.15/&amp;quot;
    RCFILE_04_SYSCTL               = &amp;quot;sysctl -w fs.file-max=20000000 fs.nr_open=20000000 net.ipv4.neigh.default.gc_thresh1=80000 net.ipv4.neigh.default.gc_thresh2=90000 net.ipv4.neigh.default.gc_thresh3=100000&amp;quot;
    RCFILE_05_FORWARD              = &amp;quot;echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward&amp;quot;
    RCFILE_06_MODULES              = &amp;quot;modprobe br_netfilter&amp;quot;
    RCFILE_07_JOIN_K8S             = &amp;quot;kubeadm join --token 2a4576.504356e45fa3d365 10.9.0.20:6443 --discovery-token-ca-cert-hash sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KEEP_SYSTEM_SERVICES&lt;/strong&gt; - during the boot, LTSP automatically removes some services, this variable is needed to prevent this behavior.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PREINIT_&lt;/strong&gt;* - commands listed here will be executed before systemd runs (this function was added by the &lt;a href=&#34;#used-patches-list&#34;&gt;feature_preinit.diff&lt;/a&gt; patch)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FSTAB_&lt;/strong&gt;* - entries written here will be added to the &lt;code&gt;/etc/fstab&lt;/code&gt; file.
As you can see, I use the &lt;code&gt;nofail&lt;/code&gt; option, that means that if a partition doesn&amp;rsquo;t exist, it will continue to boot without error.
If you have fully diskless nodes you can remove the FSTAB settings or configure the remote filesystem there.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RCFILE_&lt;/strong&gt;* - those commands will be written to &lt;code&gt;rc.local&lt;/code&gt; file, which will be called by systemd during the boot.
Here I load the kernel modules and add some sysctl tunes, then call the &lt;code&gt;kubeadm join&lt;/code&gt; command, which adds my node to the Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can get more details on all the variables used from &lt;a href=&#34;http://manpages.ubuntu.com/manpages/xenial/man5/lts.conf.5.html&#34; target=&#34;_blank&#34;&gt;lts.conf manpage&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now you can configure your DHCP. Basically you should set the &lt;code&gt;next-server&lt;/code&gt; and &lt;code&gt;filename&lt;/code&gt; options.&lt;/p&gt;

&lt;p&gt;I use ISC-DHCP server, and here is an example &lt;code&gt;dhcpd.conf&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;shared-network ltsp-netowrk {
    subnet 10.9.0.0 netmask 255.255.0.0 {
        authoritative;
        default-lease-time -1;
        max-lease-time -1;

        option domain-name              &amp;quot;example.org&amp;quot;;
        option domain-name-servers      10.9.0.1;
        option routers                  10.9.0.1;
        next-server                     ltsp-1;  # write LTSP-server hostname here

        if option architecture = 00:07 {
            filename &amp;quot;/ltsp/amd64/grub/x86_64-efi/core.efi&amp;quot;;
        } else {
            filename &amp;quot;/ltsp/amd64/grub/i386-pc/core.0&amp;quot;;
        }

        range 10.9.200.0 10.9.250.254; 
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can start from this, but what about me, I have multiple LTSP-servers and I configure leases statically for each node via the Ansible playbook.&lt;/p&gt;

&lt;p&gt;Try to run your first node. If everything was right, you will have a running system there.
The node also will be added to your Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Now you can try to make your own changes.&lt;/p&gt;

&lt;p&gt;If you need something more, note that LTSP can be easily changed to meet your needs.
Feel free to look into the source code and you can find many answers there.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;UPD:&lt;/strong&gt; Many people asking me: Why not simple use CoreOS and Ignition?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I can answer. The main feature here is image preparation process, not configuration. In case with LTSP you have classic Ubuntu system, and everything that can be installed on Ubuntu it can also be written here in the Dockerfile. In case CoreOS you have no so many freedom and you can’t easily add custom kernel modules and packages at the build stage of the boot image.&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Health checking gRPC servers on Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;a href=&#34;https://twitter.com/ahmetb&#34; target=&#34;_blank&#34;&gt;Ahmet Alp Balkan&lt;/a&gt; (Google)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://grpc.io&#34; target=&#34;_blank&#34;&gt;gRPC&lt;/a&gt; is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
&lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-health-probe/&#34; target=&#34;_blank&#34;&gt;grpc-health-probe&lt;/a&gt;, a
Kubernetes-native way to health check gRPC apps.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re unfamiliar, Kubernetes &lt;a href=&#34;https://docstest.github.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34;&gt;health
checks&lt;/a&gt;
(liveness and readiness probes) is what&amp;rsquo;s keeping your applications available
while you&amp;rsquo;re sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.&lt;/p&gt;

&lt;p&gt;Kubernetes &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/21493&#34; target=&#34;_blank&#34;&gt;does not
support&lt;/a&gt; gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docstest.github.io/images/blog/2019-09-30-health-checking-grpc/options.png&#34;&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2019-09-30-health-checking-grpc/options.png&#34; alt=&#34;options for health checking grpc on kubernetes today&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;httpGet probe:&lt;/strong&gt; Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tcpSocket probe:&lt;/strong&gt; Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;exec probe:&lt;/strong&gt; This invokes a program in a container&amp;rsquo;s ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Can we do better? Absolutely.&lt;/p&gt;

&lt;h2 id=&#34;introducing-grpc-health-probe&#34;&gt;Introducing “grpc-health-probe”&lt;/h2&gt;

&lt;p&gt;To standardize the &amp;ldquo;exec probe&amp;rdquo; approach mentioned above, we need:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;standard&lt;/strong&gt; health check &amp;ldquo;protocol&amp;rdquo; that can be implemented in any gRPC
server easily.&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;standard&lt;/strong&gt; health check &amp;ldquo;tool&amp;rdquo; that can query the health protocol easily.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thankfully, gRPC has a &lt;a href=&#34;https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md&#34; target=&#34;_blank&#34;&gt;standard health checking
protocol&lt;/a&gt;. It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.&lt;/p&gt;

&lt;p&gt;If you
&lt;a href=&#34;https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto&#34; target=&#34;_blank&#34;&gt;implement&lt;/a&gt;
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this &lt;code&gt;Check()&lt;/code&gt; method to determine server status.&lt;/p&gt;

&lt;p&gt;The next thing you need is the &amp;ldquo;standard tool&amp;rdquo;, and it&amp;rsquo;s the
&lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-health-probe/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;grpc-health-probe&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#39;https://docstest.github.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png&#39;&gt;
    &lt;img width=&#34;768&#34;  title=&#39;grpc-health-probe on kubernetes&#39;
        src=&#39;https://docstest.github.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png&#39;/&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Find the gRPC &amp;ldquo;health&amp;rdquo; module in your favorite language and start using it
(example &lt;a href=&#34;https://godoc.org/github.com/grpc/grpc-go/health&#34; target=&#34;_blank&#34;&gt;Go library&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Ship the
&lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-health-probe/&#34; target=&#34;_blank&#34;&gt;grpc_health_probe&lt;/a&gt;
binary in your container.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes&#34; target=&#34;_blank&#34;&gt;Configure&lt;/a&gt;
Kubernetes &amp;ldquo;exec&amp;rdquo; probe to invoke the &amp;ldquo;grpc_health_probe&amp;rdquo; tool in the
container.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this case, executing &amp;ldquo;grpc_health_probe&amp;rdquo; will call your gRPC server over
&lt;code&gt;localhost&lt;/code&gt;, since they are in the same pod.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s next&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;grpc-health-probe&lt;/strong&gt; project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.&lt;/p&gt;

&lt;p&gt;If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and &lt;a href=&#34;https://github.com/grpc-ecosystem/grpc-health-probe/&#34; target=&#34;_blank&#34;&gt;give
feedback&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;further-reading&#34;&gt;Further reading&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Protocol: &lt;a href=&#34;https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md&#34; target=&#34;_blank&#34;&gt;GRPC Health Checking Protocol&lt;/a&gt; (&lt;a href=&#34;https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto&#34; target=&#34;_blank&#34;&gt;health.proto&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Documentation: &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/&#34; target=&#34;_blank&#34;&gt;Kubernetes liveness and readiness probes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Article: &lt;a href=&#34;https://ahmet.im/blog/advanced-kubernetes-health-checks/&#34; target=&#34;_blank&#34;&gt;Advanced Kubernetes Health Check Patterns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.12: Kubelet TLS Bootstrap and Azure Virtual Machine Scale Sets (VMSS) Move to General Availability</title>
      <link>https://docstest.github.io/blog/2018/09/27/kubernetes-1.12-kubelet-tls-bootstrap-and-azure-virtual-machine-scale-sets-vmss-move-to-general-availability/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/09/27/kubernetes-1.12-kubelet-tls-bootstrap-and-azure-virtual-machine-scale-sets-vmss-move-to-general-availability/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: The 1.12 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.12, our third release of 2018!&lt;/p&gt;

&lt;p&gt;Today’s release continues to focus on internal improvements and graduating features to stable in Kubernetes. This newest version graduates key features such as security and Azure. Notable additions in this release include two highly-anticipated features graduating to general availability: Kubelet TLS Bootstrap and Support for Azure Virtual Machine Scale Sets (VMSS).&lt;/p&gt;

&lt;p&gt;These new features mean increased security, availability, resiliency, and ease of use to get production applications to market faster. The release also signifies the increasing maturation and sophistication of Kubernetes on the developer side.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;introducing-general-availability-of-kubelet-tls-bootstrap&#34;&gt;Introducing General Availability of Kubelet TLS Bootstrap&lt;/h2&gt;

&lt;p&gt;We’re excited to announce General Availability (GA) of &lt;a href=&#34;https://github.com/kubernetes/features/issues/43&#34; target=&#34;_blank&#34;&gt;Kubelet TLS Bootstrap&lt;/a&gt;. In Kubernetes 1.4, we introduced an API for requesting certificates from a cluster-level Certificate Authority (CA). The original intent of this API is to enable provisioning of TLS client certificates for kubelets. This feature allows for a kubelet to bootstrap itself into a TLS-secured cluster. Most importantly, it automates the provision and distribution of signed certificates.&lt;/p&gt;

&lt;p&gt;Before, when a kubelet ran for the first time, it had to be given client credentials in an out-of-band process during cluster startup. The burden was on the operator to provision these credentials. Because this task was so onerous to manually execute and complex to automate, many operators deployed clusters with a single credential and single identity for all kubelets. These setups prevented deployment of node lockdown features like the Node authorizer and the NodeRestriction admission controller.&lt;/p&gt;

&lt;p&gt;To alleviate this, &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-auth&#34; target=&#34;_blank&#34;&gt;SIG Auth&lt;/a&gt; introduced a way for kubelet to generate a private key and a CSR for submission to a cluster-level certificate signing process. The v1 (GA) designation indicates production hardening and readiness, and comes with the guarantee of long-term backwards compatibility.&lt;/p&gt;

&lt;p&gt;Alongside this, &lt;a href=&#34;https://github.com/kubernetes/features/issues/267&#34; target=&#34;_blank&#34;&gt;Kubelet server certificate bootstrap and rotation&lt;/a&gt; is moving to beta. Currently, when a kubelet first starts, it generates a self-signed certificate/key pair that is used for accepting incoming TLS connections. This feature introduces a process for generating a key locally and then issuing a Certificate Signing Request to the cluster API server to get an associated certificate signed by the cluster’s root certificate authority. Also, as certificates approach expiration, the same mechanism will be used to request an updated certificate.&lt;/p&gt;

&lt;h2 id=&#34;support-for-azure-virtual-machine-scale-sets-vmss-and-cluster-autoscaler-is-now-stable&#34;&gt;Support for Azure Virtual Machine Scale Sets (VMSS) and Cluster-Autoscaler is Now Stable&lt;/h2&gt;

&lt;p&gt;Azure Virtual Machine Scale Sets (VMSS) allow you to create and manage a homogenous VM pool that can automatically increase or decrease based on demand or a set schedule. This enables you to easily manage, scale, and load balance multiple VMs to provide high availability and application resiliency, ideal for large-scale applications that can run as Kubernetes workloads.&lt;/p&gt;

&lt;p&gt;With this new stable feature, Kubernetes supports the &lt;a href=&#34;https://github.com/kubernetes/features/issues/514&#34; target=&#34;_blank&#34;&gt;scaling of containerized applications with Azure VMSS&lt;/a&gt;, including the ability to &lt;a href=&#34;https://github.com/kubernetes/features/issues/513&#34; target=&#34;_blank&#34;&gt;integrate it with cluster-autoscaler&lt;/a&gt; to automatically adjust the size of the Kubernetes clusters based on the same conditions.&lt;/p&gt;

&lt;h2 id=&#34;additional-notable-feature-updates&#34;&gt;Additional Notable Feature Updates&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/585&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;RuntimeClass&lt;/code&gt;&lt;/a&gt; is a new cluster-scoped resource that surfaces container runtime properties to the control plane being released as an alpha feature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/177&#34; target=&#34;_blank&#34;&gt;Snapshot / restore functionality for Kubernetes and CSI&lt;/a&gt; is being introduced as an alpha feature. This provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/561&#34; target=&#34;_blank&#34;&gt;Topology aware dynamic provisioning&lt;/a&gt; is now in beta, meaning storage resources can now understand where they live. This also includes beta support to &lt;a href=&#34;https://github.com/kubernetes/features/issues/567&#34; target=&#34;_blank&#34;&gt;AWS EBS&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/features/issues/558&#34; target=&#34;_blank&#34;&gt;GCE PD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/495&#34; target=&#34;_blank&#34;&gt;Configurable pod process namespace sharing&lt;/a&gt; is moving to beta, meaning users can configure containers within a pod to share a common PID namespace by setting an option in the PodSpec.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/382&#34; target=&#34;_blank&#34;&gt;Taint node by condition&lt;/a&gt; is now in beta, meaning users have the ability to represent node conditions that block scheduling by using taints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/117&#34; target=&#34;_blank&#34;&gt;Arbitrary / Custom Metrics&lt;/a&gt; in the Horizontal Pod Autoscaler is moving to a second beta to test some additional feature enhancements. This reworked Horizontal Pod Autoscaler functionality includes support for custom metrics and status conditions.&lt;/p&gt;

&lt;p&gt;Improvements that will allow the &lt;a href=&#34;https://github.com/kubernetes/features/issues/591&#34; target=&#34;_blank&#34;&gt;Horizontal Pod Autoscaler to reach proper size faster&lt;/a&gt; are moving to beta.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/21&#34; target=&#34;_blank&#34;&gt;Vertical Scaling of Pods&lt;/a&gt; is now in beta, which makes it possible to vary the resource limits on a pod over its lifetime. In particular, this is valuable for pets (i.e., pods that are very costly to destroy and re-create).&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/features/issues/460&#34; target=&#34;_blank&#34;&gt;Encryption at rest via KMS&lt;/a&gt; is now in beta. This adds multiple encryption providers, including Google Cloud KMS, Azure Key Vault, AWS KMS, and Hashicorp Vault, that will encrypt data as it is stored to etcd.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.12 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.12.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;. You can also install 1.12 using &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;Kubeadm&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;5-day-features-blog-series&#34;&gt;5 Day Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back next week for our 5 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Day 1 - Kubelet TLS Bootstrap&lt;/li&gt;
&lt;li&gt;Day 2 - Support for Azure Virtual Machine Scale Sets (VMSS) and Cluster-Autoscaler&lt;/li&gt;
&lt;li&gt;Day 3 - Snapshots Functionality&lt;/li&gt;
&lt;li&gt;Day 4 - RuntimeClass&lt;/li&gt;
&lt;li&gt;Day 5 - Topology Resources&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Tim Pepper, Orchestration &amp;amp; Containers Lead, at VMware Open Source Technology Center. The 36 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 22,000 individual contributors to date and an active community of more than 45,000 people.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average, 259 different companies and over 1,400 individuals contribute to Kubernetes each month. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Ygrene&lt;/strong&gt;, a PACE (Property Assessed Clean Energy) financing company, is using cloud native to &lt;a href=&#34;https://kubernetes.io/case-studies/ygrene/&#34; target=&#34;_blank&#34;&gt;bring security and scalability to the finance industry&lt;/a&gt;, cutting deployment times down to five minutes with Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sling TV&lt;/strong&gt;, a live TV streaming service, uses Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/slingtv/&#34; target=&#34;_blank&#34;&gt;enable their hybrid cloud strategy&lt;/a&gt; and deliver a high-quality service for their customers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ING&lt;/strong&gt;, a Dutch multinational banking and financial services corporation, moved to Kubernetes with the intent to eventually be able to go from &lt;a href=&#34;https://kubernetes.io/case-studies/ing/&#34; target=&#34;_blank&#34;&gt;idea to production within 48 hours&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pinterest&lt;/strong&gt;, a web and mobile application company that is running on 1,000 microservices and hundreds of thousands of data jobs, moved to Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/pinterest/&#34; target=&#34;_blank&#34;&gt;build on-demand scaling and simply the deployment process&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pearson&lt;/strong&gt;, a global education company serving 75 million learners, is using Kubernetes to &lt;a href=&#34;https://kubernetes.io/case-studies/pearson/&#34; target=&#34;_blank&#34;&gt;transform the way that educational content is delivered online&lt;/a&gt; and has saved 15-20% in developer productivity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? &lt;a href=&#34;https://docs.google.com/a/google.com/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;Share your story&lt;/a&gt; with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;CNCF recently released the findings of their &lt;a href=&#34;https://www.cncf.io/blog/2018/08/29/cncf-survey-use-of-cloud-native-technologies-in-production-has-grown-over-200-percent/&#34; target=&#34;_blank&#34;&gt;bi-annual CNCF survey&lt;/a&gt;, finding that the use of cloud native technologies in production has grown over 200% within the last six months.&lt;/li&gt;
&lt;li&gt;CNCF expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual&amp;rsquo;s ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found &lt;a href=&#34;https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online training&lt;/a&gt; that teaches the skills needed to create and configure a real-world Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Kubernetes documentation now features &lt;a href=&#34;https://k8s.io/docs/home/&#34; target=&#34;_blank&#34;&gt;user journeys&lt;/a&gt;: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to &lt;a href=&#34;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/&#34; target=&#34;_blank&#34;&gt;Shanghai&lt;/a&gt; from November 13-15, 2018 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;Seattle&lt;/a&gt; from December 10-13, 2018. This conference will feature technical sessions, case studies, developer deep dives, salons and more! &lt;a href=&#34;https://www.cncf.io/community/kubecon-cloudnativecon-events/&#34; target=&#34;_blank&#34;&gt;Register today&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.12 release team on November 6th at 10am PDT to learn about the major features in this release. Register &lt;a href=&#34;https://zoom.us/webinar/register/WN_DYMejau3TMaTbk91oC3YkA&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Hands On With Linkerd 2.0</title>
      <link>https://docstest.github.io/blog/2018/09/18/hands-on-with-linkerd-2.0/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/09/18/hands-on-with-linkerd-2.0/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Thomas Rampelberg (Buoyant)&lt;/p&gt;

&lt;p&gt;Linkerd 2.0 was recently announced as generally available (GA), signaling its readiness for production use. In this tutorial, we’ll walk you through how to get Linkerd 2.0 up and running on your Kubernetes cluster in a matter seconds.&lt;/p&gt;

&lt;p&gt;But first, what is Linkerd and why should you care? Linkerd is a service sidecar that augments a Kubernetes service, providing zero-config dashboards and UNIX-style CLI tools for runtime debugging, diagnostics, and reliability. Linkerd is also a service mesh, applied to multiple (or all) services in a cluster to provide a uniform layer of telemetry, security, and control across them.&lt;/p&gt;

&lt;p&gt;Linkerd works by installing ultralight proxies into each pod of a service. These proxies report telemetry data to, and receive signals from, a control plane. This means that using Linkerd doesn’t require any code changes, and can even be installed live on a running service. Linkerd is fully open source, Apache v2 licensed, and is hosted by the Cloud Native Computing Foundation (just like Kubernetes itself!)&lt;/p&gt;

&lt;p&gt;Without further ado, let’s see just how quickly you can get Linkerd running on your Kubernetes cluster. In this tutorial, we’ll walk you through how to deploy Linkerd on any Kubernetes 1.9+ cluster and how to use it to debug failures in a sample gRPC application.&lt;/p&gt;

&lt;h2 id=&#34;step-1-install-the-demo-app&#34;&gt;Step 1: Install the demo app 🚀&lt;/h2&gt;

&lt;p&gt;Before we install Linkerd, let’s start by installing a basic gRPC demo application called Emojivoto onto your Kubernetes cluster. To install Emojivoto, run:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl https://run.linkerd.io/emojivoto.yml | kubectl apply -f -&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command downloads the Kubernetes manifest for Emojivoto, and uses kubectl to apply it to your Kubernetes cluster. Emojivoto is comprised of several services that run in the “emojivoto” namespace. You can see the services by running:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl get -n emojivoto deployments&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You can also see the app live by running&lt;/p&gt;

&lt;p&gt;&lt;code&gt;minikube -n emojivoto service web-svc --url # if you’re on minikube&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;… or:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl get svc web-svc -n emojivoto -o jsonpath=&amp;quot;{.status.loadBalancer.ingress[0].*}&amp;quot; #&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;… if you’re somewhere else&lt;/p&gt;

&lt;p&gt;Click around. You might notice that some parts of the application are broken! If you were to inspect your handly local Kubernetes dashboard, you wouldn’t see very much interesting&amp;mdash;as far as Kubernetes is concerned, the app is running just fine. This is a very common situation! Kubernetes understands whether your pods are running, but not whether they are responding properly.&lt;/p&gt;

&lt;p&gt;In the next few steps, we’ll walk you through how to use Linkerd to diagnose the problem.&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-linkerd-s-cli&#34;&gt;Step 2: Install Linkerd’s CLI&lt;/h2&gt;

&lt;p&gt;We’ll start by installing Linkerd’s command-line interface (CLI) onto your local machine.  Visit the &lt;a href=&#34;https://github.com/linkerd/linkerd2/releases/&#34; target=&#34;_blank&#34;&gt;Linkerd releases page&lt;/a&gt;, or simply run:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;curl -sL https://run.linkerd.io/install | sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Once installed, add the &lt;code&gt;linkerd&lt;/code&gt; command to your path with:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;export PATH=$PATH:$HOME/.linkerd2/bin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You should now be able to run the command &lt;code&gt;linkerd version&lt;/code&gt;, which should display:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Client version: v2.0
Server version: unavailable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;“Server version: unavailable” means that we need to add Linkerd’s control plane to the cluster, which we’ll do next. But first, let’s validate that your cluster is prepared for Linkerd by running:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;linkerd check --pre&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This handy command will report any problems that will interfere with your ability to install Linkerd. Hopefully everything looks OK and you’re ready to move on to the next step.&lt;/p&gt;

&lt;h2 id=&#34;step-3-install-linkerd-s-control-plane-onto-the-cluster&#34;&gt;Step 3: Install Linkerd’s control plane onto the cluster&lt;/h2&gt;

&lt;p&gt;In this step, we’ll install Linkerd’s lightweight control plane into its own namespace (“linkerd”) on your cluster. To do this, run:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;linkerd install | kubectl apply -f -&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command generates a Kubernetes manifest and uses &lt;code&gt;kubectl&lt;/code&gt; command to apply it to your Kubernetes cluster. (Feel free to inspect the manifest before you apply it.)&lt;/p&gt;

&lt;p&gt;(Note: if your Kubernetes cluster is on GKE with RBAC enabled, you’ll need an extra step: you must grant a ClusterRole of cluster-admin to your Google Cloud account first, in order to install certain telemetry features in the control plane. To do that, run: &lt;code&gt;kubectl create clusterrolebinding cluster-admin-binding-$USER --clusterrole=cluster-admin --user=$(gcloud config get-value account)&lt;/code&gt;.)&lt;/p&gt;

&lt;p&gt;Depending on the speed of your internet connection, it may take a minute or two for your Kubernetes cluster to pull the Linkerd images. While that’s happening, we can validate that everything’s happening correctly by running:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;linkerd check&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command will patiently wait until Linkerd has been installed and is running.&lt;/p&gt;

&lt;p&gt;Finally, we’re ready to view Linkerd’s dashboard! Just run:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;linkerd dashboard&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see something like below, Linkerd is now running on your cluster. 🎉&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/1-dashboard.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-4-add-linkerd-to-the-web-service&#34;&gt;Step 4: Add Linkerd to the web service&lt;/h2&gt;

&lt;p&gt;At this point we have the Linkerd control plane installed in the “linkerd” namespace, and we have our emojivoto demo app installed in the “emojivoto” namespace. But we haven’t actually added Linkerd to our service yet. So let’s do that.&lt;/p&gt;

&lt;p&gt;In this example, let’s pretend we are the owners of the “web” service. Other services, like “emoji” and “voting”, are owned by other teams&amp;ndash;so we don’t want to touch them.&lt;/p&gt;

&lt;p&gt;There are a couple ways to add Linkerd to our service. For demo purposes, the easiest is to do something like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl get -n emojivoto deploy/web -o yaml | linkerd inject - | kubectl apply -f -&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This command retrieves the manifest of the “web” service from Kubernetes, runs this manifest through &lt;code&gt;linkerd inject&lt;/code&gt;, and finally reapplies it to the Kubernetes cluster. The &lt;code&gt;linkerd inject&lt;/code&gt; command augments the manifest to include Linkerd’s data plane proxies. As with &lt;code&gt;linkerd install&lt;/code&gt;, &lt;code&gt;linkerd inject&lt;/code&gt; is a pure text operation, meaning that you can inspect the input and output before you use it. Since “web” is a Deployment, Kubernetes is kind enough to slowly roll the service one pod at a time&amp;ndash;meaning that “web” can be serving traffic live while we add Linkerd to it!&lt;/p&gt;

&lt;p&gt;We now have a service sidecar running on the “web” service!&lt;/p&gt;

&lt;h2 id=&#34;step-5-debugging-for-fun-and-for-profit&#34;&gt;Step 5: Debugging for Fun and for Profit&lt;/h2&gt;

&lt;p&gt;Congratulations! You now have a full gRPC application running on your Kubernetes cluster with Linkerd installed on the “web” service. Of course, that application is failing when you use it&amp;ndash;so now let’s use Linkerd to track down those errors.&lt;/p&gt;

&lt;p&gt;If you glance at the Linkerd dashboard (the &lt;code&gt;linkerd dashboard&lt;/code&gt; command), you should see all services in the “emojivoto” namespace show up. Since “web” has the Linkerd service sidecar installed on it, you’ll also see success rate, requests per second, and latency percentiles show up.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/2-web-overview.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;That’s pretty neat, but the first thing you might notice is that success rate is well below 100%! Click on “web” and let’s dig in.&lt;/p&gt;

&lt;p&gt;You should now be looking at the Deployment page for the web service. The first thing you’ll see here is that web is taking traffic from vote-bot (a service included in the Emojivoto manifest to continually generate a low level of live traffic), and has two outgoing dependencies, emoji and voting.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/3-web-detail.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The emoji service is operating at 100%, but the voting service is failing! A failure in a dependent service may be exactly what’s causing the errors that web is returning.&lt;/p&gt;

&lt;p&gt;Let’s scroll a little further down the page, we’ll see a live list of all traffic endpoints that “web” is receiving. This is interesting:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/4-web-top.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;There are two calls that are not at 100%: the first is vote-bot’s call the “/api/vote” endpoint. The second is the “VotePoop” call from the web service to the voting service. Very interesting! Since /api/vote is an incoming call, and “/VotePoop” is an outgoing call, this is a good clue that the failure of the vote service’s VotePoop endpoint is what’s causing the problem!&lt;/p&gt;

&lt;p&gt;Finally, if we click on the “tap” icon for that row in the far right column, we’ll be taken to live list of requests that match this endpoint. This allows us to confirm that the requests are failing (they all have &lt;a href=&#34;https://godoc.org/google.golang.org/grpc/codes#Code&#34; target=&#34;_blank&#34;&gt;gRPC status code 2&lt;/a&gt;, indicating an error).&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/5-web-tap.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;At this point we have the ammunition we need to talk to the owners of the vote “voting” service. We’ve identified an endpoint on their service that consistently returns an error, and have found no other obvious sources of failures in the system.&lt;/p&gt;

&lt;p&gt;We hope you’ve enjoyed this journey through Linkerd 2.0. There is much more for you to explore. For example, everything we did above using the web UI can also be accomplished via pure CLI commands, e.g. &lt;code&gt;linkerd top&lt;/code&gt;, &lt;code&gt;linkerd stat&lt;/code&gt;, and &lt;code&gt;linkerd tap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Also, did you notice the little Grafana icon on the very first page we looked at? Linkerd ships with automatic Grafana dashboards for all those metrics, allowing you to view everything you’re seeing in the Linkerd dashboard in a time series format. Check it out!&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-18-2018-linkerd-2.0/6-grafana.png&#34; width=&#34;700&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;want-more&#34;&gt;Want more?&lt;/h2&gt;

&lt;p&gt;In this tutorial, we’ve shown you how to install Linkerd on a cluster, add it as a service sidecar to just one service&amp;ndash;while the service is receiving live traffic!&amp;mdash;and use it to debug a runtime issue. But this is just the tip of the iceberg. We haven’t even touched any of Linkerd’s reliability or security features!&lt;/p&gt;

&lt;p&gt;Linkerd has a thriving community of adopters and contributors, and we’d love for YOU to be a part of it. For more, check out the &lt;a href=&#34;https://linkerd.io/docs&#34; target=&#34;_blank&#34;&gt;docs&lt;/a&gt; and &lt;a href=&#34;https://github.com/linkerd/linkerd&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; repo, join the &lt;a href=&#34;https://slack.linkerd.io/&#34; target=&#34;_blank&#34;&gt;Linkerd Slack&lt;/a&gt; and mailing lists (&lt;a href=&#34;https://lists.cncf.io/g/cncf-linkerd-users&#34; target=&#34;_blank&#34;&gt;users&lt;/a&gt;, &lt;a href=&#34;https://lists.cncf.io/g/cncf-linkerd-dev&#34; target=&#34;_blank&#34;&gt;developers&lt;/a&gt;, &lt;a href=&#34;https://lists.cncf.io/g/cncf-linkerd-announce&#34; target=&#34;_blank&#34;&gt;announce&lt;/a&gt;), and, of course, follow &lt;a href=&#34;https://twitter.com/linkerd&#34; target=&#34;_blank&#34;&gt;@linkerd&lt;/a&gt; on Twitter! We can’t wait to have you aboard!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 2018 Steering Committee Election Cycle Kicks Off</title>
      <link>https://docstest.github.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/</link>
      <pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Paris Pittman (Google), Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF)&lt;/p&gt;

&lt;p&gt;Having a clear, definable governance model is crucial for the health of open source projects. For one of the highest velocity projects in the open source world, governance is critical especially for one as large and active as Kubernetes, which is one of the most high-velocity projects in the open source world. A clear structure helps users trust that the project will be nurtured and progress forward. Initially, this structure was laid by the former 7 member bootstrap committee composed of founders and senior contributors with a goal to create the foundational governance building blocks.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://git.k8s.io/steering/charter.md&#34; target=&#34;_blank&#34;&gt;initial charter&lt;/a&gt; and establishment of an election process to seat a full Steering Committee was a part of those first building blocks. Last year, the bootstrap committee &lt;a href=&#34;https://groups.google.com/d/msg/kubernetes-dev/piPuoqFkJwA/mCjwLH81BgAJ&#34; target=&#34;_blank&#34;&gt;kicked off&lt;/a&gt; the first Kubernetes Steering Committee election which brought forth 6 new members from the community as voted on by contributors. These new members plus the bootstrap committee formed the &lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Steering Committee that we know today&lt;/a&gt;. This yearly election cycle will continue to ensure that new representatives get cycled through to add different voices and thoughts on the Kubernetes project strategy.&lt;/p&gt;

&lt;p&gt;The committee has worked hard on topics that will streamline the project and how we operate. SIG (Special Interest Group) governance was an overarching recurring theme this year: Kubernetes community is not a monolithic organization, but a huge, distributed community, where &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups (SIGs) and Working Groups (WGs)&lt;/a&gt; are the atomic community units, that are making Kubernetes so successful from the ground.&lt;/p&gt;

&lt;h3 id=&#34;contributors-this-is-where-you-come-in&#34;&gt;Contributors - this is where you come in.&lt;/h3&gt;

&lt;p&gt;There are three seats up for election this year. The &lt;a href=&#34;https://git.k8s.io/community/events/elections/2018&#34; target=&#34;_blank&#34;&gt;voters guide&lt;/a&gt; will get you up to speed on the specifics of this years election including candidate bios as they are updated in real time. The &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/elections.md&#34; target=&#34;_blank&#34;&gt;elections process doc&lt;/a&gt; will steer you towards eligibility, operations, and the fine print.&lt;/p&gt;

&lt;p&gt;1) Nominate yourself, someone else, and/or put your support to others.&lt;/p&gt;

&lt;p&gt;Want to help chart our course? Interested in governance and community topics? Add your name! &lt;em&gt;The nomination process is optional&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;2) Vote.&lt;/p&gt;

&lt;p&gt;On September 19th, eligible voters will receive an email poll invite conducted by &lt;a href=&#34;https://civs.cs.cornell.edu&#34; target=&#34;_blank&#34;&gt;CIVS&lt;/a&gt;. The newly elected will be announced at the &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/communication#weekly-meeting&#34; target=&#34;_blank&#34;&gt;weekly community meeting&lt;/a&gt; on Thursday, October 4th at 5pm UTC.&lt;/p&gt;

&lt;p&gt;To those who are running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-09-06-2018-steering-committee-election-cycle-kicks-off/sc-elections.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;helpful-resources&#34;&gt;Helpful resources&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/steering&#34; target=&#34;_blank&#34;&gt;Steering Committee&lt;/a&gt; - who sits on the committee and terms, their projects and meetings info&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git.k8s.io/steering/charter.md&#34; target=&#34;_blank&#34;&gt;Steering Committee Charter&lt;/a&gt; - this is a great read if you’re interested in running (or assessing for the best candidates!)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git.k8s.io/steering/elections.md&#34; target=&#34;_blank&#34;&gt;Election Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://git.k8s.io/community/events/elections/2018&#34; target=&#34;_blank&#34;&gt;Voters Guide!&lt;/a&gt; - Updated on a rolling basis. This guide will always have the latest information throughout the election cycle. The complete schedule of events and candidate bios will be housed here.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title>
      <link>https://docstest.github.io/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link>
      <pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Aaron Crickenberger (Google) and Benjamin Elder (Google)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Large projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.”&lt;/em&gt; - &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/values.md#automation-over-process&#34; target=&#34;_blank&#34;&gt;Kubernetes Community Values&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.&lt;/p&gt;

&lt;p&gt;This strategy worked. It worked so well that the project quickly scaled past its contributors’ capacity as humans. What followed was an incredible journey of automation and innovation. We didn’t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.&lt;/p&gt;

&lt;h2 id=&#34;the-work&#34;&gt;The Work&lt;/h2&gt;

&lt;p&gt;Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.&lt;/p&gt;

&lt;p&gt;Further experience revealed other areas where machines could do the work for us:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PR Workflow

&lt;ul&gt;
&lt;li&gt;Did the contributor sign our CLA?&lt;/li&gt;
&lt;li&gt;Did the PR pass tests?&lt;/li&gt;
&lt;li&gt;Is the PR mergeable?&lt;/li&gt;
&lt;li&gt;Did the merge commit pass tests?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Triage

&lt;ul&gt;
&lt;li&gt;Who should be reviewing PRs?&lt;/li&gt;
&lt;li&gt;Is there enough information to route an issue to the right people?&lt;/li&gt;
&lt;li&gt;Is an issue still relevant?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Project Health

&lt;ul&gt;
&lt;li&gt;What is happening in the project?&lt;/li&gt;
&lt;li&gt;What should we be paying attention to?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As we developed automation to improve our situation, we followed a few guiding principles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Follow the push/poll control loop patterns that worked well for Kubernetes&lt;/li&gt;
&lt;li&gt;Prefer stateless loosely coupled services that do one thing well&lt;/li&gt;
&lt;li&gt;Prefer empowering the entire community over empowering a few core contributors&lt;/li&gt;
&lt;li&gt;Eat our own dogfood and avoid reinventing wheels&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;enter-prow&#34;&gt;Enter Prow&lt;/h2&gt;

&lt;p&gt;This led us to create &lt;a href=&#34;https://git.k8s.io/test-infra/prow&#34; target=&#34;_blank&#34;&gt;Prow&lt;/a&gt; as the central component for our automation. Prow is sort of like an &lt;a href=&#34;https://ifttt.com/&#34; target=&#34;_blank&#34;&gt;If This, Then That&lt;/a&gt; for GitHub events, with a built-in library of &lt;a href=&#34;https://prow.k8s.io/command-help&#34; target=&#34;_blank&#34;&gt;commands&lt;/a&gt;, &lt;a href=&#34;https://prow.k8s.io/plugins&#34; target=&#34;_blank&#34;&gt;plugins&lt;/a&gt;, and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.&lt;/p&gt;

&lt;p&gt;Prow lets us do things like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Allow our community to triage issues/PRs by commenting commands such as “/priority critical-urgent”, “/assign mary” or “/close”&lt;/li&gt;
&lt;li&gt;Auto-label PRs based on how much code they change, or which files they touch&lt;/li&gt;
&lt;li&gt;Age out issues/PRs that have remained inactive for too long&lt;/li&gt;
&lt;li&gt;Auto-merge PRs that meet our PR workflow requirements&lt;/li&gt;
&lt;li&gt;Run CI jobs defined as &lt;a href=&#34;https://github.com/knative/build&#34; target=&#34;_blank&#34;&gt;Knative Builds&lt;/a&gt;, Kubernetes Pods, or Jenkins jobs&lt;/li&gt;
&lt;li&gt;Enforce org-wide and per-repo GitHub policies like &lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector&#34; target=&#34;_blank&#34;&gt;branch protection&lt;/a&gt; and &lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/label_sync&#34; target=&#34;_blank&#34;&gt;GitHub labels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. &lt;a href=&#34;https://github.com/kubernetes/test-infra/blob/master/prow/getting_started.md&#34; target=&#34;_blank&#34;&gt;Getting started with Prow&lt;/a&gt; takes a Kubernetes cluster and &lt;code&gt;kubectl apply starter.yaml&lt;/code&gt; (running pods on a Kubernetes cluster).&lt;/p&gt;

&lt;p&gt;Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/boskos&#34; target=&#34;_blank&#34;&gt;Boskos&lt;/a&gt;: manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/ghproxy&#34; target=&#34;_blank&#34;&gt;ghProxy&lt;/a&gt;: a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesn’t hit API limits (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/greenhouse&#34; target=&#34;_blank&#34;&gt;Greenhouse&lt;/a&gt;: allows us to use a remote bazel cache to provide faster build and test results for PRs (&lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1&#34; target=&#34;_blank&#34;&gt;with monitoring&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice&#34; target=&#34;_blank&#34;&gt;Splice&lt;/a&gt;: allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide&#34; target=&#34;_blank&#34;&gt;Tide&lt;/a&gt;: allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;scaling-project-health&#34;&gt;Scaling Project Health&lt;/h2&gt;

&lt;p&gt;With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/gubernator&#34; target=&#34;_blank&#34;&gt;Gubernator&lt;/a&gt;: display the results and test history for a given PR&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/test-infra/tree/master/kettle&#34; target=&#34;_blank&#34;&gt;Kettle&lt;/a&gt;: transfer data from GCS to a publicly accessible bigquery dataset&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s-gubernator.appspot.com/pr&#34; target=&#34;_blank&#34;&gt;PR dashboard&lt;/a&gt;: a workflow-aware dashboard that allows contributors to understand which PRs require attention and why&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://storage.googleapis.com/k8s-gubernator/triage/index.html&#34; target=&#34;_blank&#34;&gt;Triage&lt;/a&gt;: identify common failures that happen across all jobs and tests&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s-testgrid.appspot.com/&#34; target=&#34;_blank&#34;&gt;Testgrid&lt;/a&gt;: display test results for a given job across all runs, summarize test results across groups of jobs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1&#34; target=&#34;_blank&#34;&gt;Which prow commands are people most actively using&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All&#34; target=&#34;_blank&#34;&gt;PR reviews by contributor over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&#34; target=&#34;_blank&#34;&gt;Time spent in each phase of our PR workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;into-the-beyond&#34;&gt;Into the Beyond&lt;/h2&gt;

&lt;p&gt;Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had &lt;a href=&#34;https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&#34; target=&#34;_blank&#34;&gt;participation from over 13,800 unique developers&lt;/a&gt; on GitHub.&lt;/p&gt;

&lt;p&gt;On any given weekday our Prow instance &lt;a href=&#34;http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now&#34; target=&#34;_blank&#34;&gt;runs over 10,000 CI jobs&lt;/a&gt;; from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.&lt;/p&gt;

&lt;p&gt;With today’s &lt;a href=&#34;https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google&#34; target=&#34;_blank&#34;&gt;announcement from CNCF&lt;/a&gt; – noting that Google Cloud has begun transferring ownership and management of the Kubernetes project’s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.&lt;/p&gt;

&lt;p&gt;Want to find out more? Come check out these resources:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bentheelder.io/posts/prow&#34; target=&#34;_blank&#34;&gt;Prow: Testing the way to Kubernetes Next&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BsIC7gPkH5M&#34; target=&#34;_blank&#34;&gt;Automation and the Kubernetes Contributor Experience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Introducing Kubebuilder: an SDK for building Kubernetes APIs using CRDs</title>
      <link>https://docstest.github.io/blog/2018/08/10/introducing-kubebuilder-an-sdk-for-building-kubernetes-apis-using-crds/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/08/10/introducing-kubebuilder-an-sdk-for-building-kubernetes-apis-using-crds/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Phillip Wittrock (Google), Sunil Arora (Google)&lt;/p&gt;

&lt;p&gt;How can we enable applications such as MySQL, Spark and Cassandra to manage themselves just like Kubernetes Deployments and Pods do? How do we configure these applications as their own first class APIs instead of a collection of StatefulSets, Services, and ConfigMaps?&lt;/p&gt;

&lt;p&gt;We have been working on a solution and are happy to introduce &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;kubebuilder&lt;/em&gt;&lt;/a&gt;, a comprehensive development kit for rapidly building and publishing Kubernetes APIs and Controllers using CRDs. Kubebuilder scaffolds projects and API definitions and is built on top of the &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34; target=&#34;_blank&#34;&gt;controller-runtime&lt;/a&gt; libraries.&lt;/p&gt;

&lt;h3 id=&#34;why-kubebuilder-and-kubernetes-apis&#34;&gt;Why Kubebuilder and Kubernetes APIs?&lt;/h3&gt;

&lt;p&gt;Applications and cluster resources typically require some operational work - whether it is replacing failed replicas with new ones, or scaling replica counts while resharding data. Running the MySQL application may require scheduling backups, reconfiguring replicas after scaling, setting up failure detection and remediation, etc.&lt;/p&gt;

&lt;p&gt;With the Kubernetes API model, management logic is embedded directly into an application specific Kubernetes API, e.g. a “MySQL” API. Users then declaratively manage the application through YAML configuration using tools such as kubectl, just like they do for Kubernetes objects. This approach is referred to as an Application Controller, also known as an Operator. Controllers are a powerful technique backing the core Kubernetes APIs that may be used to build many kinds of solutions in addition to Applications; such as Autoscalers, Workload APIs, Configuration APIs, CI/CD systems, and more.&lt;/p&gt;

&lt;p&gt;However, while it has been possible for trailblazers to build new Controllers on top of the raw API machinery, doing so has been a DIY “from scratch” experience, requiring developers to learn low level details about how Kubernetes libraries are implemented, handwrite boilerplate code, and wrap their own solutions for integration testing, RBAC configuration, documentation, etc. Kubebuilder makes this experience simple and easy by applying the lessons learned from building the core Kubernetes APIs.&lt;/p&gt;

&lt;h3 id=&#34;getting-started-building-application-controllers-and-kubernetes-apis&#34;&gt;Getting Started Building Application Controllers and Kubernetes APIs&lt;/h3&gt;

&lt;p&gt;By providing an opinionated and structured solution for creating Controllers and Kubernetes APIs, developers have a working “out of the box” experience that uses the lessons and best practices learned from developing the core Kubernetes APIs. Creating a new &amp;ldquo;Hello World&amp;rdquo; Controller with &lt;code&gt;kubebuilder&lt;/code&gt; is as simple as:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a project with &lt;code&gt;kubebuilder init&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Define a new API with &lt;code&gt;kubebuilder create api&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Build and run the provided main function with &lt;code&gt;make install &amp;amp; make run&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This will scaffold the API and Controller for users to modify, as well as scaffold integration tests, RBAC rules, Dockerfiles, Makefiles, etc.
After adding their implementation to the project, users create the artifacts to publish their API through:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Build and push the container image from the provided Dockerfile using &lt;code&gt;make docker-build&lt;/code&gt; and &lt;code&gt;make docker-push&lt;/code&gt; commands&lt;/li&gt;
&lt;li&gt;Deploy the API using &lt;code&gt;make deploy&lt;/code&gt; command&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Whether you are already a Controller aficionado or just want to learn what the buzz is about, check out the &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34; target=&#34;_blank&#34;&gt;kubebuilder repo&lt;/a&gt; or take a look at an example in the &lt;a href=&#34;https://book.kubebuilder.io&#34; target=&#34;_blank&#34;&gt;kubebuilder book&lt;/a&gt; to learn about how simple and easy it is to build Controllers.&lt;/p&gt;

&lt;h3 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h3&gt;

&lt;p&gt;Kubebuilder is a project under &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-api-machinery&#34; target=&#34;_blank&#34;&gt;SIG API Machinery&lt;/a&gt; and is being actively developed by contributors from many companies such as Google, Red Hat, VMware, Huawei and others. Get involved by giving us feedback through these channels:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubebuilder &lt;a href=&#34;https://slack.k8s.io/#kubebuilder&#34; target=&#34;_blank&#34;&gt;chat room on Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SIG &lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery&#34; target=&#34;_blank&#34;&gt;mailing list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder/issues/new&#34; target=&#34;_blank&#34;&gt;Github issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Send a pull request in the &lt;a href=&#34;https://github.com/kubernetes-sigs/kubebuilder&#34; target=&#34;_blank&#34;&gt;kubebuilder repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Out of the Clouds onto the Ground: How to Make Kubernetes Production Grade Anywhere</title>
      <link>https://docstest.github.io/blog/2018/08/03/out-of-the-clouds-onto-the-ground-how-to-make-kubernetes-production-grade-anywhere/</link>
      <pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/08/03/out-of-the-clouds-onto-the-ground-how-to-make-kubernetes-production-grade-anywhere/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Steven Wong (VMware), Michael Gasch (VMware)&lt;/p&gt;

&lt;p&gt;This blog offers some guidelines for running a production grade Kubernetes cluster in an environment like an on-premise data center or edge location.&lt;/p&gt;

&lt;p&gt;What does it mean to be “production grade”?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The installation is secure&lt;/li&gt;
&lt;li&gt;The deployment is managed with a repeatable and recorded process&lt;/li&gt;
&lt;li&gt;Performance is predictable and consistent&lt;/li&gt;
&lt;li&gt;Updates and configuration changes can be safely applied&lt;/li&gt;
&lt;li&gt;Logging and monitoring is in place to detect and diagnose failures and resource shortages&lt;/li&gt;
&lt;li&gt;Service is “highly available enough” considering available resources, including constraints on money, physical space, power, etc.&lt;/li&gt;
&lt;li&gt;A recovery process is available, documented, and tested for use in the event of failures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, production grade means anticipating accidents and preparing for recovery with minimal pain and delay.&lt;/p&gt;

&lt;p&gt;This article is directed at on-premise Kubernetes deployments on a hypervisor or bare-metal platform, facing finite backing resources compared to the expansibility of the major public clouds. However, some of these recommendations may also be useful in a public cloud if budget constraints limit the resources you choose to consume.&lt;/p&gt;

&lt;p&gt;A single node bare-metal Minikube deployment may be cheap and easy, but is not production grade. Conversely, you’re not likely to achieve Google’s Borg experience in a retail store, branch office, or edge location, nor are you likely to need it.&lt;/p&gt;

&lt;p&gt;This blog offers some guidance on achieving a production worthy Kubernetes deployment, even when dealing with some resource constraints.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/without-incidence.png&#34; alt=&#34;without incidence&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;critical-components-in-a-kubernetes-cluster&#34;&gt;Critical components in a Kubernetes cluster&lt;/h2&gt;

&lt;p&gt;Before we dive into the details, it is critical to understand the overall Kubernetes architecture.&lt;/p&gt;

&lt;p&gt;A Kubernetes cluster is a highly distributed system based on a control plane and clustered worker node architecture as depicted below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/api-server.png&#34; alt=&#34;api server&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Typically the API server, Controller Manager and Scheduler components are co-located within multiple instances of control plane (aka Master) nodes. Master nodes usually include etcd too, although there are high availability and large cluster scenarios that call for running etcd on independent hosts. The components can be run as containers, and optionally be supervised by Kubernetes, i.e. running as statics pods.&lt;/p&gt;

&lt;p&gt;For high availability, redundant instances of these components are used. The importance and required degree of redundancy varies.&lt;/p&gt;

&lt;h3 id=&#34;kubernetes-components-from-an-ha-perspective&#34;&gt;Kubernetes components from an HA perspective&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/kubernetes-components-ha.png&#34; alt=&#34;kubernetes components HA&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Risks to these components include hardware failures, software bugs, bad updates, human errors, network outages, and overloaded systems resulting in resource exhaustion. Redundancy can mitigate the impact of many of these hazards. In addition, the resource scheduling and high availability features of a hypervisor platform can be useful to surpass what can be achieved using the Linux operating system, Kubernetes, and a container runtime alone.&lt;/p&gt;

&lt;p&gt;The API Server uses multiple instances behind a load balancer to achieve scale and availability. The load balancer is a critical component for purposes of high availability. Multiple DNS API Server ‘A’ records might be an alternative if you don’t have a load balancer.&lt;/p&gt;

&lt;p&gt;The kube-scheduler and kube-controller-manager engage in a leader election process, rather than utilizing a load balancer. Since a &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/&#34; target=&#34;_blank&#34;&gt;cloud-controller-manager&lt;/a&gt; is used for selected types of hosting infrastructure, and these have implementation variations, they will not be discussed, beyond indicating that they are a control plane component.&lt;/p&gt;

&lt;p&gt;Pods running on Kubernetes worker nodes are managed by the kubelet agent. Each worker instance runs the kubelet agent and a &lt;a href=&#34;https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/&#34; target=&#34;_blank&#34;&gt;CRI-compatible&lt;/a&gt; container runtime. Kubernetes itself is designed to monitor and recover from worker node outages. But for critical workloads, hypervisor resource management, workload isolation and availability features can be used to enhance availability and make performance more predictable.&lt;/p&gt;

&lt;h2 id=&#34;etcd&#34;&gt;etcd&lt;/h2&gt;

&lt;p&gt;etcd is the persistent store for all Kubernetes objects. The availability and recoverability of the etcd cluster should be the first consideration in a production-grade Kubernetes deployment.&lt;/p&gt;

&lt;p&gt;A five-node etcd cluster is a best practice if you can afford it. Why? Because you could engage in maintenance on one and still tolerate a failure. A three-node cluster is the minimum &lt;a href=&#34;https://coreos.com/etcd/docs/latest/v2/admin_guide.html#optimal-cluster-size&#34; target=&#34;_blank&#34;&gt;recommendation&lt;/a&gt; for production-grade service, even if only a single hypervisor host is available. More than seven nodes is not recommended except for &lt;a href=&#34;https://monzo.com/blog/2017/11/29/very-robust-etcd/&#34; target=&#34;_blank&#34;&gt;very large installations&lt;/a&gt; straddling multiple availability zones.&lt;/p&gt;

&lt;p&gt;The minimum recommendation for hosting an etcd cluster node is 2GB of RAM with 8GB of SSD-backed disk. Usually, 8GB RAM and a 20GB disk will be enough. Disk performance affects failed node recovery time. See &lt;a href=&#34;https://coreos.com/etcd/docs/latest/op-guide/hardware.html&#34; target=&#34;_blank&#34;&gt;https://coreos.com/etcd/docs/latest/op-guide/hardware.html&lt;/a&gt; for more on this.&lt;/p&gt;

&lt;h3 id=&#34;consider-multiple-etcd-clusters-in-special-situations&#34;&gt;Consider multiple etcd clusters in special situations&lt;/h3&gt;

&lt;p&gt;For very large Kubernetes clusters, consider using a separate etcd cluster for Kubernetes events so that event storms do not impact the main Kubernetes API service. If you use flannel networking, it retains configuration in etcd and may have differing version requirements than Kubernetes, which can complicate etcd backup &amp;ndash; consider using a dedicated etcd cluster for flannel.&lt;/p&gt;

&lt;h2 id=&#34;single-host-deployments&#34;&gt;Single host deployments&lt;/h2&gt;

&lt;p&gt;The availability risk list includes hardware, software and people. If you are limited to a single host, the use of redundant storage, error-correcting memory and dual power supplies can reduce hardware failure exposure. Running a hypervisor on the physical host will allow operation of redundant software components and add operational advantages related to deployment, upgrade, and resource consumption governance, with predictable and repeatable performance under stress. For example, even if you can only afford to run singletons of the master services, they need to be protected from overload and resource exhaustion while competing with your application workload. A hypervisor can be more effective and easier to manage than configuring Linux scheduler priorities, cgroups, Kubernetes flags, etc.&lt;/p&gt;

&lt;p&gt;If resources on the host permit, you can deploy three etcd VMs. Each of the etcd VMs should be backed by a different physical storage device, or they should use separate partitions of a backing store using redundancy (mirroring, RAID, etc).&lt;/p&gt;

&lt;p&gt;Dual redundant instances of the API server, scheduler and controller manager would be the next upgrade, if your single host has the resources.&lt;/p&gt;

&lt;h3 id=&#34;single-host-deployment-options-least-production-worthy-to-better&#34;&gt;Single host deployment options, least production worthy to better&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/single-host-deployment.png&#34; alt=&#34;single host deployment&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dual-host-deployments&#34;&gt;Dual host deployments&lt;/h2&gt;

&lt;p&gt;With two hosts, storage concerns for etcd are the same as a single host, you want redundancy. And you would preferably run 3 etcd instances. Although possibly counter-intuitive, it is better to concentrate all etcd nodes on a single host. You do not gain reliability by doing a 2+1 split across two hosts -  because loss of the node holding the majority of etcd instances results in an outage, whether that majority is 2 or 3. If the hosts are not identical, put the whole etcd cluster on the most reliable host.&lt;/p&gt;

&lt;p&gt;Running redundant API Servers, kube-schedulers, and kube-controller-managers is recommended. These should be split across hosts to minimize risk due to container runtime, OS and hardware failures.&lt;/p&gt;

&lt;p&gt;Running a hypervisor layer on the physical hosts will allow operation of redundant software components with resource consumption governance, and can have planned maintenance operational advantages.&lt;/p&gt;

&lt;h3 id=&#34;dual-host-deployment-options-least-production-worthy-to-better&#34;&gt;Dual host deployment options, least production worthy to better&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/dual-host-deployment.png&#34; alt=&#34;dual host deployment&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Triple (or larger) host deployments &amp;ndash; Moving into uncompromised production-grade service
Splitting etcd across three hosts is recommended. A single hardware failure will reduce application workload capacity, but should not result in a complete service outage.&lt;/p&gt;

&lt;p&gt;With very large clusters, more etcd instances will be required.&lt;/p&gt;

&lt;p&gt;Running a hypervisor layer offers operational advantages and better workload isolation. It is beyond the scope of this article, but at the three-or-more host level, advanced features may be available (clustered redundant shared storage, resource governance with dynamic load balancing, automated health monitoring with live migration or failover).&lt;/p&gt;

&lt;h3 id=&#34;triple-or-more-host-options-least-production-worthy-to-better&#34;&gt;Triple (or more) host options, least production worthy to better&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/triple-host-deployment.png&#34; alt=&#34;triple host deployment&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;kubernetes-configuration-settings&#34;&gt;Kubernetes configuration settings&lt;/h2&gt;

&lt;p&gt;Master and Worker nodes should be protected from overload and resource exhaustion. Hypervisor features can be used to isolate critical components and reserve resources. There are also Kubernetes configuration settings that can throttle things like API call rates and pods per node. Some install suites and commercial distributions take care of this, but if you are performing a custom Kubernetes deployment, you may find that the defaults are not appropriate, particularly if your resources are small or your cluster is large.&lt;/p&gt;

&lt;p&gt;Resource consumption by the control plane will correlate with the number of pods and the pod churn rate. Very large and very small clusters will benefit from non-default &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34; target=&#34;_blank&#34;&gt;settings&lt;/a&gt; of kube-apiserver request throttling and memory. Having these too high can lead to request limit exceeded and out of memory errors.&lt;/p&gt;

&lt;p&gt;On worker nodes, &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/&#34; target=&#34;_blank&#34;&gt;Node Allocatable&lt;/a&gt; should be configured based on a reasonable supportable workload density at each node. Namespaces can be created to subdivide the worker node cluster into multiple virtual clusters with resource CPU and memory &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/&#34; target=&#34;_blank&#34;&gt;quotas&lt;/a&gt;. Kubelet handling of &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/&#34; target=&#34;_blank&#34;&gt;out of resource&lt;/a&gt; conditions can be configured.&lt;/p&gt;

&lt;h2 id=&#34;security&#34;&gt;Security&lt;/h2&gt;

&lt;p&gt;Every Kubernetes cluster has a cluster root Certificate Authority (CA). The Controller Manager, API Server, Scheduler, kubelet client, kube-proxy and administrator certificates need to be generated and installed. If you use an install tool or a distribution this may be handled for you. A manual process is described &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/04-certificate-authority.md&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. You should be prepared to reinstall certificates in the event of node replacements or expansions.&lt;/p&gt;

&lt;p&gt;As Kubernetes is entirely API driven, controlling and limiting who can access the cluster and what actions they are allowed to perform is essential. Encryption and authentication options are addressed in this &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Kubernetes application workloads are based on container images. You want the source and content of these images to be trustworthy. This will almost always mean that you will host a local container image repository. Pulling images from the public Internet can present both reliability and security issues. You should choose a repository that supports image signing, security scanning, access controls on pushing and pulling images, and logging of activity.&lt;/p&gt;

&lt;p&gt;Processes must be in place to support applying updates for host firmware, hypervisor, OS, Kubernetes, and other dependencies. Version monitoring should be in place to support audits.&lt;/p&gt;

&lt;p&gt;Recommendations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tighten security settings on the control plane components beyond defaults (e.g., &lt;a href=&#34;http://blog.kontena.io/locking-down-kubernetes-workers/&#34; target=&#34;_blank&#34;&gt;locking down worker nodes&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Utilize &lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34; target=&#34;_blank&#34;&gt;Pod Security Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consider the &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34;&gt;NetworkPolicy&lt;/a&gt; integration available with your networking solution, including how you will accomplish tracing, monitoring and troubleshooting.&lt;/li&gt;
&lt;li&gt;Use RBAC to drive authorization decisions and enforcement.&lt;/li&gt;
&lt;li&gt;Consider physical security, especially when deploying to edge or remote office locations that may be unattended. Include storage encryption to limit exposure from stolen devices and protection from attachment of malicious devices like USB keys.&lt;/li&gt;
&lt;li&gt;Protect Kubernetes plain-text cloud provider credentials (access keys, tokens, passwords, etc.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34; target=&#34;_blank&#34;&gt;secret&lt;/a&gt; objects are appropriate for holding small amounts of sensitive data. These are retained within etcd. These can be readily used to hold credentials for the Kubernetes API but there are times when a workload or an extension of the cluster itself needs a more full-featured solution. The HashiCorp Vault project is a popular solution if you need more than the built-in secret objects can provide.&lt;/p&gt;

&lt;h2 id=&#34;disaster-recovery-and-backup&#34;&gt;Disaster Recovery and Backup&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/disaster-recovery.png&#34; alt=&#34;disaster recovery&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Utilizing redundancy through the use of multiple hosts and VMs helps reduce some classes of outages, but scenarios such as a sitewide natural disaster, a bad update, getting hacked, software bugs, or human error could still result in an outage.&lt;/p&gt;

&lt;p&gt;A critical part of a production deployment is anticipating a possible future recovery.&lt;/p&gt;

&lt;p&gt;It’s also worth noting that some of your investments in designing, documenting, and automating a recovery process might also be re-usable if you need to do large-scale replicated deployments at multiple sites.&lt;/p&gt;

&lt;p&gt;Elements of a DR plan include backups (and possibly replicas), replacements, a planned process, people who can carry out the process, and recurring training. Regular test exercises and &lt;a href=&#34;https://github.com/dastergon/awesome-chaos-engineering&#34; target=&#34;_blank&#34;&gt;chaos engineering principles&lt;/a&gt; can be used to audit your readiness.&lt;/p&gt;

&lt;p&gt;Your availability requirements might demand that you retain local copies of the OS, Kubernetes components, and container images to allow recovery even during an Internet outage. The ability to deploy replacement hosts and nodes in an “air-gapped” scenario can also offer security and speed of deployment advantages.&lt;/p&gt;

&lt;p&gt;All Kubernetes objects are stored on etcd. Periodically backing up the etcd cluster data is important to recover Kubernetes clusters under disaster scenarios, such as losing all master nodes.&lt;/p&gt;

&lt;p&gt;Backing up an etcd cluster can be accomplished with etcd’s &lt;a href=&#34;https://coreos.com/etcd/docs/latest/op-guide/recovery.html&#34; target=&#34;_blank&#34;&gt;built-in&lt;/a&gt; snapshot mechanism, and copying the resulting file to storage in a different failure domain. The snapshot file contains all the Kubernetes states and critical information. In order to keep the sensitive Kubernetes data safe, encrypt the snapshot files.&lt;/p&gt;

&lt;p&gt;Using disk volume based snapshot recovery of etcd can have issues; see &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/40027&#34; target=&#34;_blank&#34;&gt;#40027&lt;/a&gt;. API-based backup solutions (e.g., &lt;a href=&#34;https://github.com/heptio/ark&#34; target=&#34;_blank&#34;&gt;Ark&lt;/a&gt;) can offer more granular recovery than a etcd snapshot, but also can be slower. You could utilize both snapshot and API-based backups, but you should do one form of etcd backup as a minimum.&lt;/p&gt;

&lt;p&gt;Be aware that some Kubernetes extensions may maintain state in independent etcd clusters, on persistent volumes, or through other mechanisms. If this state is critical, it should have a backup and recovery plan.&lt;/p&gt;

&lt;p&gt;Some critical state is held outside etcd. Certificates, container images, and other configuration- and operation-related state may be managed by your automated install/update tooling. Even if these items can be regenerated, backup or replication might allow for faster recovery after a failure. Consider backups with a recovery plan for these items:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Certificate and key pairs

&lt;ul&gt;
&lt;li&gt;CA&lt;/li&gt;
&lt;li&gt;API Server&lt;/li&gt;
&lt;li&gt;Apiserver-kubelet-client&lt;/li&gt;
&lt;li&gt;ServiceAccount signing&lt;/li&gt;
&lt;li&gt;“Front proxy”&lt;/li&gt;
&lt;li&gt;Front proxy client&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Critical DNS records&lt;/li&gt;
&lt;li&gt;IP/subnet assignments and reservations&lt;/li&gt;
&lt;li&gt;External load-balancers&lt;/li&gt;
&lt;li&gt;kubeconfig files&lt;/li&gt;
&lt;li&gt;LDAP or other authentication details&lt;/li&gt;
&lt;li&gt;Cloud provider specific account and configuration data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;considerations-for-your-production-workloads&#34;&gt;Considerations for your production workloads&lt;/h2&gt;

&lt;p&gt;Anti-affinity specifications can be used to split clustered services across backing hosts, but at this time the settings are used only when the pod is scheduled. This means that Kubernetes can restart a failed node of your clustered application, but does not have a native mechanism to rebalance after a fail back. This is a topic worthy of a separate blog, but supplemental logic might be useful to achieve optimal workload placements after host or worker node recoveries or expansions. The &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/&#34; target=&#34;_blank&#34;&gt;Pod Priority and Preemption feature&lt;/a&gt; can be used to specify a preferred triage in the event of resource shortages caused by failures or bursting workloads.&lt;/p&gt;

&lt;p&gt;For stateful services, external attached volume mounts are the standard Kubernetes recommendation for a non-clustered service (e.g., a typical SQL database). At this time Kubernetes managed snapshots of these external volumes is in the category of a &lt;a href=&#34;https://docs.google.com/presentation/d/1dgxfnroRAu0aF67s-_bmeWpkM1h2LCxe6lB1l1oS0EQ/edit#slide=id.g3ca07c98c2_0_47&#34; target=&#34;_blank&#34;&gt;roadmap feature request&lt;/a&gt;, likely to align with the Container Storage Interface (CSI) integration. Thus performing backups of such a service would involve application specific, in-pod activity that is beyond the scope of this document. While awaiting better Kubernetes support for a snapshot and backup workflow, running your database service in a VM rather than a container, and exposing it to your Kubernetes workload may be worth considering.&lt;/p&gt;

&lt;p&gt;Cluster-distributed stateful services (e.g., Cassandra) can benefit from splitting across hosts, using &lt;a href=&#34;https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/#disclaimer&#34; target=&#34;_blank&#34;&gt;local persistent volumes&lt;/a&gt; if resources allow. This would require deploying multiple Kubernetes worker nodes (could be VMs on hypervisor hosts) to preserve a quorum under single point failures.&lt;/p&gt;

&lt;h2 id=&#34;other-considerations&#34;&gt;Other considerations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34; target=&#34;_blank&#34;&gt;Logs&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34; target=&#34;_blank&#34;&gt;metrics&lt;/a&gt; (if collected and persistently retained) are valuable to diagnose outages, but given the variety of technologies available it will not be addressed in this blog. If Internet connectivity is available, it may be desirable to retain logs and metrics externally at a central location.&lt;/p&gt;

&lt;p&gt;Your production deployment should utilize an automated installation, configuration and update tool (e.g., &lt;a href=&#34;https://github.com/kubernetes-incubator/kubespray&#34; target=&#34;_blank&#34;&gt;Ansible&lt;/a&gt;, &lt;a href=&#34;https://github.com/cloudfoundry-incubator/kubo-deployment&#34; target=&#34;_blank&#34;&gt;BOSH&lt;/a&gt;, &lt;a href=&#34;https://github.com/chef-cookbooks/kubernetes&#34; target=&#34;_blank&#34;&gt;Chef&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/getting-started-guides/ubuntu/installation/&#34; target=&#34;_blank&#34;&gt;Juju&lt;/a&gt;, &lt;a href=&#34;https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/&#34; target=&#34;_blank&#34;&gt;kubeadm&lt;/a&gt;, &lt;a href=&#34;https://forge.puppet.com/puppetlabs/kubernetes&#34; target=&#34;_blank&#34;&gt;Puppet&lt;/a&gt;, etc.). A manual process will have repeatability issues, be labor intensive, error prone, and difficult to scale. &lt;a href=&#34;https://www.cncf.io/certification/software-conformance/#logos&#34; target=&#34;_blank&#34;&gt;Certified distributions&lt;/a&gt; are likely to include a facility for retaining configuration settings across updates, but if you implement your own install and config toolchain, then retention, backup and recovery of the configuration artifacts is essential. Consider keeping your deployment components and settings under a version control system such as Git.&lt;/p&gt;

&lt;h2 id=&#34;outage-recovery&#34;&gt;Outage recovery&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Runbook&#34; target=&#34;_blank&#34;&gt;Runbooks&lt;/a&gt; documenting recovery procedures should be tested and retained offline &amp;ndash; perhaps even printed. When an on-call staff member is called up at 2 am on a Friday night, it may not be a great time to improvise. Better to execute from a pre-planned, tested checklist &amp;ndash; with shared access by remote and onsite personnel.&lt;/p&gt;

&lt;h2 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-03-make-kubernetes-production-grade-anywhere/airplane.png&#34; alt=&#34;airplane&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Buying a ticket on a commercial airline is convenient and safe. But when you travel to a remote location with a short runway, that commercial Airbus A320 flight isn’t an option. This doesn’t mean that air travel is off the table. It does mean that some compromises are necessary.&lt;/p&gt;

&lt;p&gt;The adage in aviation is that on a single engine aircraft, an engine failure means you crash. With twin engines, at the very least, you get more choices of where you crash. Kubernetes on a small number of hosts is similar, and if your business case justifies it, you might scale up to a larger fleet of mixed large and small vehicles (e.g., FedEx, Amazon).&lt;/p&gt;

&lt;p&gt;Those designing a production-grade Kubernetes solution have a lot of options and decisions. A blog-length article can’t provide all the answers, and can’t know your specific priorities. We do hope this offers a checklist of things to consider, along with some useful guidance. Some options were left “on the cutting room floor” (e.g., running Kubernetes components using &lt;a href=&#34;https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-and-alpha-in-v19-self-hosting&#34; target=&#34;_blank&#34;&gt;self-hosting&lt;/a&gt; instead of static pods). These might be covered in a follow up if there is interest. Also, Kubernetes’ high enhancement rate means that if your search engine found this article after 2019, some content might be past the “sell by” date.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Dynamically Expand Volume with CSI and Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/08/02/dynamically-expand-volume-with-csi-and-kubernetes/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/08/02/dynamically-expand-volume-with-csi-and-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Orain Xiong (Co-Founder, WoquTech)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.&lt;/p&gt;

&lt;p&gt;They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.&lt;/p&gt;

&lt;p&gt;Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.&lt;/p&gt;

&lt;p&gt;With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.&lt;/p&gt;

&lt;p&gt;Currently, expanding volume is only available with those storage provisioners:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;OpenStack Cinder&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to enable this feature, we should set feature gate &lt;code&gt;ExpandPersistentVolumes&lt;/code&gt; true and turn on the &lt;code&gt;PersistentVolumeClaimResize&lt;/code&gt; admission plugin. Once &lt;code&gt;PersistentVolumeClaimResize&lt;/code&gt; has been enabled, resizing will be allowed by a Storage Class whose &lt;code&gt;allowVolumeExpansion&lt;/code&gt; field is set to true.&lt;/p&gt;

&lt;p&gt;Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.&lt;/p&gt;

&lt;p&gt;This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.&lt;/p&gt;

&lt;h2 id=&#34;container-storage-interface-csi&#34;&gt;Container Storage Interface (CSI)&lt;/h2&gt;

&lt;p&gt;To have a better understanding of what we&amp;rsquo;re going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.&lt;/p&gt;

&lt;p&gt;CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.&lt;/p&gt;

&lt;p&gt;This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png&#34; alt=&#34;csi diagram&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Three new external components are introduced to decouple Kubernetes and Storage Provider logic&lt;/li&gt;
&lt;li&gt;Blue arrows present the conventional way to call against API Server&lt;/li&gt;
&lt;li&gt;Red arrows present gRPC to call against Volume Driver&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more details, please visit: &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;https://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;extend-csi-and-kubernetes&#34;&gt;Extend CSI and Kubernetes&lt;/h2&gt;

&lt;p&gt;In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, “in-tree” volume plugin, external-provisioner and external-attacher.&lt;/p&gt;

&lt;h2 id=&#34;extend-csi-spec&#34;&gt;Extend CSI spec&lt;/h2&gt;

&lt;p&gt;The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including &lt;code&gt;RequiresFSResize&lt;/code&gt; and &lt;code&gt;ControllerResizeVolume&lt;/code&gt; and &lt;code&gt;NodeResizeVolume&lt;/code&gt;, should be introduced.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service Controller {
 rpc CreateVolume (CreateVolumeRequest)
   returns (CreateVolumeResponse) {}
……
 rpc RequiresFSResize (RequiresFSResizeRequest)
   returns (RequiresFSResizeResponse) {}
 rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
   returns (ControllerResizeVolumeResponse) {}
}

service Node {
 rpc NodeStageVolume (NodeStageVolumeRequest)
   returns (NodeStageVolumeResponse) {}
……
 rpc NodeResizeVolume (NodeResizeVolumeRequest)
   returns (NodeResizeVolumeResponse) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;extend-in-tree-volume-plugin&#34;&gt;Extend “In-Tree” Volume Plugin&lt;/h2&gt;

&lt;p&gt;In addition to the extend CSI specification, the &lt;code&gt;csiPlugin﻿&lt;/code&gt; interface within Kubernetes should also implement &lt;code&gt;expandablePlugin&lt;/code&gt;. The &lt;code&gt;csiPlugin&lt;/code&gt; interface will expand &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; representing for &lt;code&gt;ExpanderController&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;type&lt;/span&gt; ExpandableVolumePlugin &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;interface&lt;/span&gt; {
VolumePlugin
&lt;span style=&#34;color:#00a000&#34;&gt;ExpandVolumeDevice&lt;/span&gt;(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;error&lt;/span&gt;)
&lt;span style=&#34;color:#00a000&#34;&gt;RequiresFSResize&lt;/span&gt;() &lt;span style=&#34;color:#0b0;font-weight:bold&#34;&gt;bool&lt;/span&gt;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;implement-volume-driver&#34;&gt;Implement Volume Driver&lt;/h3&gt;

&lt;p&gt;Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CreateVolume&lt;/li&gt;
&lt;li&gt;DeleteVolume&lt;/li&gt;
&lt;li&gt;ControllerPublishVolume&lt;/li&gt;
&lt;li&gt;ControllerUnpublishVolume&lt;/li&gt;
&lt;li&gt;ValidateVolumeCapabilities&lt;/li&gt;
&lt;li&gt;ListVolumes&lt;/li&gt;
&lt;li&gt;GetCapacity&lt;/li&gt;
&lt;li&gt;ControllerGetCapabilities&lt;/li&gt;
&lt;li&gt;RequiresFSResize&lt;/li&gt;
&lt;li&gt;ControllerResizeVolume&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;demonstration&#34;&gt;Demonstration&lt;/h2&gt;

&lt;p&gt;Let’s demonstrate this feature with a concrete user case.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Create storage class for CSI storage provisioner&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;allowVolumeExpansion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;storage.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;StorageClass&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-qcfs&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;parameters:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;csiProvisionerSecretName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;orain-test&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;csiProvisionerSecretNamespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;provisioner:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-qcfsplugin&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;reclaimPolicy:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Delete&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;volumeBindingMode:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Immediate&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Deploy CSI Volume Driver including storage provisioner &lt;code&gt;csi-qcfsplugin&lt;/code&gt; across Kubernetes cluster&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create PVC &lt;code&gt;qcfs-pvc&lt;/code&gt; which will be dynamically provisioned by storage class &lt;code&gt;csi-qcfs&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;PersistentVolumeClaim&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;qcfs-pvc&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;....&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;accessModes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ReadWriteOnce&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;resources:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;requests:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;storage:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;300Gi&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;storageClassName:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;csi-qcfs&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Create MySQL 5.7 instance to use PVC &lt;code&gt;qcfs-pvc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:

&lt;ul&gt;
&lt;li&gt;Batch insert to make MySQL consuming more file system capacity&lt;/li&gt;
&lt;li&gt;Surge query request&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dynamically expand volume capacity through edit pvc &lt;code&gt;qcfs-pvc&lt;/code&gt; configuration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png&#34; alt=&#34;prometheus grafana&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: KubeVirt: Extending Kubernetes with CRDs for Virtualized Workloads</title>
      <link>https://docstest.github.io/blog/2018/07/27/kubevirt-extending-kubernetes-with-crds-for-virtualized-workloads/</link>
      <pubDate>Fri, 27 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/27/kubevirt-extending-kubernetes-with-crds-for-virtualized-workloads/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: David Vossel (Red Hat)&lt;/p&gt;

&lt;h2 id=&#34;what-is-kubevirt&#34;&gt;What is KubeVirt?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubevirt/kubevirt&#34; target=&#34;_blank&#34;&gt;KubeVirt&lt;/a&gt; is a Kubernetes addon that provides users the ability to schedule traditional virtual machine workloads side by side with container workloads. Through the use of &lt;a href=&#34;https://Kubernetes.io/docs/concepts/extend-Kubernetes/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;Custom Resource Definitions&lt;/a&gt; (CRDs) and other Kubernetes features, KubeVirt seamlessly extends existing Kubernetes clusters to provide a set of virtualization APIs that can be used to manage virtual machines.&lt;/p&gt;

&lt;h2 id=&#34;why-use-crds-over-an-aggregated-api-server&#34;&gt;Why Use CRDs Over an Aggregated API Server?&lt;/h2&gt;

&lt;p&gt;Back in the middle of 2017, those of us working on KubeVirt were at a crossroads. We had to make a decision whether or not to extend Kubernetes using an aggregated API server or to make use of the new Custom Resource Definitions (CRDs) feature.&lt;/p&gt;

&lt;p&gt;At the time, CRDs lacked much of the functionality we needed to deliver our feature set. The ability to create our own aggregated API server gave us all the flexibility we needed, but it had one major flaw. &lt;strong&gt;An aggregated API server significantly increased the complexity involved with installing and operating KubeVirt.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The crux of the issue for us was that aggregated API servers required access to etcd for object persistence. This meant that cluster admins would have to either accept that KubeVirt needs a separate etcd deployment which increases complexity, or provide KubeVirt with shared access to the Kubernetes etcd store which introduces risk.&lt;/p&gt;

&lt;p&gt;We weren’t okay with this tradeoff. Our goal wasn’t to just extend Kubernetes to run virtualization workloads, it was to do it in the most seamless and effortless way possible. We felt that the added complexity involved with an aggregated API server sacrificed the part of the user experience involved with installing and operating KubeVirt.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ultimately we chose to go with CRDs and trust that the Kubernetes ecosystem would grow with us to meet the needs of our use case.&lt;/strong&gt; Our bets were well placed. At this point there are either solutions in place or solutions under discussion that solve every feature gap we encountered back in 2017 when were evaluating CRDs vs an aggregated API server.&lt;/p&gt;

&lt;h2 id=&#34;building-layered-kubernetes-like-apis-with-crds&#34;&gt;Building Layered “Kubernetes like” APIs with CRDs&lt;/h2&gt;

&lt;p&gt;We designed KubeVirt’s API to follow the same patterns users are already familiar with in the Kubernetes core API.&lt;/p&gt;

&lt;p&gt;For example, in Kubernetes the lowest level unit that users create to perform work is a Pod. Yes, Pods do have multiple containers but logically the Pod is the unit at the bottom of the stack. A Pod represents a mortal workload. The Pod gets scheduled, eventually the Pod’s workload terminates, and that’s the end of the Pod’s lifecycle.&lt;/p&gt;

&lt;p&gt;Workload controllers such as the ReplicaSet and StatefulSet are layered on top of the Pod abstraction to help manage scale out and stateful applications. From there we have an even higher level controller called a Deployment which is layered on top of ReplicaSets help manage things like rolling updates.&lt;/p&gt;

&lt;p&gt;In KubeVirt, this concept of layering controllers is at the very center of our design. The KubeVirt VirtualMachineInstance (VMI) object is the lowest level unit at the very bottom of the KubeVirt stack. Similar in concept to a Pod, a VMI represents a single mortal virtualized workload that executes once until completion (powered off).&lt;/p&gt;

&lt;p&gt;Layered on top of VMIs we have a workload controller called a VirtualMachine (VM). The VM controller is where we really begin to see the differences between how users manage virtualized workloads vs containerized workloads. Within the context of existing Kubernetes functionality, the best way to describe the VM controller’s behavior is to compare it to a StatefulSet of size one. This is because the VM controller represents a single stateful (immortal) virtual machine capable of persisting state across both node failures and multiple restarts of its underlying VMI. This object behaves in the way that is familiar to users who have managed virtual machines in AWS, GCE, OpenStack or any other similar IaaS cloud platform. The user can shutdown a VM, then choose to start that exact same VM up again at a later time.&lt;/p&gt;

&lt;p&gt;In addition to VMs, we also have a VirtualMachineInstanceReplicaSet (VMIRS) workload controller which manages scale out of identical VMI objects. This controller behaves nearly identically to the Kubernetes ReplicSet controller. The primary difference being that the VMIRS manages VMI objects and the ReplicaSet manages Pods. Wouldn’t it be nice if we could come up with a way to &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/65622&#34; target=&#34;_blank&#34;&gt;use the Kubernetes ReplicaSet controller to scale out CRDs?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Each one of these KubeVirt objects (VMI, VM, VMIRS) are registered with Kubernetes as a CRD when the KubeVirt install manifest is posted to the cluster. By registering our APIs as CRDs with Kubernetes, all the tooling involved with managing Kubernetes clusters (like kubectl) have access to the KubeVirt APIs just as if they are native Kubernetes objects.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-webhooks-for-api-validation&#34;&gt;Dynamic Webhooks for API Validation&lt;/h2&gt;

&lt;p&gt;One of the responsibilities of the Kubernetes API server is to intercept and validate requests prior to allowing objects to be persisted into etcd. For example, if someone tries to create a Pod using a malformed Pod specification, the Kubernetes API server immediately catches the error and rejects the POST request. This all occurs before the object is persistent into etcd preventing the malformed Pod specification from making its way into the cluster.&lt;/p&gt;

&lt;p&gt;This validation occurs during a process called admission control. Until recently, it was not possible to extend the default Kubernetes admission controllers without altering code and compiling/deploying an entirely new Kubernetes API server. This meant that if we wanted to perform admission control on KubeVirt’s CRD objects while they are posted to the cluster, we’d have to build our own version of the Kubernetes API server and convince our users to use that instead. That was not a viable solution for us.&lt;/p&gt;

&lt;p&gt;Using the new &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&#34; target=&#34;_blank&#34;&gt;Dynamic Admission Control&lt;/a&gt; feature that first landed in Kubernetes 1.9, we now have a path for performing custom validation on KubeVirt API through the use of a &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#external-admission-webhooks&#34; target=&#34;_blank&#34;&gt;ValidatingAdmissionWebhook&lt;/a&gt;. This feature allows KubeVirt to dynamically register an HTTPS webhook with Kubernetes at KubeVirt install time. After registering the custom webhook, all requests related to KubeVirt API objects are forwarded from the Kubernetes API server to our HTTPS endpoint for validation. If our endpoint rejects a request for any reason, the object will not be persisted into etcd and the client receives our response outlining the reason for the rejection.&lt;/p&gt;

&lt;p&gt;For example, if someone posts a malformed VirtualMachine object, they’ll receive an error indicating what the problem is.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f my-vm.yaml 
Error from server: error when creating &amp;quot;my-vm.yaml&amp;quot;: admission webhook &amp;quot;virtualmachine-validator.kubevirt.io&amp;quot; denied the request: spec.template.spec.domain.devices.disks[0].volumeName &#39;registryvolume&#39; not found.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the example output above, that error response is coming directly from KubeVirt’s admission control webhook.&lt;/p&gt;

&lt;h2 id=&#34;crd-openapiv3-validation&#34;&gt;CRD OpenAPIv3 Validation&lt;/h2&gt;

&lt;p&gt;In addition to the validating webhook, KubeVirt also uses the ability to provide an &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-API/extend-api-custom-resource-definitions/#advanced-topics&#34; target=&#34;_blank&#34;&gt;OpenAPIv3 validation schema&lt;/a&gt; when registering a CRD with the cluster. While the OpenAPIv3 schema does not let us express some of the more advanced validation checks that the validation webhook provides, it does offer the ability to enforce simple validation checks involving things like required fields, max/min value lengths, and verifying that values are formatted in a way that matches a regular expression string.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-webhooks-for-podpreset-like-behavior&#34;&gt;Dynamic Webhooks for “PodPreset Like” Behavior&lt;/h2&gt;

&lt;p&gt;The Kubernetes Dynamic Admission Control feature is not only limited to validation logic, it also provides the ability for applications like KubeVirt to both intercept and mutate requests as they enter the cluster. This is achieved through the use of a &lt;strong&gt;MutatingAdmissionWebhook&lt;/strong&gt; object. In KubeVirt, we are looking to use a mutating webhook to support our VirtualMachinePreset (VMPreset) feature.&lt;/p&gt;

&lt;p&gt;A VMPreset acts in a similar way to a PodPreset. Just like a PodPreset allows users to define values that should automatically be injected into pods at creation time, a VMPreset allows users to define values that should be injected into VMs at creation time. Through the use of a mutating webhook, KubeVirt can intercept a request to create a VM, apply VMPresets to the VM spec, and then validate that the resulting VM object. This all occurs before the VM object is persisted into etcd which allows KubeVirt to immediately notify the user of any conflicts at the time the request is made.&lt;/p&gt;

&lt;h2 id=&#34;subresources-for-crds&#34;&gt;Subresources for CRDs&lt;/h2&gt;

&lt;p&gt;When comparing the use of CRDs to an aggregated API server, one of the features CRDs lack is the ability to support subresources. Subresources are used to provide additional resource functionality. For example, the &lt;code&gt;pod/logs&lt;/code&gt; and &lt;code&gt;pod/exec&lt;/code&gt; subresource endpoints are used behind the scenes to provide the &lt;code&gt;kubectl logs&lt;/code&gt; and &lt;code&gt;kubectl exec&lt;/code&gt; command functionality.&lt;/p&gt;

&lt;p&gt;Just like Kubernetes uses the &lt;code&gt;pod/exec&lt;/code&gt; subresource to provide access to a pod’s environment, in KubeVirt we want subresources to provide serial-console, VNC, and SPICE access to a virtual machine. By adding virtual machine guest access through subresources, we can leverage RBAC to provide access control for these features.&lt;/p&gt;

&lt;p&gt;So, given that the KubeVirt team decided to use CRD’s instead of an aggregated API server for custom resource support, how can we have subresources for CRDs when the CRD feature expiclity does not support subresources?&lt;/p&gt;

&lt;p&gt;We created a workaround for this limitation by implementing a stateless aggregated API server that exists only to serve subresource requests. With no state, we don’t have to worry about any of the issues we identified earlier with regards to access to etcd. This means the KubeVirt API is actually supported through a combination of both CRDs for resources and an aggregated API server for stateless subresources.&lt;/p&gt;

&lt;p&gt;This isn’t a perfect solution for us. Both aggregated API servers and CRDs require us to register an API GroupName with Kubernetes. This API GroupName field essentially namespaces the API’s REST path in a way that prevents API naming conflicts between other third party applications. Because CRDs and aggregated API servers can’t share the same GroupName, we have to register two separate GroupNames. One is used by our CRDs and the other is used by the aggregated API server for subresource requests.&lt;/p&gt;

&lt;p&gt;Having two GroupNames in our API is slightly inconvenient because it means the REST path for the endpoints that serve the KubeVirt subresource requests have a slightly different base path than the resources.&lt;/p&gt;

&lt;p&gt;For example, the endpoint to create a VMI object is as follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;/apis/kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;However, the subresource endpoint to access graphical VNC looks like this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;/apis/subresources.kubevirt.io/v1alpha2/namespaces/my-namespace/virtualmachineinstances/my-vm/vnc&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Notice that the first request uses &lt;strong&gt;kubevirt.io&lt;/strong&gt; and the second request uses &lt;strong&gt;subresource.kubevirt.io&lt;/strong&gt;. We don’t like that, but that’s how we’ve managed to combine CRDs with a stateless aggregated API server for subresources.&lt;/p&gt;

&lt;p&gt;One thing worth noting is that in Kubernetes 1.10 a very basic form of CRD subresource support was added in the form of the &lt;code&gt;/status&lt;/code&gt; and &lt;code&gt;/scale&lt;/code&gt; subresources. This support does not help us deliver the virtualization features we want subresources for. However, there have been discussions about exposing custom CRD subresources as webhooks in a future Kubernetes version. If this functionality lands, we will gladly transition away from our stateless aggregated API server workaround to use a subresource webhook feature.&lt;/p&gt;

&lt;h2 id=&#34;crd-finalizers&#34;&gt;CRD Finalizers&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&#34;https://kubernetes.io/docs/tasks/access-kubernetes-API/extend-api-custom-resource-definitions/#advanced-topics&#34; target=&#34;_blank&#34;&gt;CRD finalizer&lt;/a&gt; is a feature that lets us provide a pre-delete hook in order to perform actions before allowing a CRD object to be removed from persistent storage. In KubeVirt, we use finalizers to guarantee a virtual machine has completely terminated before we allow the corresponding VMI object to be removed from etcd.&lt;/p&gt;

&lt;h2 id=&#34;api-versioning-for-crds&#34;&gt;API Versioning for CRDs&lt;/h2&gt;

&lt;p&gt;The Kubernetes core APIs have the ability to support multiple versions for a single object type and perform conversions between those versions. This gives the Kubernetes core APIs a path for advancing the &lt;code&gt;v1alpha1&lt;/code&gt; version of an object to a &lt;code&gt;v1beta1&lt;/code&gt; version and so forth.&lt;/p&gt;

&lt;p&gt;Prior to Kubernetes 1.11, CRDs did not have support for multiple versions. This meant when we wanted to progress a CRD from &lt;code&gt;kubevirt.io/v1alpha1&lt;/code&gt; to &lt;code&gt;kubevirt.io/v1beta1&lt;/code&gt;, the only path available to was to backup our CRD objects, delete the registered CRD from Kubernetes, register a new CRD with the updated version, convert the backed up CRD objects to the new version, and finally post the migrated CRD objects back to the cluster.&lt;/p&gt;

&lt;p&gt;That strategy was not exactly a viable option for us.&lt;/p&gt;

&lt;p&gt;Fortunately thanks to some recent &lt;a href=&#34;https://github.com/kubernetes/features/issues/544&#34; target=&#34;_blank&#34;&gt;work to rectify this issue in Kubernetes&lt;/a&gt;, the latest Kubernetes v1.11 now supports &lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/63830&#34; target=&#34;_blank&#34;&gt;CRDs with multiple versions&lt;/a&gt;. Note however that this initial multi version support is limited. While a CRD can now have multiple versions, the feature does not currently contain a path for performing conversions between versions. In KubeVirt, the lack of conversion makes it difficult us to evolve our API as we progress versions. Luckily, support for conversions between versions is underway and we look forward to taking advantage of that feature once it lands in a future Kubernetes release.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Feature Highlight: CPU Manager</title>
      <link>https://docstest.github.io/blog/2018/07/24/feature-highlight-cpu-manager/</link>
      <pubDate>Tue, 24 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/24/feature-highlight-cpu-manager/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Balaji Subramaniam (&lt;a href=&#34;mailto:balaji.subramaniam@intel.com&#34; target=&#34;_blank&#34;&gt;Intel&lt;/a&gt;), Connor Doyle (&lt;a href=&#34;mailto:connor.p.doyle@intel.com&#34; target=&#34;_blank&#34;&gt;Intel&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;This blog post describes the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/&#34; target=&#34;_blank&#34;&gt;CPU Manager&lt;/a&gt;, a beta feature in &lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt;. The CPU manager feature enables better placement of workloads in the &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&#34; target=&#34;_blank&#34;&gt;Kubelet&lt;/a&gt;, the Kubernetes node agent, by allocating exclusive CPUs to certain pod containers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-24-cpu-manager/cpu-manager.png&#34; alt=&#34;cpu manager&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;sounds-good-but-does-the-cpu-manager-help-me&#34;&gt;Sounds Good! But Does the CPU Manager Help Me?&lt;/h2&gt;

&lt;p&gt;It depends on your workload. A single compute node in a Kubernetes cluster can run many &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod/&#34; target=&#34;_blank&#34;&gt;pods&lt;/a&gt; and some of these pods could be running CPU-intensive workloads. In such a scenario, the pods might contend for the CPU resources available in that compute node. When this contention intensifies, the workload can move to different CPUs depending on whether the pod is throttled and the availability of CPUs at scheduling time. There might also be cases where the workload could be sensitive to context switches. In all the above scenarios, the performance of the workload might be affected.&lt;/p&gt;

&lt;p&gt;If your workload is sensitive to such scenarios, then CPU Manager can be enabled to provide better performance isolation by allocating exclusive CPUs for your workload.&lt;/p&gt;

&lt;p&gt;CPU manager might help workloads with the following characteristics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sensitive to CPU throttling effects.&lt;/li&gt;
&lt;li&gt;Sensitive to context switches.&lt;/li&gt;
&lt;li&gt;Sensitive to processor cache misses.&lt;/li&gt;
&lt;li&gt;Benefits from sharing a processor resources (e.g., data and instruction caches).&lt;/li&gt;
&lt;li&gt;Sensitive to cross-socket memory traffic.&lt;/li&gt;
&lt;li&gt;Sensitive or requires hyperthreads from the same physical CPU core.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ok-how-do-i-use-it&#34;&gt;Ok! How Do I use it?&lt;/h2&gt;

&lt;p&gt;Using the CPU manager is simple. First, &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies&#34; target=&#34;_blank&#34;&gt;enable CPU manager with the Static policy&lt;/a&gt; in the Kubelet running on the compute nodes of your cluster. Then configure your pod to be in the &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed&#34; target=&#34;_blank&#34;&gt;Guaranteed Quality of Service (QoS) class&lt;/a&gt;. Request whole numbers of CPU cores (e.g., &lt;code&gt;1000m&lt;/code&gt;, &lt;code&gt;4000m&lt;/code&gt;) for containers that need exclusive cores. Create your pod in the same way as before (e.g., &lt;code&gt;kubectl create -f pod.yaml&lt;/code&gt;). And &lt;em&gt;voilà&lt;/em&gt;, the CPU manager will assign exclusive CPUs to each of container in the pod according to their CPU requests.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: exclusive-2
spec:
  containers:
  - image: quay.io/connordoyle/cpuset-visualizer
    name: exclusive-2
    resources:
      # Pod is in the Guaranteed QoS class because requests == limits
      requests:
        # CPU request is an integer
        cpu: 2
        memory: &amp;quot;256M&amp;quot;
      limits:
        cpu: 2
        memory: &amp;quot;256M&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Pod specification requesting two exclusive CPUs.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;hmm-how-does-the-cpu-manager-work&#34;&gt;Hmm … How Does the CPU Manager Work?&lt;/h2&gt;

&lt;p&gt;For Kubernetes, and the purposes of this blog post, we will discuss three kinds of CPU resource controls available in most Linux distributions. The first two are CFS shares (what&amp;rsquo;s my weighted fair share of CPU time on this system) and CFS quota (what&amp;rsquo;s my hard cap of CPU time over a period). The CPU manager uses a third control called CPU affinity (on what logical CPUs am I allowed to execute).&lt;/p&gt;

&lt;p&gt;By default, all the pods and the containers running on a compute node of your Kubernetes cluster can execute on any available cores in the system. The total amount of allocatable shares and quota are limited by the CPU resources explicitly &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/&#34; target=&#34;_blank&#34;&gt;reserved for kubernetes and system daemons&lt;/a&gt;. However, limits on the CPU time being used can be specified using &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/#specify-a-cpu-request-and-a-cpu-limit&#34; target=&#34;_blank&#34;&gt;CPU limits in the pod spec&lt;/a&gt;. Kubernetes uses &lt;a href=&#34;https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt&#34; target=&#34;_blank&#34;&gt;CFS quota&lt;/a&gt; to enforce CPU limits on pod containers.&lt;/p&gt;

&lt;p&gt;When CPU manager is enabled with the &amp;ldquo;static&amp;rdquo; policy, it manages a shared pool of CPUs. Initially this shared pool contains all the CPUs in the compute node. When a container with integer CPU request in a Guaranteed pod is created by the Kubelet, CPUs for that container are removed from the shared pool and assigned exclusively for the lifetime of the container. Other containers are migrated off these exclusively allocated CPUs.&lt;/p&gt;

&lt;p&gt;All non-exclusive-CPU containers (Burstable, BestEffort and Guaranteed with non-integer CPU) run on the CPUs remaining in the shared pool. When a container with exclusive CPUs terminates, its CPUs are added back to the shared CPU pool.&lt;/p&gt;

&lt;h2 id=&#34;more-details-please&#34;&gt;More Details Please &amp;hellip;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-24-cpu-manager/cpu-manager-anatomy.png&#34; alt=&#34;cpu manager&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows the anatomy of the CPU manager. The CPU Manager uses the Container Runtime Interface&amp;rsquo;s &lt;code&gt;UpdateContainerResources&lt;/code&gt; method to modify the CPUs on which containers can run. The Manager periodically reconciles the current State of the CPU resources of each running container with &lt;code&gt;cgroupfs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The CPU Manager uses &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/cpumanager/policy.go#L25&#34; target=&#34;_blank&#34;&gt;Policies&lt;/a&gt; to decide the allocation of CPUs. There are two policies implemented: None and Static. By default, the CPU manager is enabled with the None policy from Kubernetes version 1.10.&lt;/p&gt;

&lt;p&gt;The Static policy allocates exclusive CPUs to pod containers in the Guaranteed QoS class which request integer CPUs. On a best-effort basis, the Static policy tries to allocate CPUs topologically in the following order:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Allocate all the CPUs in the same processor socket if available and the container requests at least an entire socket worth of CPUs.&lt;/li&gt;
&lt;li&gt;Allocate all the logical CPUs (hyperthreads) from the same physical CPU core if available and the container requests an entire core worth of CPUs.&lt;/li&gt;
&lt;li&gt;Allocate any available logical CPU, preferring to acquire CPUs from the same socket.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;how-is-performance-isolation-improved-by-cpu-manager&#34;&gt;How is Performance Isolation Improved by CPU Manager?&lt;/h2&gt;

&lt;p&gt;With CPU manager static policy enabled, the workloads might perform better due to one of the following reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Exclusive CPUs can be allocated for the workload container but not the other containers. These containers do not share the CPU resources. As a result, we expect better performance due to isolation when an aggressor or a co-located workload is involved.&lt;/li&gt;
&lt;li&gt;There is a reduction in interference between the resources used by the workload since we can partition the CPUs among workloads. These resources might also include the cache hierarchies and memory bandwidth and not just the CPUs. This helps improve the performance of workloads in general.&lt;/li&gt;
&lt;li&gt;CPU Manager allocates CPUs in a topological order on a best-effort basis. If a whole socket is free, the CPU Manager will exclusively allocate the CPUs from the free socket to the workload. This boosts the performance of the workload by avoiding any cross-socket traffic.&lt;/li&gt;
&lt;li&gt;Containers in Guaranteed QoS pods are subject to CFS quota. Very bursty workloads may get scheduled, burn through their quota before the end of the period, and get throttled. During this time, there may or may not be meaningful work to do with those CPUs. Because of how the resource math lines up between CPU quota and number of exclusive CPUs allocated by the static policy, these containers are not subject to CFS throttling (quota is equal to the maximum possible cpu-time over the quota period).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;ok-ok-do-you-have-any-results&#34;&gt;Ok! Ok! Do You Have Any Results?&lt;/h2&gt;

&lt;p&gt;Glad you asked! To understand the performance improvement and isolation provided by enabling the CPU Manager feature in the Kubelet, we ran experiments on a dual-socket compute node (Intel Xeon CPU E5-2680 v3) with hyperthreading enabled. The node consists of 48 logical CPUs (24 physical cores each with 2-way hyperthreading). Here we demonstrate the performance benefits and isolation provided by the CPU Manager feature using benchmarks and real-world workloads for three different scenarios.&lt;/p&gt;

&lt;h3 id=&#34;how-do-i-interpret-the-plots&#34;&gt;How Do I Interpret the Plots?&lt;/h3&gt;

&lt;p&gt;For each scenario, we show box plots that illustrates the normalized execution time and its variability of running a benchmark or real-world workload with and without CPU Manager enabled. The execution time of the runs are normalized to the best-performing run (1.00 on y-axis represents the best performing run and lower is better). The height of the box plot shows the variation in performance. For example if the box plot is a line, then there is no variation in performance across runs. In the box, middle line is the median, upper line is 75th percentile and lower line is 25th percentile. The height of the box (i.e., difference between 75th and 25th percentile) is defined as the interquartile range (IQR). Whiskers shows data outside that range and the points show outliers. The outliers are defined as any data 1.5x IQR below or above the lower or upper quartile respectively. Every experiment is run ten times.&lt;/p&gt;

&lt;h3 id=&#34;protection-from-aggressor-workloads&#34;&gt;Protection from Aggressor Workloads&lt;/h3&gt;

&lt;p&gt;We ran six benchmarks from the &lt;a href=&#34;http://parsec.cs.princeton.edu/&#34; target=&#34;_blank&#34;&gt;PARSEC benchmark suite&lt;/a&gt; (the victim workloads) co-located with a CPU stress container (the aggressor workload) with and without the CPU Manager feature enabled. The CPU stress container is run &lt;a href=&#34;https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c&#34; target=&#34;_blank&#34;&gt;as a pod&lt;/a&gt; in the Burstable QoS class requesting 23 CPUs with &lt;code&gt;--cpus 48&lt;/code&gt; flag. &lt;a href=&#34;https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902&#34; target=&#34;_blank&#34;&gt;The benchmarks are run as pods&lt;/a&gt; in the Guaranteed QoS class requesting a full socket worth of CPUs (24 CPUs on this system). The figure below plots the normalized execution time of running a benchmark pod co-located with the stress pod, with and without the CPU Manager static policy enabled. We see improved performance and reduced performance variability when static policy is enabled for all test cases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-24-cpu-manager/execution-time.png&#34; alt=&#34;execution time&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;performance-isolation-for-co-located-workloads&#34;&gt;Performance Isolation for Co-located Workloads&lt;/h3&gt;

&lt;p&gt;In this section, we demonstrate how CPU manager can be beneficial to multiple workloads in a co-located workload scenario. In the box plots below we show the performance of two benchmarks (Blackscholes and Canneal) from the PARSEC benchmark suite run in the Guaranteed (Gu) and Burstable (Bu) QoS classes co-located with each other, with and without the CPU manager static policy enabled.&lt;/p&gt;

&lt;p&gt;Starting from the top left and proceeding clockwise, we show the performance of Blackscholes in the Bu QoS class (top left), Canneal in the Bu QoS class (top right), Canneal in Gu QoS class (bottom right) and Blackscholes in the Gu QoS class (bottom left, respectively. In each case, they are co-located with Canneal in the Gu QoS class (top left), Blackscholes in the Gu QoS class (top right), Blackscholes in the Bu QoS class (bottom right) and Canneal in the Bu QoS class (bottom left) going clockwise from top left, respectively. For example, Bu-blackscholes-Gu-canneal plot (top left) is showing the performance of Blackscholes running in the Bu QoS class when co-located with Canneal running in the Gu QoS class. In each case, the pod in Gu QoS class requests cores worth a whole socket (i.e., 24 CPUs) and the pod in Bu QoS class request 23 CPUs.&lt;/p&gt;

&lt;p&gt;There is better performance and less performance variation for both the co-located workloads in all the tests. For example, consider the case of Bu-blackscholes-Gu-canneal (top left) and Gu-canneal-Bu-blackscholes (bottom right). They show the performance of Blackscholes and Canneal run simultaneously with and without the CPU manager enabled. In this particular case, Canneal gets exclusive cores due to CPU manager since it is in the Gu QoS class and requesting integer number of CPU cores. But Blackscholes also gets exclusive set of CPUs as it is the only workload in the shared pool. As a result, both Blackscholes and Canneal get some performance isolation benefits due to the CPU manager.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-24-cpu-manager/performance-comparison.png&#34; alt=&#34;performance comparison&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;performance-isolation-for-stand-alone-workloads&#34;&gt;Performance Isolation for Stand-Alone Workloads&lt;/h3&gt;

&lt;p&gt;This section shows the performance improvement and isolation provided by the CPU manager for stand-alone real-world workloads. We use two workloads from the &lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official&#34; target=&#34;_blank&#34;&gt;TensorFlow official models&lt;/a&gt;: &lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official/wide_deep&#34; target=&#34;_blank&#34;&gt;wide and deep&lt;/a&gt; and &lt;a href=&#34;https://github.com/tensorflow/models/tree/master/official/resnet&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt;. We use the census and CIFAR10 dataset for the wide and deep and ResNet models respectively. In each case the &lt;a href=&#34;https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58&#34; target=&#34;_blank&#34;&gt;pods&lt;/a&gt; (&lt;a href=&#34;https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c&#34; target=&#34;_blank&#34;&gt;wide and deep&lt;/a&gt;, &lt;a href=&#34;https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt; request 24 CPUs which corresponds to a whole socket worth of cores. As shown in the plots, CPU manager enables better performance isolation in both cases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-24-cpu-manager/performance-comparison-2.png&#34; alt=&#34;performance comparison&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;Users might want to get CPUs allocated on the socket near to the bus which connects to an external device, such as an accelerator or high-performance network card, in order to avoid cross-socket traffic. This type of alignment is not yet supported by CPU manager.
Since the CPU manager provides a best-effort allocation of CPUs belonging to a socket and physical core, it is susceptible to corner cases and might lead to fragmentation.
The CPU manager does not take the isolcpus Linux kernel boot parameter into account, although this is reportedly common practice for some low-jitter use cases.&lt;/p&gt;

&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank the members of the community who have contributed to this feature or given feedback including members of WG-Resource-Management and SIG-Node.
cmx.io (for the fun drawing tool).&lt;/p&gt;

&lt;h4 id=&#34;notices-and-disclaimers&#34;&gt;Notices and Disclaimers&lt;/h4&gt;

&lt;p&gt;Software and workloads used in performance tests may have been optimized for performance only on Intel microprocessors. Performance tests, such as SYSmark and MobileMark, are measured using specific computer systems, components, software, operations and functions. Any change to any of those factors may cause the results to vary. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases, including the performance of that product when combined with other products. For more information go to www.intel.com/benchmarks.&lt;/p&gt;

&lt;p&gt;Intel technologies’ features and benefits depend on system configuration and may require enabled hardware, software or service activation. Performance varies depending on system configuration. No computer system can be absolutely secure. Check with your system manufacturer or retailer or learn more at intel.com.&lt;/p&gt;

&lt;p&gt;Workload Configuration:
&lt;a href=&#34;https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/balajismaniam/fac7923f6ee44f1f36969c29354e3902&lt;/a&gt;
&lt;a href=&#34;https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/balajismaniam/7c2d57b2f526a56bb79cf870c122a34c&lt;/a&gt;
&lt;a href=&#34;https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/balajismaniam/941db0d0ec14e2bc93b7dfe04d1f6c58&lt;/a&gt;
&lt;a href=&#34;https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/balajismaniam/a1919010fe9081ca37a6e1e7b01f02e3&lt;/a&gt;
&lt;a href=&#34;https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/balajismaniam/9953b54dd240ecf085b35ab1bc283f3c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;System Configuration:
CPU
 Architecture:          x86_64
 CPU op-mode(s):        32-bit, 64-bit
 Byte Order:            Little Endian
 CPU(s):                48
 On-line CPU(s) list:   0-47
 Thread(s) per core:    2
 Core(s) per socket:    12
 Socket(s):             2
 NUMA node(s):          2
 Vendor ID:             GenuineIntel
 Model name:            Intel&amp;reg; Xeon&amp;reg; CPU E5-2680 v3
Memory
 256 GB
OS/Kernel
 Linux 3.10.0-693.21.1.el7.x86_64&lt;/p&gt;

&lt;p&gt;Intel, the Intel logo, Xeon are trademarks of Intel Corporation or its subsidiaries in the U.S. and/or other countries.&lt;br /&gt;
*Other names and brands may be claimed as the property of others.
© Intel Corporation.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: The History of Kubernetes &amp; the Community Behind It</title>
      <link>https://docstest.github.io/blog/2018/07/20/the-history-of-kubernetes-the-community-behind-it/</link>
      <pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/20/the-history-of-kubernetes-the-community-behind-it/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Brendan Burns (Distinguished Engineer, Microsoft)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-20-history-kubernetes-community.png&#34; alt=&#34;oscon award&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It is remarkable to me to return to Portland and OSCON to stand on stage with members of the Kubernetes community and accept this award for Most Impactful Open Source Project. It was scarcely three years ago, that on this very same stage we declared Kubernetes 1.0 and the project was added to the newly formed Cloud Native Computing Foundation.&lt;/p&gt;

&lt;p&gt;To think about how far we have come in that short period of time and to see the ways in which this project has shaped the cloud computing landscape is nothing short of amazing. The success is a testament to the power and contributions of this amazing open source community. And the daily passion and quality contributions of our endlessly engaged, world-wide community is nothing short of humbling.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Congratulations &lt;a href=&#34;https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw&#34;&gt;@kubernetesio&lt;/a&gt; for winning the &amp;quot;most impact&amp;quot; award at &lt;a href=&#34;https://twitter.com/hashtag/OSCON?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#OSCON&lt;/a&gt; I&amp;#39;m so proud to be a part of this amazing community! &lt;a href=&#34;https://twitter.com/CloudNativeFdn?ref_src=twsrc%5Etfw&#34;&gt;@CloudNativeFdn&lt;/a&gt; &lt;a href=&#34;https://t.co/5sRUYyefAK&#34;&gt;pic.twitter.com/5sRUYyefAK&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jaice Singer DuMars (@jaydumars) &lt;a href=&#34;https://twitter.com/jaydumars/status/1019993233487613952?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2018&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;👏 congrats &lt;a href=&#34;https://twitter.com/kubernetesio?ref_src=twsrc%5Etfw&#34;&gt;@kubernetesio&lt;/a&gt; community on winning the &lt;a href=&#34;https://twitter.com/hashtag/oscon?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#oscon&lt;/a&gt; Most Impact Award, we are proud of you! &lt;a href=&#34;https://t.co/5ezDphi6J6&#34;&gt;pic.twitter.com/5ezDphi6J6&lt;/a&gt;&lt;/p&gt;&amp;mdash; CNCF (@CloudNativeFdn) &lt;a href=&#34;https://twitter.com/CloudNativeFdn/status/1019996928296095744?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2018&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;At a meetup in Portland this week, I had a chance to tell the story of Kubernetes’ past, its present and some thoughts about its future, so I thought I would write down some pieces of what I said for those of you who couldn’t be there in person.&lt;/p&gt;

&lt;p&gt;It all began in the fall of 2013, with three of us: Craig McLuckie, Joe Beda and I were working on public cloud infrastructure. If you cast your mind back to the world of cloud in 2013, it was a vastly different place than it is today. Imperative bash scripts were only just starting to give way to declarative configuration of IaaS with systems. Netflix was popularizing the idea of immutable infrastructure but doing it with heavy-weight full VM images. The notion of orchestration, and certainly container orchestration existed in a few internet scale companies, but not in cloud and certainly not in the enterprise.&lt;/p&gt;

&lt;p&gt;Docker changed all of that. By popularizing a lightweight container runtime and providing a simple way to package, distributed and deploy applications onto a machine, the Docker tooling and experience popularized a brand-new cloud native approach to application packaging and maintenance. Were it not for Docker’s shifting of the cloud developer’s perspective, Kubernetes simply would not exist.&lt;/p&gt;

&lt;p&gt;I think that it was Joe who first suggested that we look at Docker in the summer of 2013, when Craig, Joe and I were all thinking about how we could bring a cloud native application experience to a broader audience. And for all three of us, the implications of this new tool were immediately obvious. We knew it was a critical component in the development of cloud native infrastructure.&lt;/p&gt;

&lt;p&gt;But as we thought about it, it was equally obvious that Docker, with its focus on a single machine, was not the complete solution. While Docker was great at building and packaging individual containers and running them on individual machines, there was a clear need for an orchestrator that could deploy and manage large numbers of containers across a fleet of machines.&lt;/p&gt;

&lt;p&gt;As we thought about it some more, it became increasingly obvious to Joe, Craig and I, that not only was such an orchestrator necessary, it was also inevitable, and it was equally inevitable that this orchestrator would be open source. This realization crystallized for us in the late fall of 2013, and thus began the rapid development of first a prototype, and then the system that would eventually become known as Kubernetes. As 2013 turned into 2014 we were lucky to be joined by some incredibly talented developers including Ville Aikas, Tim Hockin, Dawn Chen, Brian Grant and Daniel Smith.&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Happy to see k8s team members winning the “most impact” award. &lt;a href=&#34;https://twitter.com/hashtag/oscon?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#oscon&lt;/a&gt; &lt;a href=&#34;https://t.co/D6mSIiDvsU&#34;&gt;pic.twitter.com/D6mSIiDvsU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bridget Kromhout (@bridgetkromhout) &lt;a href=&#34;https://twitter.com/bridgetkromhout/status/1019992441825341440?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2018&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Kubernetes won the O&amp;#39;Reilly Most Impact Award. Thanks to our contributors and users! &lt;a href=&#34;https://t.co/T6Co1wpsAh&#34;&gt;pic.twitter.com/T6Co1wpsAh&lt;/a&gt;&lt;/p&gt;&amp;mdash; Brian Grant (@bgrant0607) &lt;a href=&#34;https://twitter.com/bgrant0607/status/1019995276235325440?ref_src=twsrc%5Etfw&#34;&gt;July 19, 2018&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The initial goal of this small team was to develop a “minimally viable orchestrator.” From experience we knew that the basic feature set for such an orchestrator was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Replication to deploy multiple instances of an application&lt;/li&gt;
&lt;li&gt;Load balancing and service discovery to route traffic to these replicated containers&lt;/li&gt;
&lt;li&gt;Basic health checking and repair to ensure a self-healing system&lt;/li&gt;
&lt;li&gt;Scheduling to group many machines into a single pool and distribute work to them&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Along the way, we also spent a significant chunk of our time convincing executive leadership that open sourcing this project was a good idea. I’m endlessly grateful to Craig for writing numerous whitepapers and to Eric Brewer, for the early and vocal support that he lent us to ensure that Kubernetes could see the light of day.&lt;/p&gt;

&lt;p&gt;In June of 2014 when Kubernetes was released to the world, the list above was the sum total of its basic feature set. As an early stage open source community, we then spent a year building, expanding, polishing and fixing this initial minimally viable orchestrator into the product that we released as a 1.0 in OSCON in 2015. We were very lucky to be joined early on by the very capable OpenShift team which lent significant engineering and real world enterprise expertise to the project. Without their perspective and contributions, I don’t think we would be standing here today.&lt;/p&gt;

&lt;p&gt;Three years later, the Kubernetes community has grown exponentially, and Kubernetes has become synonymous with cloud native container orchestration. There are more than 1700 people who have contributed to Kubernetes, there are more than 500 Kubernetes meetups worldwide and more than 42000 users have joined the #kubernetes-dev channel. What’s more, the community that we have built works successfully across geographic, language and corporate boundaries. It is a truly open, engaged and collaborative community, and in-and-of-itself and amazing achievement. Many thanks to everyone who has helped make it what it is today. Kubernetes is a commodity in the public cloud because of you.&lt;/p&gt;

&lt;p&gt;But if Kubernetes is a commodity, then what is the future? Certainly, there are an endless array of tweaks, adjustments and improvements to the core codebase that will occupy us for years to come, but the true future of Kubernetes are the applications and experiences that are being built on top of this new, ubiquitous platform.&lt;/p&gt;

&lt;p&gt;Kubernetes has dramatically reduced the complexity to build new developer experiences, and a myriad of new experiences have been developed or are in the works that provide simplified or targeted developer experiences like Functions-as-a-Service, on top of core Kubernetes-as-a-Service.&lt;/p&gt;

&lt;p&gt;The Kubernetes cluster itself is being extended with custom resource definitions (which I first described to Kelsey Hightower on a walk from OSCON to a nearby restaurant in 2015), these new resources allow cluster operators to enable new plugin functionality that extend and enhance the APIs that their users have access to.&lt;/p&gt;

&lt;p&gt;By embedding core functionality like logging and monitoring in the cluster itself and enabling developers to take advantage of such services simply by deploying their application into the cluster, Kubernetes has reduced the learning necessary for developers to build scalable reliable applications.&lt;/p&gt;

&lt;p&gt;Finally, Kubernetes has provided a new, common vocabulary for expressing the patterns and paradigms of distributed system development. This common vocabulary means that we can more easily describe and discuss the common ways in which our distributed systems are built, and furthermore we can build standardized, re-usable implementations of such systems. The net effect of this is the development of higher quality, reliable distributed systems, more quickly.&lt;/p&gt;

&lt;p&gt;It’s truly amazing to see how far Kubernetes has come, from a rough idea in the minds of three people in Seattle to a phenomenon that has redirected the way we think about cloud native development across the world. It has been an amazing journey, but what’s truly amazing to me, is that I think we’re only just now scratching the surface of the impact that Kubernetes will have. Thank you to everyone who has enabled us to get this far, and thanks to everyone who will take us further.&lt;/p&gt;

&lt;p&gt;Brendan&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes Wins the 2018 OSCON Most Impact Award</title>
      <link>https://docstest.github.io/blog/2018/07/19/kubernetes-wins-2018-oscon-most-impact-award/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/19/kubernetes-wins-2018-oscon-most-impact-award/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Brian Grant (Principal Engineer, Google) and Tim Hockin (Principal Engineer, Google)&lt;/p&gt;

&lt;p&gt;We are humbled to be recognized by the community with this award.&lt;/p&gt;

&lt;p&gt;We had high hopes when we created Kubernetes. We wanted to change the way cloud applications were deployed and managed. Whether we’d succeed or not was very uncertain. And look how far we’ve come in such a short time.&lt;/p&gt;

&lt;p&gt;The core technology behind Kubernetes was informed by &lt;a href=&#34;https://ai.google/research/pubs/pub44843&#34; target=&#34;_blank&#34;&gt;lessons learned from Google’s internal infrastructure&lt;/a&gt;, but nobody can deny the enormous role of the Kubernetes community in the success of the project. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&#34; target=&#34;_blank&#34;&gt;The community, of which Google is a part&lt;/a&gt;, now drives every aspect of the project: the design, development, testing, documentation, releases, and more. That is what makes Kubernetes fly.&lt;/p&gt;

&lt;p&gt;While we actively sought partnerships and community engagement, none of us anticipated just how important the open-source community would be, how fast it would grow, or how large it would become. Honestly,  we really didn’t have much of a plan.&lt;/p&gt;

&lt;p&gt;We looked to other open-source projects for inspiration and advice: Docker (now Moby), other open-source projects at Google such as Angular and Go, the Apache Software Foundation, OpenStack, Node.js, Linux, and others. But it became clear that there was no clear-cut recipe we could follow. So we winged it.&lt;/p&gt;

&lt;p&gt;Rather than rehashing history, we thought we’d share two high-level lessons we learned along the way.&lt;/p&gt;

&lt;p&gt;First, in order to succeed, community health and growth needs to be treated as a top priority. It’s hard, and it is time-consuming. It requires attention to both internal project dynamics and outreach, as well as constant vigilance to build and sustain relationships, be inclusive, maintain open communication, and remain responsive to contributors and users. Growing existing contributors and onboarding new ones is critical to sustaining project growth, but that takes time and energy that might otherwise be spent on development. These things have to become core values in order for contributors to keep them going.&lt;/p&gt;

&lt;p&gt;Second, start simple with how the project is organized and operated, but be ready to adopt to more scalable approaches as it grows. Over time, Kubernetes has transitioned from what was effectively a single team and git repository to many subgroups (Special Interest Groups and Working Groups), sub-projects, and repositories. From manual processes to fully automated ones. From informal policies to formal governance.&lt;/p&gt;

&lt;p&gt;We certainly didn’t get everything right or always adapt quickly enough, and we constantly struggle with scale. &lt;a href=&#34;https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1&#34; target=&#34;_blank&#34;&gt;At this point&lt;/a&gt;, Kubernetes has more than 20,000 contributors and is approaching one million comments on its issues and pull requests, &lt;a href=&#34;https://www.cncf.io/blog/2017/02/27/measuring-popularity-kubernetes-using-bigquery/&#34; target=&#34;_blank&#34;&gt;making it one of the fastest moving projects in the history of open source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thank you to all our contributors and to all the users who’ve stuck with us on the sometimes bumpy journey. This project would not be what it is today without the community.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 11 Ways (Not) to Get Hacked</title>
      <link>https://docstest.github.io/blog/2018/07/18/11-ways-not-to-get-hacked/</link>
      <pubDate>Wed, 18 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/18/11-ways-not-to-get-hacked/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Andrew Martin (ControlPlane)&lt;/p&gt;

&lt;p&gt;Kubernetes security has come a long way since the project&amp;#39;s inception, but still contains some gotchas. Starting with the control plane, building up through workload and network security, and finishing with a projection into the future of security, here is a list of handy tips to help harden your clusters and increase their resilience if compromised.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part-one-the-control-plane&#34;&gt;Part One: The Control Plane&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-tls-everywhere&#34;&gt;1. TLS Everywhere&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-enable-rbac-with-least-privilege-disable-abac-and-monitor-logs&#34;&gt;2. Enable RBAC with Least Privilege, Disable ABAC, and Monitor Logs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-use-third-party-auth-for-api-server&#34;&gt;3. Use Third Party Auth for API Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-separate-and-firewall-your-etcd-cluster&#34;&gt;4. Separate and Firewall your etcd Cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-rotate-encryption-keys&#34;&gt;5. Rotate Encryption Keys&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-two-workloads&#34;&gt;Part Two: Workloads&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#6-use-linux-security-features-and-podsecuritypolicies&#34;&gt;6. Use Linux Security Features and PodSecurityPolicies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-statically-analyse-yaml&#34;&gt;7. Statically Analyse YAML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-run-containers-as-a-non-root-user&#34;&gt;8. Run Containers as a Non-Root User&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-use-network-policies&#34;&gt;9. Use Network Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#10-scan-images-and-run-ids&#34;&gt;10. Scan Images and Run IDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-three-the-future&#34;&gt;Part Three: The Future&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#11-run-a-service-mesh&#34;&gt;11. Run a Service Mesh&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;part-one-the-control-plane&#34;&gt;Part One: The Control Plane&lt;/h1&gt;

&lt;p&gt;The control plane is Kubernetes&amp;#39; brain. It has an overall view of every container and pod running on the cluster, can schedule new pods (which can include containers with root access to their parent node), and can read all the secrets stored in the cluster. This valuable cargo needs protecting from accidental leakage and malicious intent: when it&amp;#39;s accessed, when it&amp;#39;s at rest, and when it&amp;#39;s being transported across the network.&lt;/p&gt;

&lt;h2 id=&#34;1-tls-everywhere&#34;&gt;1. TLS Everywhere&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;TLS should be enabled for every component that supports it to prevent traffic sniffing, verify the identity of the server, and (for mutual TLS) verify the identity of the client.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that some components and installation methods may enable local ports over HTTP and administrators should familiarize themselves with the settings of each component to identify potentially unsecured traffic.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/#use-transport-level-security-tls-for-all-api-traffic&#34; target=&#34;_blank&#34;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This network diagram by &lt;a href=&#34;https://docs.google.com/presentation/d/1Gp-2blk5WExI_QR59EUZdwfO2BWLJqa626mK2ej-huo/edit#slide=id.g1e639c415b_0_56&#34; target=&#34;_blank&#34;&gt;Lucas Käldström&lt;/a&gt; demonstrates some of the places TLS should ideally be applied: between every component on the master, and between the Kubelet and API server. &lt;a href=&#34;https://twitter.com/kelseyhightower/&#34; target=&#34;_blank&#34;&gt;Kelsey Hightower&lt;/a&gt;&amp;#39;s canonical &lt;a href=&#34;https://github.com/kelseyhightower/kubernetes-the-hard-way/blob/1.9.0/docs/04-certificate-authority.md&#34; target=&#34;_blank&#34;&gt;Kubernetes The Hard Way&lt;/a&gt; provides detailed manual instructions, as does &lt;a href=&#34;https://coreos.com/etcd/docs/latest/op-guide/security.html&#34; target=&#34;_blank&#34;&gt;etcd&amp;#39;s security model&lt;/a&gt; documentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/kubernetes-control-plane.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Autoscaling Kubernetes nodes was historically difficult, as each node requires a TLS key to connect to the master, and baking secrets into base images is not good practice. &lt;a href=&#34;https://medium.com/@toddrosner/kubernetes-tls-bootstrapping-cf203776abc7&#34; target=&#34;_blank&#34;&gt;Kubelet TLS bootstrapping&lt;/a&gt; provides the ability for a new kubelet to create a certificate signing request so that certificates are generated at boot time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/node-tls-bootstrap.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-enable-rbac-with-least-privilege-disable-abac-and-monitor-logs&#34;&gt;2. Enable RBAC with Least Privilege, Disable ABAC, and Monitor Logs&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Role-based access control provides fine-grained policy management for user access to resources, such as access to namespaces.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/rbac2.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes&amp;rsquo; ABAC (Attribute Based Access Control) has been &lt;a href=&#34;http://kubernetes.io/blog/2017/04/rbac-support-in-kubernetes.html&#34; target=&#34;_blank&#34;&gt;superseded by RBAC&lt;/a&gt; since release 1.6, and should not be enabled on the API server. Use RBAC instead:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--authorization-mode=RBAC
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or use this flag to disable it in GKE:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;--no-enable-legacy-authorization
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are plenty of &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/&#34; target=&#34;_blank&#34;&gt;good examples&lt;/a&gt; of &lt;a href=&#34;https://github.com/uruddarraju/kubernetes-rbac-policies&#34; target=&#34;_blank&#34;&gt;RBAC policies for cluster services&lt;/a&gt;, as well as &lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#role-binding-examples&#34; target=&#34;_blank&#34;&gt;the docs&lt;/a&gt;. And it doesn&amp;#39;t have to stop there - fine-grained RBAC policies can be extracted from audit logs with &lt;a href=&#34;https://github.com/liggitt/audit2rbac&#34; target=&#34;_blank&#34;&gt;audit2rbac&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Incorrect or excessively permissive RBAC policies are a security threat in case of a compromised pod. Maintaining least privilege, and continuously reviewing and improving RBAC rules, should be considered part of the &amp;ldquo;technical debt hygiene&amp;rdquo; that teams build into their development lifecycle.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/audit/&#34; target=&#34;_blank&#34;&gt;Audit Logging&lt;/a&gt; (beta in 1.10) provides customisable API logging at the payload (e.g. request and response), and also metadata levels. Log levels can be tuned to your organisation&amp;#39;s security policy - &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging#audit_policy&#34; target=&#34;_blank&#34;&gt;GKE&lt;/a&gt; provides sane defaults to get you started.&lt;/p&gt;

&lt;p&gt;For read requests such as get, list, and watch, only the request object is saved in the audit logs; the response object is not. For requests involving sensitive data such as Secret and ConfigMap, only the metadata is exported. For all other requests, both request and response objects are saved in audit logs.&lt;/p&gt;

&lt;p&gt;Don&amp;#39;t forget: keeping these logs inside the cluster is a security threat in case of compromise. These, like all other security-sensitive logs, should be transported outside the cluster to prevent tampering in the event of a breach.&lt;/p&gt;

&lt;h2 id=&#34;3-use-third-party-auth-for-api-server&#34;&gt;3. Use Third Party Auth for API Server&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Centralising authentication and authorisation across an organisation (aka Single Sign On) helps onboarding, offboarding, and consistent permissions for users&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Integrating Kubernetes with third party auth providers (like Google or Github) uses the remote platform&amp;#39;s identity guarantees (backed up by things like 2FA) and prevents administrators having to reconfigure the Kubernetes API server to add or remove users.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/coreos/dex&#34; target=&#34;_blank&#34;&gt;Dex&lt;/a&gt; is an OpenID Connect Identity (OIDC) and OAuth 2.0 provider with pluggable connectors. Pusher takes this a stage further with &lt;a href=&#34;https://thenewstack.io/kubernetes-single-sign-one-less-identity/&#34; target=&#34;_blank&#34;&gt;some custom tooling&lt;/a&gt;, and there are some &lt;a href=&#34;https://github.com/negz/kuberos&#34; target=&#34;_blank&#34;&gt;other&lt;/a&gt; &lt;a href=&#34;https://github.com/micahhausler/k8s-oidc-helper&#34; target=&#34;_blank&#34;&gt;helpers&lt;/a&gt; available with slightly different use cases.&lt;/p&gt;

&lt;h2 id=&#34;4-separate-and-firewall-your-etcd-cluster&#34;&gt;4. Separate and Firewall your etcd Cluster&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;etcd stores information on state and secrets, and is a critical Kubernetes component - it should be protected differently from the rest of your cluster.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Write access to the API server&amp;#39;s etcd is equivalent to gaining root on the entire cluster, and even read access can be used to escalate privileges fairly easily.&lt;/p&gt;

&lt;p&gt;The Kubernetes scheduler will search etcd for pod definitions that do not have a node. It then sends the pods it finds to an available kubelet for scheduling. Validation for submitted pods is performed by the API server before it writes them to etcd, so malicious users writing directly to etcd can bypass many security mechanisms - e.g. PodSecurityPolicies.&lt;/p&gt;

&lt;p&gt;etcd should be configured with &lt;a href=&#34;https://github.com/coreos/etcd/blob/master/Documentation/op-guide/security.md&#34; target=&#34;_blank&#34;&gt;peer and client TLS certificates&lt;/a&gt;, and deployed on dedicated nodes. To mitigate against private keys being stolen and used from worker nodes, the cluster can also be firewalled to the API server.&lt;/p&gt;

&lt;h2 id=&#34;5-rotate-encryption-keys&#34;&gt;5. Rotate Encryption Keys&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A security best practice is to regularly rotate encryption keys and certificates, in order to limit the &amp;quot;blast radius&amp;quot; of a key compromise.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes will &lt;a href=&#34;https://kubernetes.io/docs/tasks/tls/certificate-rotation/&#34; target=&#34;_blank&#34;&gt;rotate some certificates automatically&lt;/a&gt; (notably, the kubelet client and server certs) by creating new CSRs as its existing credentials expire.&lt;/p&gt;

&lt;p&gt;However, the &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34; target=&#34;_blank&#34;&gt;symmetric encryption keys&lt;/a&gt; that the API server uses to encrypt etcd values are not automatically rotated - they must be &lt;a href=&#34;https://www.twistlock.com/2017/08/02/kubernetes-secrets-encryption/&#34; target=&#34;_blank&#34;&gt;rotated manually&lt;/a&gt;. Master access is required to do this, so managed services (such as GKE or AKS) abstract this problem from an operator.&lt;/p&gt;

&lt;h1 id=&#34;part-two-workloads&#34;&gt;Part Two: Workloads&lt;/h1&gt;

&lt;p&gt;With minimum viable security on the control plane the cluster is able to operate securely. But, like a ship carrying potentially dangerous cargo, the ship&amp;rsquo;s containers must be protected to contain that cargo in the event of an unexpected accident or breach. The same is true for Kubernetes workloads (pods, deployments, jobs, sets, etc.) - they may be trusted at deployment time, but if they&amp;#39;re internet-facing there&amp;#39;s always a risk of later exploitation. Running workloads with minimal privileges and hardening their runtime configuration can help to mitigate this risk.&lt;/p&gt;

&lt;h2 id=&#34;6-use-linux-security-features-and-podsecuritypolicies&#34;&gt;6. Use Linux Security Features and PodSecurityPolicies&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The Linux kernel has a number of overlapping security extensions (capabilities, SELinux, AppArmor, seccomp-bpf) that can be configured to provide least privilege to applications&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Tools like &lt;a href=&#34;https://github.com/genuinetools/bane&#34; target=&#34;_blank&#34;&gt;bane&lt;/a&gt; can help to generate AppArmor profiles, and &lt;a href=&#34;https://github.com/docker-slim/docker-slim#quick-seccomp-example&#34; target=&#34;_blank&#34;&gt;docker-slim&lt;/a&gt; for seccomp profiles, but beware - a comprehensive test suite it required to exercise all code paths in your application when verifying the side effects of applying these policies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34; target=&#34;_blank&#34;&gt;PodSecurityPolicies&lt;/a&gt; can be used to mandate the use of security extensions and other Kubernetes security directives. They provide a minimum contract that a pod must fulfil to be submitted to the API server - including security profiles, the privileged flag, and the sharing of host network, process, or IPC namespaces.&lt;/p&gt;

&lt;p&gt;These directives are important, as they help to prevent containerised processes from escaping their isolation boundaries, and &lt;a href=&#34;https://twitter.com/tallclair&#34; target=&#34;_blank&#34;&gt;Tim Allclair&lt;/a&gt;&amp;#39;s &lt;a href=&#34;https://gist.github.com/tallclair/11981031b6bfa829bb1fb9dcb7e026b0&#34; target=&#34;_blank&#34;&gt;example PodSecurityPolicy&lt;/a&gt; is a comprehensive resource that you can customise to your use case.&lt;/p&gt;

&lt;h2 id=&#34;7-statically-analyse-yaml&#34;&gt;7. Statically Analyse YAML&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Where PodSecurityPolicies deny access to the API server, static analysis can also be used in the development workflow to model an organisation&amp;#39;s compliance requirements or risk appetite.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sensitive information should not be stored in pod-type YAML resource (deployments, pods, sets, etc.), and sensitive configmaps and secrets should be encrypted with tools such as &lt;a href=&#34;https://github.com/coreos/vault-operator&#34; target=&#34;_blank&#34;&gt;vault&lt;/a&gt; (with CoreOS&amp;#39;s operator), &lt;a href=&#34;https://github.com/AGWA/git-crypt&#34; target=&#34;_blank&#34;&gt;git-crypt&lt;/a&gt;, &lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets&#34; target=&#34;_blank&#34;&gt;sealed secrets&lt;/a&gt;, or &lt;a href=&#34;https://cloud.google.com/kms/&#34; target=&#34;_blank&#34;&gt;cloud provider KMS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Static analysis of YAML configuration can be used to establish a baseline for runtime security. &lt;a href=&#34;https://kubesec.io/&#34; target=&#34;_blank&#34;&gt;kubesec&lt;/a&gt; generates risk scores for resources:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;score&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;-30&lt;/span&gt;,
  &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;scoring&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;critical&amp;#34;&lt;/span&gt;: [{
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;selector&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;containers[] .securityContext .privileged == true&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Privileged containers can allow almost completely unrestricted host access&amp;#34;&lt;/span&gt;
    }],
    &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;advise&amp;#34;&lt;/span&gt;: [{
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;selector&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;containers[] .securityContext .runAsNonRoot == true&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Force the running image to run as a non-root user to ensure least privilege&amp;#34;&lt;/span&gt;
    }, {
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;selector&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;containers[] .securityContext .capabilities .drop&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Reducing kernel capabilities available to a container limits its attack surface&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#008000;font-weight:bold&#34;&gt;&amp;#34;href&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;https://kubernetes.io/docs/tasks/configure-pod-container/security-context/&amp;#34;&lt;/span&gt;
    }]
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And &lt;a href=&#34;https://github.com/garethr/kubetest&#34; target=&#34;_blank&#34;&gt;kubetest&lt;/a&gt; is a unit test framework for Kubernetes configurations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;#// vim: set ft=python:&lt;/span&gt;
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#00a000&#34;&gt;test_for_team_label&lt;/span&gt;():
    &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;if&lt;/span&gt; spec[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#666&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Deployment&amp;#34;&lt;/span&gt;:
        labels &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; spec[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;spec&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;template&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;][&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;]
        assert_contains(labels, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;team&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;should indicate which team owns the deployment&amp;#34;&lt;/span&gt;)

test_for_team_label()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These tools &amp;quot;&lt;a href=&#34;https://en.wikipedia.org/wiki/Shift_left_testing&#34; target=&#34;_blank&#34;&gt;shift left&lt;/a&gt;&amp;quot; (moving checks and verification earlier in the development cycle). Security testing in the development phase gives users fast feedback about code and configuration that may be rejected by a later manual or automated check, and can reduce the friction of introducing more secure practices.&lt;/p&gt;

&lt;h2 id=&#34;8-run-containers-as-a-non-root-user&#34;&gt;8. Run Containers as a Non-Root User&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Containers that run as root frequently have far more permissions than their workload requires which, in case of compromise, could help an attacker further their attack.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Containers still rely on the traditional Unix security model (called &lt;a href=&#34;https://www.linux.com/learn/overview-linux-kernel-security-features&#34; target=&#34;_blank&#34;&gt;discretionary access control&lt;/a&gt; or DAC) - everything is a file, and permissions are granted to users and groups.&lt;/p&gt;

&lt;p&gt;User namespaces are not enabled in Kubernetes. This means that a container&amp;#39;s user ID table maps to the host&amp;#39;s user table, and running a process as the root user inside a container runs it as root on the host. Although we have layered security mechanisms to prevent container breakouts, running as root inside the container is still not recommended.&lt;/p&gt;

&lt;p&gt;Many container images use the root user to run PID 1 - if that process is compromised, the attacker has root in the container, and any mis-configurations become much easier to exploit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://engineering.bitnami.com/articles/running-non-root-containers-on-openshift.html&#34; target=&#34;_blank&#34;&gt;Bitnami has done a lot of work&lt;/a&gt; moving their container images to &lt;a href=&#34;https://github.com/bitnami/bitnami-docker-nginx/blob/b068b8bd01eb2f5a7314c09466724f86aa4548f9/1.12/Dockerfile#L28&#34; target=&#34;_blank&#34;&gt;non-root users&lt;/a&gt; (especially as OpenShift requires this by default), which may ease a migration to non-root container images.&lt;/p&gt;

&lt;p&gt;This PodSecurityPolicy snippet prevents running processes as root inside a container, and also escalation to root:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Required to prevent escalations to root.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;allowPrivilegeEscalation:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;runAsUser:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# Require the container to run without root privileges.&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;rule:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;MustRunAsNonRoot&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Non-root containers cannot bind to the privileged ports under 1024 (this is gated by the CAP_NET_BIND_SERVICE kernel capability), but services can be used to disguise this fact. In this example the fictional MyApp application is bound to port 8443 in its container, but the service exposes it on 443 by proxying the request to the targetPort:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;my-service&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;selector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;MyApp&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;protocol:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;TCP&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;port:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;targetPort:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;8443&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Having to run workloads as a non-root user is not going to change until user namespaces are usable, or the ongoing work to &lt;a href=&#34;https://rootlesscontaine.rs/&#34; target=&#34;_blank&#34;&gt;run containers without root&lt;/a&gt; lands in container runtimes.&lt;/p&gt;

&lt;h2 id=&#34;9-use-network-policies&#34;&gt;9. Use Network Policies&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;By default, Kubernetes networking allows all pod to pod traffic; this can be restricted using a&lt;/strong&gt; &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Network Policy&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/kubernetes-networking.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Traditional services are restricted with firewalls, which use static IP and port ranges for each service. As these IPs very rarely change they have historically been used as a form of identity. Containers rarely have static IPs - they are built to fail fast, be rescheduled quickly, and use service discovery instead of static IP addresses. These properties mean that firewalls become much more difficult to configure and review.&lt;/p&gt;

&lt;p&gt;As Kubernetes stores all its system state in etcd it can configure dynamic firewalling - if it is supported by the CNI networking plugin. Calico, Cilium, kube-router, Romana, and Weave Net all support network policy.&lt;/p&gt;

&lt;p&gt;It should be noted that these policies fail-closed, so the absence of a podSelector here defaults to a wildcard:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NetworkPolicy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;default-deny&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;podSelector:&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here&amp;#39;s an example NetworkPolicy that denies all egress except UDP 53 (DNS), which also prevents inbound connections to your application. &lt;a href=&#34;https://www.weave.works/blog/securing-microservices-kubernetes/&#34; target=&#34;_blank&#34;&gt;NetworkPolicies are stateful&lt;/a&gt;, so the replies to outbound requests still reach the application.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;networking.k8s.io/v1&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;kind:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;NetworkPolicy&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;metadata:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;name:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myapp-deny-external-egress&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;&lt;/span&gt;spec:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;podSelector:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;matchLabels:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;app:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;myapp&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;policyTypes:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;Egress&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;egress:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;ports:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;port:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;53&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;protocol:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;UDP&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;to:&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;-&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;namespaceSelector:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;{}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Kubernetes network policies can not be applied to DNS names. This is because DNS can resolve round-robin to many IPs, or dynamically based on the calling IP, so network policies can be applied to a fixed IP or podSelector (for dynamic Kubernetes IPs) only.&lt;/p&gt;

&lt;p&gt;Best practice is to start by denying all traffic for a namespace and incrementally add routes to allow an application to pass its acceptance test suite. This can become complex, so ControlPlane hacked together &lt;a href=&#34;https://github.com/controlplaneio/netassert&#34; target=&#34;_blank&#34;&gt;netassert&lt;/a&gt; - network security testing for DevSecOps workflows with highly parallelised nmap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;k8s:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# used for Kubernetes pods&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;deployment:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# only deployments currently supported&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;test-frontend:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# pod name, defaults to `default` namespace&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;test-microservice:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# `test-microservice` is the DNS name of the target service&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;test-database:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;     &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# `test-frontend` should not be able to access test-database’s port 80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;169.254&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;169.254&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;           &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# AWS metadata API&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;metadata.google.internal:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# GCP metadata API&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;    &lt;/span&gt;new-namespace:test-microservice:&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# `new-namespace` is the namespace name&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;test-database.new-namespace:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# longer DNS names can be used for other namespaces&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;test-frontend.default:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;169.254&lt;/span&gt;.&lt;span style=&#34;color:#666&#34;&gt;169.254&lt;/span&gt;:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;           &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# AWS metadata API&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;      &lt;/span&gt;metadata.google.internal:&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;80&lt;/span&gt;,&lt;span style=&#34;color:#bbb&#34;&gt; &lt;/span&gt;-&lt;span style=&#34;color:#666&#34;&gt;443&lt;/span&gt;&lt;span style=&#34;color:#bbb&#34;&gt;  &lt;/span&gt;&lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# GCP metadata API&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Cloud provider metadata APIs are a constant source of escalation (as the recent &lt;a href=&#34;https://hackerone.com/reports/341876&#34; target=&#34;_blank&#34;&gt;Shopify&lt;/a&gt; &lt;a href=&#34;https://hackerone.com/reports/341876&#34; target=&#34;_blank&#34;&gt;bug bounty&lt;/a&gt; demonstrates), so specific tests to confirm that the APIs are blocked on the container network helps to guard against accidental misconfiguration.&lt;/p&gt;

&lt;h2 id=&#34;10-scan-images-and-run-ids&#34;&gt;10. Scan Images and Run IDS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Web servers present an attack surface to the network they&amp;#39;re attached to: scanning an image&amp;#39;s installed files ensures the absence of known vulnerabilities that an attacker could exploit to gain remote access to the container. An IDS (Intrusion Detection System) detects them if they do.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes permits pods into the cluster through a series of &lt;a href=&#34;https://kubernetes.io/docs/admin/admission-controllers/&#34; target=&#34;_blank&#34;&gt;admission controller&lt;/a&gt; gates, which are applied to pods and other resources like deployments. These gates can validate each pod for admission or change its contents, and they now support backend webhooks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/admission-controllers.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These webhooks can be used by container image scanning tools to validate images before they are deployed to the cluster. Images that have failed checks can be refused admission.&lt;/p&gt;

&lt;p&gt;Scanning container images for known vulnerabilities can reduce the window of time that an attacker can exploit a disclosed CVE. Free tools such as CoreOS&amp;#39;s &lt;a href=&#34;https://github.com/coreos/clair&#34; target=&#34;_blank&#34;&gt;Clair&lt;/a&gt; and Aqua&amp;#39;s &lt;a href=&#34;https://github.com/aquasecurity/microscanner&#34; target=&#34;_blank&#34;&gt;Micro Scanner&lt;/a&gt; should be used in a deployment pipeline to prevent the deployment of images with critical, exploitable vulnerabilities.&lt;/p&gt;

&lt;p&gt;Tools such as &lt;a href=&#34;https://grafeas.io/&#34; target=&#34;_blank&#34;&gt;Grafeas&lt;/a&gt; can store image metadata for constant compliance and vulnerability checks against a container&amp;#39;s unique signature (a &lt;a href=&#34;https://en.wikipedia.org/wiki/Content-addressable_storage&#34; target=&#34;_blank&#34;&gt;content addressable&lt;/a&gt; hash). This means that scanning a container image with that hash is the same as scanning the images deployed in production, and can be done continually without requiring access to production environments.&lt;/p&gt;

&lt;p&gt;Unknown Zero Day vulnerabilities will always exist, and so intrusion detection tools such as &lt;a href=&#34;https://www.twistlock.com/&#34; target=&#34;_blank&#34;&gt;Twistlock&lt;/a&gt;, &lt;a href=&#34;https://www.aquasec.com/&#34; target=&#34;_blank&#34;&gt;Aqua&lt;/a&gt;, and &lt;a href=&#34;https://sysdig.com/product/secure/&#34; target=&#34;_blank&#34;&gt;Sysdig Secure&lt;/a&gt; should be deployed in Kubernetes. IDS detects unusual behaviours in a container and pauses or kills it - &lt;a href=&#34;https://github.com/draios/falco&#34; target=&#34;_blank&#34;&gt;Sysdig&amp;#39;s Falco&lt;/a&gt; is a an Open Source rules engine, and an entrypoint to this ecosystem.&lt;/p&gt;

&lt;h1 id=&#34;part-three-the-future&#34;&gt;Part Three: The Future&lt;/h1&gt;

&lt;p&gt;The next stage of security&amp;#39;s &amp;quot;cloud native evolution&amp;quot; looks to be the service mesh, although adoption may take time - migration involves shifting complexity from applications to the mesh infrastructure, and organisations will be keen to understand best-practice.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-11-ways-not-to-get-hacked/service-mesh-@sebiwicb.png&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;11-run-a-service-mesh&#34;&gt;11. Run a Service Mesh&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A service mesh is a web of encrypted persistent connections, made between high performance &amp;quot;sidecar&amp;quot; proxy servers like Envoy and Linkerd. It adds traffic management, monitoring, and policy - all without microservice changes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Offloading microservice security and networking code to a shared, battle tested set of libraries was already possible with &lt;a href=&#34;https://linkerd.io/&#34; target=&#34;_blank&#34;&gt;Linkerd&lt;/a&gt;, and the introduction of &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34;&gt;Istio&lt;/a&gt; by Google, IBM, and Lyft, has added an alternative in this space. With the addition of &lt;a href=&#34;https://spiffe.io&#34; target=&#34;_blank&#34;&gt;SPIFFE&lt;/a&gt; for per-pod cryptographic identity and a plethora of &lt;a href=&#34;https://istio.io/docs/concepts/what-is-istio/overview.html&#34; target=&#34;_blank&#34;&gt;other features&lt;/a&gt;, Istio could simplify the deployment of the next generation of network security.&lt;/p&gt;

&lt;p&gt;In &amp;quot;Zero Trust&amp;quot; networks there may be no need for traditional firewalling or Kubernetes network policy, as every interaction occurs over mTLS (mutual TLS), ensuring that both parties are not only communicating securely, but that the identity of both services is known.&lt;/p&gt;

&lt;p&gt;This shift from traditional networking to Cloud Native security principles is not one we expect to be easy for those with a traditional security mindset, and the &lt;a href=&#34;https://amzn.to/2Gg6Pav&#34; target=&#34;_blank&#34;&gt;Zero Trust Networking book&lt;/a&gt; from SPIFFE&amp;#39;s &lt;a href=&#34;https://twitter.com/evan2645&#34; target=&#34;_blank&#34;&gt;Evan Gilman&lt;/a&gt; is a highly recommended introduction to this brave new world.&lt;/p&gt;

&lt;p&gt;Istio &lt;a href=&#34;https://istio.io/about/notes/0.8/&#34; target=&#34;_blank&#34;&gt;0.8 LTS&lt;/a&gt; is out, and the project is rapidly approaching a 1.0 release. Its stability versioning is the same as the Kubernetes model: a stable core, with individual APIs identifying themselves under their own alpha/beta stability namespace. Expect to see an uptick in Istio adoption over the coming months.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Cloud Native applications have a more fine-grained set of lightweight security primitives to lock down workloads and infrastructure. The power and flexibility of these tools is both a blessing and curse - with insufficient automation it has become easier to expose insecure workloads which permit breakouts from the container or its isolation model.&lt;/p&gt;

&lt;p&gt;There are more defensive tools available than ever, but caution must be taken to reduce attack surfaces and the potential for misconfiguration.&lt;/p&gt;

&lt;p&gt;However if security slows down an organisation&amp;#39;s pace of feature delivery it will never be a first-class citizen. Applying Continuous Delivery principles to the software supply chain allows an organisation to achieve compliance, continuous audit, and enforced governance without impacting the business&amp;#39;s bottom line.&lt;/p&gt;

&lt;p&gt;Iteratating quickly on security is easiest when supported by a comprehensive test suite. This is achieved with Continuous Security - an alternative to point-in-time penetration tests, with constant pipeline validation ensuring an organisation&amp;#39;s attack surface is known, and the risk constantly understood and managed.&lt;/p&gt;

&lt;p&gt;This is ControlPlane&amp;#39;s modus operandi: if we can help kickstart a Continuous Security discipline, deliver Kubernetes security and operations training, or co-implement a secure cloud native evolution for you, please &lt;a href=&#34;https://control-plane.io&#34; target=&#34;_blank&#34;&gt;get in touch&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Andrew Martin is a co-founder at &lt;a href=&#34;https://twitter.com/controlplaneio&#34; target=&#34;_blank&#34;&gt;@controlplaneio&lt;/a&gt; and tweets about cloud native security at &lt;a href=&#34;https://twitter.com/sublimino&#34; target=&#34;_blank&#34;&gt;@sublimino&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: How the sausage is made: the Kubernetes 1.11 release interview, from the Kubernetes Podcast</title>
      <link>https://docstest.github.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/</guid>
      <description>
        
        
        &lt;p&gt;&lt;b&gt;Author&lt;/b&gt;: Craig Box (Google)&lt;/p&gt;

&lt;p&gt;At KubeCon EU, my colleague Adam Glick and I were pleased to announce the &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt;. In this weekly conversation, we focus on all the great things that are happening in the world of Kubernetes and Cloud Native. From the news of the week, to interviews with people in the community, we help you stay up to date on everything Kubernetes.&lt;/p&gt;

&lt;p&gt;We &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;recently had the pleasure of speaking&lt;/a&gt; to the release manager for Kubernetes 1.11, Josh Berkus from Red Hat, and the release manager for the upcoming 1.12, Tim Pepper from VMware.&lt;/p&gt;

&lt;p&gt;In this conversation we learned about the release process, the impact of quarterly releases on end users, and how Kubernetes is like baking.&lt;/p&gt;

&lt;p&gt;I encourage you to listen to &lt;a href=&#34;https://kubernetespodcast.com/episode/010-kubernetes-1.11/&#34; target=&#34;_blank&#34;&gt;the podcast version&lt;/a&gt; if you have a commute, or a dog to walk. If you like what you hear, &lt;a href=&#34;https://kubernetespodcast.com/subscribe&#34; target=&#34;_blank&#34;&gt;we encourage you to subscribe&lt;/a&gt;! In case you&amp;rsquo;re short on time, or just want to browse quickly, we are delighted to share the transcript with you.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: First of all, congratulations both, and thank you.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Well, thank you. Congratulations for me, because my job is done.&lt;/p&gt;

&lt;p&gt;[LAUGHTER]&lt;/p&gt;

&lt;p&gt;Congratulations and sympathy for Tim.&lt;/p&gt;

&lt;p&gt;[LAUGH]&lt;/p&gt;

&lt;p&gt;TIM PEPPER: Thank you, and I guess thank you?&lt;/p&gt;

&lt;p&gt;[LAUGH]&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: For those that don&amp;rsquo;t know a lot about the process, why don&amp;rsquo;t you help people understand — what is it like to be the release manager? What&amp;rsquo;s the process that a release goes through to get to the point when everyone just sees, OK, it&amp;rsquo;s released — 1.11 is available? What does it take to get up to that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: We have a quarterly release cycle. So every three months, we&amp;rsquo;re releasing. And ideally and fortunately, this is actually now how we are doing things. Somewhere around two, three weeks before the previous release, somebody volunteers to be the release lead. That person is confirmed by &lt;a href=&#34;https://github.com/kubernetes/sig-release&#34; target=&#34;_blank&#34;&gt;SIG Release&lt;/a&gt;. So far, we&amp;rsquo;ve never had more than one volunteer, so there hasn&amp;rsquo;t been really a fight about it.&lt;/p&gt;

&lt;p&gt;And then that person starts working with others to put together &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md&#34; target=&#34;_blank&#34;&gt;a team&lt;/a&gt; called the release team. Tim&amp;rsquo;s just gone through this with Stephen Augustus and &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.12/release_team.md&#34; target=&#34;_blank&#34;&gt;picking out a whole bunch of people&lt;/a&gt;. And then after or a little before— probably after, because we want to wait for the retrospective from the previous release— the release lead then sets &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release-1.11.md&#34; target=&#34;_blank&#34;&gt;a schedule&lt;/a&gt; for the upcoming release, as in when all the deadlines will be.&lt;/p&gt;

&lt;p&gt;And this is a thing, because we&amp;rsquo;re still tinkering with relative deadlines, and how long should code freeze be, and how should we track features? Because we don&amp;rsquo;t feel that we&amp;rsquo;ve gotten down that sort of cadence perfectly yet. I mean, like, we&amp;rsquo;ve done pretty well, but we don&amp;rsquo;t feel like we want to actually set [in stone], this is the schedule for each and every release.&lt;/p&gt;

&lt;p&gt;Also, we have to adjust the schedule because of holidays, right? Because you can&amp;rsquo;t have the code freeze deadline starting on July 4 or in the middle of design or sometime else when we&amp;rsquo;re going to have a large group of contributors who are out on vacation.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: This is something I&amp;rsquo;ve had to spend some time looking at, thinking about 1.12. Going back to early June as we were tinkering with the code freeze date, starting to think about, well, what are the implications going to be on 1.12? When would these things start falling on the calendar? And then also for 1.11, we had one complexity. If we slipped the release past this week, we start running into the US 4th of July holiday, and we&amp;rsquo;re not likely to get a lot done.&lt;/p&gt;

&lt;p&gt;So much of a slip would mean slipping into the middle of July before we&amp;rsquo;d really know that we were successfully triaging things. And worst case maybe, we&amp;rsquo;re quite a bit later into July.&lt;/p&gt;

&lt;p&gt;So instead of quarterly with a three-month sort of cadence, well, maybe we&amp;rsquo;ve accidentally ended up chopping out one month out of the next release or pushing it quite a bit into the end of the year. And that made the deliberation around things quite complex, but thankfully this week, everything&amp;rsquo;s gone smoothly in the end.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: All the releases so far have been one quarter — they&amp;rsquo;ve been a 12-week release cycle, give or take. Is that something that you think will continue going forward, or is the release team thinking about different ways they could run releases?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: The whole community is thinking about this. There are voices who&amp;rsquo;d like the cadence to be faster, and there are voices who&amp;rsquo;d like it to be shorter. And there&amp;rsquo;s good arguments for both.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Because it&amp;rsquo;s interesting. It sounds like it is a date-driven release cycle versus a feature-driven release cycle.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Yeah, certainly. I really honestly think everybody in the world of software recognizes that feature-driven release cycles just don&amp;rsquo;t work. And a big part of the duties of the release team collectively— several members of the team do this— is yanking things out of the release that are not ready. And the hard part of that is figuring out which things aren&amp;rsquo;t ready, right? Because the person who&amp;rsquo;s working on it tends to be super optimistic about what they can get done and what they can get fixed before the deadline.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Of course.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: And this is one of the things I think that&amp;rsquo;s useful about the process we have in place on the release team for having shadows who spend some time on the release team, working their way up into more of a lead position and gaining some experience, starting to get some exposure to see that optimism and see the processes for vetting.&lt;/p&gt;

&lt;p&gt;And it&amp;rsquo;s even an overstatement to say the process. Just see the way that we build the intuition for how to vet and understand and manage the risk, and really go after and chase things down proactively and early to get resolution in a timely way versus continuing to just all be optimistic and letting things maybe languish and put a release at risk.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: I&amp;rsquo;ve been reading this week about the introduction of &lt;a href=&#34;https://github.com/kubernetes/community/issues/566&#34; target=&#34;_blank&#34;&gt;feature branches&lt;/a&gt; to Kubernetes. The new server-side apply feature, for example, is being built in a branch so that it didn&amp;rsquo;t have to be half-built in master and then ripped out again as the release approached, if the feature wasn&amp;rsquo;t ready. That seems to me like something that&amp;rsquo;s a normal part of software development? Is there a reason it&amp;rsquo;s taken so long to bring that to core Kubernetes?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: I don&amp;rsquo;t actually know the history of why we&amp;rsquo;re not using feature branches. I mean, the reason why we&amp;rsquo;re not using feature branches pervasively now is that we have to transition from a different system. And I&amp;rsquo;m not really clear on how we adopted that linear development system. But it&amp;rsquo;s certainly something we discussed on the release team, because there were issues of features that we thought were going to be ready, and then developed major problems. And we&amp;rsquo;re like, if we have to back this out, that&amp;rsquo;s going to be painful. And we did actually have to back one feature out, which involved not pulling out a Git commit, but literally reversing the line changes, which is really not how you want to be doing things.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: No.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: The other big benefit, I think, to the release branches if they are well integrated with the CI system for continuous integration and testing, you really get the feedback, and you can demonstrate, this set of stuff is ready. And then you can do deferred commitment on the master branch. And what comes in to a particular release on the timely cadence that users are expecting is stuff that&amp;rsquo;s ready. You don&amp;rsquo;t have potentially destabilizing things, because you can get a lot more proof and evidence of readiness.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What are you looking at in terms of the tool chain that you&amp;rsquo;re using to do this? You mentioned a couple of things, and I know it&amp;rsquo;s obviously run through GitHub. But I imagine you have a number of other tools that you&amp;rsquo;re using in order to manage the release, to make sure that you understand what&amp;rsquo;s ready, what&amp;rsquo;s not. You mentioned balancing between people who are very optimistic about the feature they&amp;rsquo;re working on making it in versus the time-driven deadline, and balancing those two. Is that just a manual process, or do you have a set of tools that help you do that?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Well, there&amp;rsquo;s code review, obviously. So just first of all, process was somebody wants to actually put in a feature, commit, or any kind of merge really, right? So that has to be assigned to one of the SIGs, one of these Special Interest Groups. Possibly more than one, depending on what areas it touches.&lt;/p&gt;

&lt;p&gt;And then two generally overlapping groups of people have to approve that. One would be the SIG that it&amp;rsquo;s assigned to, and the second would be anybody represented in the OWNERS files in the code tree of the directories which get touched.&lt;/p&gt;

&lt;p&gt;Now sometimes those are the same group of people. I&amp;rsquo;d say often, actually. But sometimes they&amp;rsquo;re not completely the same group of people, because sometimes you&amp;rsquo;re making a change to the network, but that also happens to touch GCP support and OpenStack support, and so they need to review it as well.&lt;/p&gt;

&lt;p&gt;So the first part is the human part, which is a bunch of other people need to look at this. And possibly they&amp;rsquo;re going to comment &amp;ldquo;Hey. This is a really weird way to do this. Do you have a reason for it?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Then the second part of it is the automated testing that happens, the automated acceptance testing that happens via webhook on there. And actually, one of the things that we did that was a significant advancement in this release cycle— and by we, I actually mean not me, but the great folks at &lt;a href=&#34;https://github.com/kubernetes/community/tree/master/sig-scalability&#34; target=&#34;_blank&#34;&gt;SIG Scalability&lt;/a&gt; did— was add an &lt;a href=&#34;https://k8s-testgrid.appspot.com/sig-release-master-blocking#gce-scale-performance&#34; target=&#34;_blank&#34;&gt;additional acceptance test&lt;/a&gt; that does a mini performance test.&lt;/p&gt;

&lt;p&gt;Because one of the problems we&amp;rsquo;ve had historically is our major performance tests are large and take a long time to run, and so by the time we find out that we&amp;rsquo;re failing the performance tests, we&amp;rsquo;ve already accumulated, you know, 40, 50 commits. And so now we&amp;rsquo;re having to do git bisect to find out which of those commits actually caused the performance regression, which can make them very slow to address.&lt;/p&gt;

&lt;p&gt;And so adding that performance pre-submit, the performance acceptance test really has helped stabilize performance in terms of new commits. So then we have that level of testing that you have to get past.&lt;/p&gt;

&lt;p&gt;And then when we&amp;rsquo;re done with that level of testing, we run a whole large battery of larger tests— end-to-end tests, performance tests, upgrade and downgrade tests. And one of the things that we&amp;rsquo;ve added recently and we&amp;rsquo;re integrating to the process something called conformance tests. And the conformance test is we&amp;rsquo;re testing whether or not you broke backwards compatibility, because it&amp;rsquo;s obviously a big deal for Kubernetes users if you do that when you weren&amp;rsquo;t intending to.&lt;/p&gt;

&lt;p&gt;One of the busiest roles in the release team is a role called &lt;a href=&#34;https://github.com/kubernetes/sig-release#ci-signal-lead&#34; target=&#34;_blank&#34;&gt;CI Signal&lt;/a&gt;. And it&amp;rsquo;s that person&amp;rsquo;s job just to watch all of the tests for new things going red and then to try to figure out why they went red and bring it to people&amp;rsquo;s attention.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: I&amp;rsquo;ve often heard what you&amp;rsquo;re referring to kind of called a breaking change, because it breaks the existing systems that are running. How do you identify those to people so when they see, hey, there&amp;rsquo;s a new version of Kubernetes out there, I want to try it out, is that just going to release notes? Or is there a special way that you identify breaking changes as opposed to new features?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: That goes into release notes. I mean, keep in mind that one of the things that happens with Kubernetes&amp;rsquo; features is we go through this alpha, beta, general availability phase, right? So a feature&amp;rsquo;s alpha for a couple of releases and then becomes beta for a release or two, and then it becomes generally available. And part of the idea of having this that may require a feature to go through that cycle for a year or more before its general availability is by the time it&amp;rsquo;s general availability, we really want it to be, we are not going to change the API for this.&lt;/p&gt;

&lt;p&gt;However, stuff happens, and we do occasionally have to do those. And so far, our main way to identify that to people actually is in the release notes. If you look at &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#no-really-you-must-do-this-before-you-upgrade&#34; target=&#34;_blank&#34;&gt;the current release notes&lt;/a&gt;, there are actually two things in there right now that are sort of breaking changes.&lt;/p&gt;

&lt;p&gt;One of them is the bit with &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/&#34; target=&#34;_blank&#34;&gt;priority and preemption&lt;/a&gt; in that preemption being on by default now allows badly behaved users of the system to cause trouble in new ways. I&amp;rsquo;d actually have to look at the release notes to see what the second one was&amp;hellip;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: The &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/64612&#34; target=&#34;_blank&#34;&gt;JSON capitalization case sensitivity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Right. Yeah. And that was one of those cases where you have to break backwards compatibility, because due to a library switch, we accidentally enabled people using JSON in a case-insensitive way in certain APIs, which was never supposed to be the case. But because we didn&amp;rsquo;t have a specific test for that, we didn&amp;rsquo;t notice that we&amp;rsquo;d done it.&lt;/p&gt;

&lt;p&gt;And so for three releases, people could actually shove in malformed JSON, and Kubernetes would accept it. Well, we have to fix that now. But that does mean that there are going to be users out in the field who have malformed JSON in their configuration management that is now going to break.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: But at least the good news is Kubernetes was always outputting correct formatted JSON during this period, I understand.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Mm-hmm.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: I think that also kind of reminds of one of the other areas— so kind of going back to the question of, well, how do you share word of breaking changes? Well, one of the ways you do that is to have as much quality CI that you can to catch these things that are important. Give the feedback to the developer who&amp;rsquo;s making the breaking change, such that they don&amp;rsquo;t make the breaking change. And then you don&amp;rsquo;t actually have to communicate it out to users.&lt;/p&gt;

&lt;p&gt;So some of this is bound to happen, because you always have test escapes. But it&amp;rsquo;s also a reminder of the need to ensure that you&amp;rsquo;re also really building and maintaining your test cases and the quality and coverage of your CI system over time.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: What do you mean when you say test escapes?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: So I guess it&amp;rsquo;s a term in the art, but for those who aren&amp;rsquo;t familiar with it, you have intended behavior that wasn&amp;rsquo;t covered by test, and as a result, an unintended change happens to that. And instead of your intended behavior being shipped, you&amp;rsquo;re shipping something else.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: The JSON change is a textbook example of this, which is we were testing that the API would continue to accept correct JSON. We were not testing adequately that it wouldn&amp;rsquo;t accept incorrect JSON.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: A test escape, another way to think of it as you shipped a bug because there was not a test case highlighting the possibility of the bug.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: It&amp;rsquo;s the classic, we tested to make sure the feature worked. We didn&amp;rsquo;t test to make sure that breaking things didn&amp;rsquo;t work.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: It&amp;rsquo;s common for us to focus on &amp;ldquo;I&amp;rsquo;ve created this feature and I&amp;rsquo;m testing the positive cases&amp;rdquo;. And this also comes to thinking about things like secure by default and having a really robust system. A harder piece of engineering often is to think about the failure cases and really actively manage those well.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: I had a conversation with a contributor recently where it became apparent that contributor had never worked on a support team, because their conception of a badly behaved user was, like, a hacker, right? An attacker who comes from outside.&lt;/p&gt;

&lt;p&gt;And I&amp;rsquo;m like, no, no, no. You&amp;rsquo;re stable of badly behaved users is your own staff. You know, they will do bad things, not necessarily intending to do bad things, but because they&amp;rsquo;re trying to take a shortcut. And that is actually your primary concern in terms of preventing breaking the system.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Josh, what was your preparation to be release manager for 1.11?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: I was on the release team for two cycles, plus I was kind of auditing the release team for half a cycle before that. So in 1.9, I originally joined to be the shadow for bug triage, except I ended up not being the shadow, because the person who was supposed to be the lead for bug triage then dropped out. Then I ended up being the bug triage lead, and had to kind of improvise it because there wasn&amp;rsquo;t documentation on what was involved in the role at the time.&lt;/p&gt;

&lt;p&gt;And then I was &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/README.md#bug-triage-lead&#34; target=&#34;_blank&#34;&gt;bug triage lead&lt;/a&gt; for a second cycle, for the 1.10 cycle, and then took over as release lead for the cycle. And one of the things on my to-do list is to update the requirements to be release lead, because we actually do have written requirements, and to say that the expectation now is that you spend at least two cycles on the release team, one of them either as a lead or as a shadow to the release lead.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: And is bug triage lead just what it sounds like?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Yeah. Pretty much. There&amp;rsquo;s more tracking involved than triage. Part of it is just deficiencies in tooling, something we&amp;rsquo;re looking to address. But things like GitHub API limitations make it challenging to build automated tools that help us intelligently track issues. And we are actually working with GitHub on that. Like, they&amp;rsquo;ve been helpful. It&amp;rsquo;s just, they have their own scaling problems.&lt;/p&gt;

&lt;p&gt;But then beyond that, you know, a lot of that, it&amp;rsquo;s what you would expect it to be in terms of what triage says, right? Which is looking at every issue and saying, first of all, is this a real issue? Second, is it a serious issue? Third, who needs to address this?&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s a lot of the work, because for anybody who is a regular contributor to Kubernetes, the number of GitHub notifications that they receive per day means that most of us turn our GitHub notifications off.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Indeed.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Because it&amp;rsquo;s just this fire hose. And as a result, when somebody really needs to pay attention to something right now, that generally requires a human to go and track them down by email or Slack or whatever they prefer. Twitter in some cases. I&amp;rsquo;ve done that. And say, hey. We really need you to look at this issue, because it&amp;rsquo;s about to hold up the beta release.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: When you look at the process that you&amp;rsquo;re doing now, what are the changes that are coming in the future that will make the release process even better and easier?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Well, we just went through this whole retro, and I put in some recommendations for things. Obviously, some additional automation, which I&amp;rsquo;m going to be looking at doing now that I&amp;rsquo;m cycling off of the release team for a quarter and can actually look at more longer term goals, will help, particularly now that we&amp;rsquo;ve addressed actually some of our GitHub data flow issues.&lt;/p&gt;

&lt;p&gt;Beyond that, I put in a whole bunch of recommendations in the retro, but it&amp;rsquo;s actually up to Tim which recommendations he&amp;rsquo;s going to try to implement. So I&amp;rsquo;ll let him [comment].&lt;/p&gt;

&lt;p&gt;TIM PEPPER: I think one of the biggest changes that happened in the 1.11 cycle is this emphasis on trying to keep our continuous integration test status always green. That is huge for software development and keeping velocity. If you have this more, I guess at this point antiquated notion of waterfall development, where you do feature development for a while and are accepting of destabilization, and somehow later you&amp;rsquo;re going to come back and spend a period on stabilization and fixing, that really elongates the feedback loop for developers.&lt;/p&gt;

&lt;p&gt;And they don&amp;rsquo;t realize what was broken, and the problems become much more complex to sort out as time goes by. One, developers aren&amp;rsquo;t thinking about what it was that they&amp;rsquo;d been working on anymore. They&amp;rsquo;ve lost the context to be able to efficiently solve the problem.&lt;/p&gt;

&lt;p&gt;But then you start also getting interactions. Maybe a bug was introduced, and other people started working around it or depending on it, and you get complex dependencies then that are harder to fix. And when you&amp;rsquo;re trying to do that type of complex resolution late in the cycle, it becomes untenable over time. So I think continuing on that and building on it, I&amp;rsquo;m seeing a little bit more focus on test cases and meaningful test coverage. I think that&amp;rsquo;s a great cultural change to have happening.&lt;/p&gt;

&lt;p&gt;And maybe because I&amp;rsquo;m following Josh into this role from a bug triage position and in his mentions earlier of just the communications and tracking involved with that versus triage, I do have a bit of a concern that at times, email and Slack are relatively quiet. Some of the SIG meeting notes are a bit sparse or YouTube videos slow to upload. So the general artifacts around choice making I think is an area where we need a little more rigor. So I&amp;rsquo;m hoping to see some of that.&lt;/p&gt;

&lt;p&gt;And that can be just as subtle as commenting on issues like, hey, this commit doesn&amp;rsquo;t say what it&amp;rsquo;s doing. And for that reason on the release team, we can&amp;rsquo;t assess its risk versus value. So could you give a little more information here? Things like that give more information both to the release team and the development community as well, because this is open source. And to collaborate, you really do need to communicate in depth.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Speaking of cultural changes, professional baker to Kubernetes&amp;rsquo; release lead sounds like quite a journey.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: There was a lot of stuff in between.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Would you say there are a lot of similarities?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: You know, believe it or not, there actually are similarities. And here&amp;rsquo;s where it&amp;rsquo;s similar, because I was actually thinking about this earlier. So when I was a professional baker, one of the things that I had to do was morning pastry. Like, I was actually in charge of doing several other things for custom orders, but since I had to come to work at 3:00 AM anyway— which also distressingly has similarities with some of this process. Because I had to come to work at 3:00 AM anyway, one of my secondary responsibilities was traying the morning pastry.&lt;/p&gt;

&lt;p&gt;And one of the parts of that is you have this great big gas-fired oven with 10 rotating racks in it that are constantly rotating. Like, you get things in and out in the oven by popping them in and out while the racks are moving. That takes a certain amount of skill. You get burn marks on your wrists for your first couple of weeks of work. And then different pastries require a certain number of rotations to be done.&lt;/p&gt;

&lt;p&gt;And there&amp;rsquo;s a lot of similarities to the release cadence, because what you&amp;rsquo;re doing is you&amp;rsquo;re popping something in the oven or you&amp;rsquo;re seeing something get kicked off, and then you have a certain amount of time before you need to check on it or you need to pull it out. And you&amp;rsquo;re doing that in parallel with a whole bunch of other things. You know, with 40 other trays.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: And with presumably a bunch of colleagues who are all there at the same time.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Yeah. And the other thing is that these deadlines are kind of absolute, right? You can&amp;rsquo;t say, oh, well, I was reading a magazine article, and I didn&amp;rsquo;t have time to pull that tray out. It&amp;rsquo;s too late. The pastry is burned, and you&amp;rsquo;re going to have to throw it away, and they&amp;rsquo;re not going to have enough pastry in the front case for the morning rush. And the customers are not interested in your excuses for that.&lt;/p&gt;

&lt;p&gt;So from that perspective, from the perspective of saying, hey, we have a bunch of things that need to happen in parallel, they have deadlines and those deadlines are hard deadlines, there it&amp;rsquo;s actually fairly similar.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Tim, do you have any other history that helped get you to where you are today?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;TIM PEPPER: I think in some ways I&amp;rsquo;m more of a traditional journey. I&amp;rsquo;ve got a computer engineering bachelor&amp;rsquo;s degree. But I&amp;rsquo;m also maybe a bit of an outlier. In the late &amp;lsquo;90s, I found a passion for open source and Linux. Maybe kind of an early adopter, early believer in that.&lt;/p&gt;

&lt;p&gt;And was working in the industry in the Bay Area for a while. Got involved in the Silicon Valley and Bay Area Linux users groups a bit, and managed to find work as a Linux sysadmin, and then doing device driver and kernel work and on up into distro. So that was all kind of standard in a way. And then I also did some other work around hardware enablement, high-performance computing, non-uniform memory access. Things that are really, really systems work.&lt;/p&gt;

&lt;p&gt;And then about three years ago, my boss was really bending my ear and trying to get me to work on this cloud-related project. And that just felt so abstract and different from the low-level bits type of stuff that I&amp;rsquo;d been doing.&lt;/p&gt;

&lt;p&gt;But kind of grudgingly, I eventually came around to the realization that the cloud is interesting, and it&amp;rsquo;s so much more complex than local machine-only systems work, the type of things that I&amp;rsquo;d been doing before. It&amp;rsquo;s massively distributed and you have a high-latency, low-reliability interconnect on all the nodes in the distributed network. So it&amp;rsquo;s wildly complex engineering problems that need solved.&lt;/p&gt;

&lt;p&gt;And so that got me interested. Started working then on this open source orchestrator for virtual machines and containers. It was written in Go and was having a lot of fun. But it wasn&amp;rsquo;t Kubernetes, and it was becoming clear that Kubernetes was taking off. So about a year ago, I made the deliberate choice to move over to Kubernetes work.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Previously, Josh, you spoke a little bit about your preparation for becoming a release manager. For other folks that are interested in getting involved in the community and maybe getting involved in release management, should they follow the same path that you did? Or what are ways that would be good for them to get involved? And for you, Tim, how you&amp;rsquo;ve approached the preparation for taking on the next release.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: The great thing with the release team is that we have this formal mentorship path. And it&amp;rsquo;s fast, right? That&amp;rsquo;s the advantage of releasing quarterly, right? Is that within six months, you can go from joining the team as a shadow to being the release lead if you have the time. And you know, by the time you work your way up to release time, you better have support from your boss about this, because you&amp;rsquo;re going to end up spending a majority of your work time towards the end of the release on release management.&lt;/p&gt;

&lt;p&gt;So the answer is to sign up to look when we&amp;rsquo;re getting into the latter half of release cycle, to sign up as a shadow. Or at the beginning of a release cycle, to sign up as a shadow. Some positions actually can reasonably use more than one shadow. There&amp;rsquo;s some position that just require a whole ton of legwork like release notes. And as a result, could actually use more than one shadow meaningfully. So there&amp;rsquo;s probably still places where people could sign up for 1.12. Is that true, Tim?&lt;/p&gt;

&lt;p&gt;TIM PEPPER: Definitely. I think— gosh, right now we have 34 volunteers on the release team, which is—&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ADAM GLICK: Wow.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: OK. OK. Maybe not then.&lt;/p&gt;

&lt;p&gt;[LAUGH]&lt;/p&gt;

&lt;p&gt;TIM PEPPER: It&amp;rsquo;s potentially becoming a lot of cats to herd. But I think even outside of that formal volunteering to be a named shadow, anybody is welcome to show up to the release team meetings, follow the release team activities on &lt;a href=&#34;http://slack.k8s.io&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;, start understanding how the process works. And really, this is the case all across open source. It doesn&amp;rsquo;t even have to be the release team. If you&amp;rsquo;re passionate about networking, start following what SIG Network is doing. It&amp;rsquo;s the same sort of path, I think, into any area on the project.&lt;/p&gt;

&lt;p&gt;Each of the SIGs [has] a channel. So it would be #SIG-whatever the name is. [In our] case, #SIG-Release.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d also maybe give a plug for a &lt;a href=&#34;https://youtu.be/goAph8A20gQ&#34; target=&#34;_blank&#34;&gt;talk I did at KubeCon&lt;/a&gt; in Copenhagen this spring, talking about how the release team specifically can be a path for new contributors coming in. And had some ideas and suggestions there for newcomers.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: There&amp;rsquo;s three questions in the Google SRE postmortem template that I really like. And I&amp;rsquo;m sure you will have gone through these in the retrospective process as you released 1.11, so I&amp;rsquo;d like to ask them now one at a time.&lt;/p&gt;

&lt;p&gt;First of all, what went well?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Two things, I think, really improved things, both for contributors and for the release team. Thing number one was putting a strong emphasis on getting the test grid green well ahead of code freeze.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: Definitely.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Now partly that went well because we had a spectacular CI lead, &lt;a href=&#34;https://github.com/aishsundar&#34; target=&#34;_blank&#34;&gt;Aish Sundar&lt;/a&gt;, who&amp;rsquo;s now in training to become the release lead.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: And I&amp;rsquo;d count that partly as one of the &amp;ldquo;Where were you lucky?&amp;rdquo; areas. We happened upon a wonderful person who just popped up and volunteered.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: Yes. And then but part of that was also that we said, hey. You know, we&amp;rsquo;re not going to do what we&amp;rsquo;ve done before which is not really care about these tests until code slush. We&amp;rsquo;re going to care about these tests now.&lt;/p&gt;

&lt;p&gt;And importantly— this is really important to the Kubernetes community— when we went to the various SIGs, the SIG Cluster Lifecycle and SIG Scalability and SIG Node and the other ones who were having test failures, and we said this to them. They didn&amp;rsquo;t say, get lost. I&amp;rsquo;m busy. They said, what&amp;rsquo;s failing?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: Great.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: And so that made a big difference. And the second thing that was pretty much allowed by the first thing was to shorten the code freeze period. Because the code freeze period is frustrating for developers, because if they don&amp;rsquo;t happen to be working on a 1.11 feature, even if they worked on one before, and they delivered it early in the cycle, and it&amp;rsquo;s completely done, they&amp;rsquo;re kind of paralyzed, and they can&amp;rsquo;t do anything during code freeze. And so it&amp;rsquo;s very frustrating for them, and we want to make that period as short as possible. And we did that this time, and I think it helped everybody.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: What went poorly?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: We had a lot of problems with flaky tests. We have a lot of old tests that are not all that well maintained, and they&amp;rsquo;re testing very complicated things like upgrading a cluster that has 40 nodes. And as a result, these tests have high failure rates that have very little to do with any change in the code.&lt;/p&gt;

&lt;p&gt;And so one of the things that happened, and the reason we had a one-day delay in the release is, you know, we&amp;rsquo;re a week out from release, and just by random luck of the draw, a bunch of these tests all at once got a run of failures. And it turned out that run of failures didn&amp;rsquo;t actually mean anything, having anything to do with Kubernetes. But there was no way for us to tell that without a lot of research, and we were not going to have enough time for that research without delaying the release.&lt;/p&gt;

&lt;p&gt;So one of the things we&amp;rsquo;re looking to address in the 1.12 cycle is to actually move some of those flaky tests out. Either fix them or move them out of the release blocking category.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: In a way, I think this also highlights one of the things that Josh mentioned that went well, the emphasis early on getting the test results green, it allows us to see the extent to which these flakes are such a problem. And then the unlucky occurrence of them all happening to overlap on a failure, again, highlights that these flakes have been called out in the community for quite some time. I mean, at least a year. I know one contributor who was really concerned about them.&lt;/p&gt;

&lt;p&gt;But they became a second order concern versus just getting things done in the short term, getting features and proving that the features worked, and kind of accepting in a risk management way on the release team that, yes, those are flakes. We don&amp;rsquo;t have time to do something about them, and it&amp;rsquo;s OK. But because of the emphasis on keeping the test always green now, we have the luxury maybe to focus on improving these flakes, and really get to where we have truly high quality CI signal, and can really believe in the results that we have on an ongoing basis.&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: And having solved some of the more basic problems, we&amp;rsquo;re now seeing some of the other problems like coordination between related features. Like we right now have a feature where— and this is one of the sort of backwards compatibility release notes— where the feature went into beta, and is on by default.&lt;/p&gt;

&lt;p&gt;And the second feature that was supposed to provide access control for the first feature did not go in as beta, and is not on by default. And the team for the first feature did not realize the second feature was being held up until two days before the release. So it&amp;rsquo;s going to result in us actually patching something in 11.1.&lt;/p&gt;

&lt;p&gt;And so like, we put that into something that didn&amp;rsquo;t go well. But on the other hand, as Tim points out, a few release cycles ago, we wouldn&amp;rsquo;t even have identified that as a problem, because we were still struggling with just individual features having a clear idea well ahead of the release of what was going in and what wasn&amp;rsquo;t going in.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: I think something like this also is a case that maybe advocates for the use of feature branches. If these things are related, we might have seen it and done more pre-testing within that branch and pre-integration, and decide maybe to merge a couple of what initially had been disjoint features into a single feature branch, and really convince ourselves that together they were good. And cross all the Ts, dot all the Is on them, and not have something that&amp;rsquo;s gated on an alpha feature that&amp;rsquo;s possibly falling away.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;CRAIG BOX: And then the final question, which I think you&amp;rsquo;ve both touched on a little. Where did you get lucky, or unlucky perhaps?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;JOSH BERKUS: I would say number one where I got lucky is truly having a fantastic team. I mean, we just had a lot of terrific people who were very good and very energetic and very enthusiastic about taking on their release responsibilities including Aish and Tim and Ben and Nick and Misty who took over Docs four weeks into the release. And then went crazy with it and said, well, I&amp;rsquo;m new here, so I&amp;rsquo;m going to actually change a bunch of things we&amp;rsquo;ve been doing that didn&amp;rsquo;t work in the first place. So that was number one. I mean, that really made honestly all the difference.&lt;/p&gt;

&lt;p&gt;And then the second thing, like I said, is that we didn&amp;rsquo;t have sort of major, unexpected monkey wrenches thrown at us. So in the 1.10 cycle, we actually had two of those, which is why I still count Jace as heroic for pulling off a release that was only a week late.&lt;/p&gt;

&lt;p&gt;You know, number one was having the scalability tests start failing for unrelated reasons for a long period, which then masked the fact that they were actually failing for real reasons when we actually got them working again. And as a result, ending up debugging a major and super complicated scalability issue within days of what was supposed to be the original release date. So that was monkey wrench number one for the 1.10 cycle.&lt;/p&gt;

&lt;p&gt;Monkey wrench number two for the 1.10 cycle was we got a security hole that needed to be patched. And so again, a week out from what was supposed to be the original release date, we were releasing a security update, and that security update required patching the release branch. And it turns out that patch against the release branch broke a bunch of incoming features. And we didn&amp;rsquo;t get anything of that magnitude in the 1.11 release, and I&amp;rsquo;m thankful for that.&lt;/p&gt;

&lt;p&gt;TIM PEPPER: Also, I would maybe argue in a way that a portion of that wasn&amp;rsquo;t just luck. The extent to which this community has a good team, not just the release team but beyond, some of this goes to active work that folks all across the project, but especially in the contributor experience SIG are doing to cultivate a positive and inclusive culture here. And you really see that. When problems crop up, you&amp;rsquo;re seeing people jump on and really try to constructively tackle them. And it&amp;rsquo;s really fun to be a part of that.&lt;/p&gt;

&lt;hr/&gt;

&lt;p&gt;&lt;i&gt;Thanks to Josh Berkus and Tim Pepper for talking to the Kubernetes Podcast from Google.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/jberkus&#34; target=&#34;_blank&#34;&gt;Josh Berkus&lt;/a&gt; hangs out in #sig-release on the &lt;a href=&#34;slack.k8s.io&#34; target=&#34;_blank&#34;&gt;Kubernetes Slack&lt;/a&gt;. He maintains a newsletter called &amp;ldquo;&lt;a href=&#34;http://lwkd.info/&#34; target=&#34;_blank&#34;&gt;Last Week in Kubernetes Development&lt;/a&gt;&amp;rdquo;, with Noah Kantrowitz. You can read him on Twitter at &lt;a href=&#34;https://twitter.com/fuzzychef&#34; target=&#34;_blank&#34;&gt;@fuzzychef&lt;/a&gt;, but he does warn you that there&amp;rsquo;s a lot of politics there as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/tpepper&#34; target=&#34;_blank&#34;&gt;Tim Pepper&lt;/a&gt; is also on Slack - he&amp;rsquo;s always open to folks reaching out with a question, looking for help or advice.  On Twitter you&amp;rsquo;ll find him at &lt;a href=&#34;https://twitter.com/pythomit&#34; target=&#34;_blank&#34;&gt;@pythomit&lt;/a&gt;, which is &amp;ldquo;Timothy P&amp;rdquo; backwards. Tim is an avid soccer fan and season ticket holder for the &lt;a href=&#34;https://portlandtimbers.com/&#34; target=&#34;_blank&#34;&gt;Portland Timbers&lt;/a&gt; and the &lt;a href=&#34;https://portlandthorns.com/&#34; target=&#34;_blank&#34;&gt;Portland Thorns&lt;/a&gt;, so you&amp;rsquo;ll get all sorts of opinions on soccer in addition to technology!&lt;/p&gt;

&lt;p&gt;You can find the &lt;a href=&#34;http://www.kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; at &lt;a href=&#34;https://twitter.com/KubernetesPod&#34; target=&#34;_blank&#34;&gt;@kubernetespod&lt;/a&gt; on Twitter, and you can &lt;a href=&#34;https://kubernetespodcast.com/subscribe/&#34; target=&#34;_blank&#34;&gt;subscribe&lt;/a&gt; so you never miss an episode.&lt;/i&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Resizing Persistent Volumes using Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/</link>
      <pubDate>Thu, 12 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Hemant Kumar (Red Hat)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what’s new in Kubernetes 1.11&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Kubernetes v1.11 the persistent volume expansion feature is being promoted to beta. This feature allows users to easily resize an existing volume by editing the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; (PVC) object. Users no longer have to manually interact with the storage backend or delete and recreate PV and PVC objects to increase the size of a volume. Shrinking persistent volumes is not supported.&lt;/p&gt;

&lt;p&gt;Volume expansion was introduced in v1.8 as an Alpha feature, and versions prior to v1.11 required enabling the feature gate, &lt;code&gt;ExpandPersistentVolumes&lt;/code&gt;, as well as the admission controller, &lt;code&gt;PersistentVolumeClaimResize&lt;/code&gt; (which prevents expansion of PVCs whose underlying storage provider does not support resizing). In Kubernetes v1.11+, both the feature gate and admission controller are enabled by default.&lt;/p&gt;

&lt;p&gt;Although the feature is enabled by default, a cluster admin must opt-in to allow users to resize their volumes. Kubernetes v1.11 ships with volume expansion support for the following in-tree volume plugins: AWS-EBS, GCE-PD, Azure Disk, Azure File, Glusterfs, Cinder, Portworx, and Ceph RBD. Once the admin has determined that volume expansion is supported for the underlying provider, they can make the feature available to users by setting the &lt;code&gt;allowVolumeExpansion&lt;/code&gt; field to &lt;code&gt;true&lt;/code&gt; in their &lt;code&gt;StorageClass&lt;/code&gt; object(s). Only PVCs created from that &lt;code&gt;StorageClass&lt;/code&gt; will be allowed to trigger volume expansion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; cat standard.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
parameters:
  type: pd-standard
provisioner: kubernetes.io/gce-pd
allowVolumeExpansion: true
reclaimPolicy: Delete
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any PVC created from this &lt;code&gt;StorageClass&lt;/code&gt; can be edited (as illustrated below) to request more space. Kubernetes will interpret a change to the storage field as a request for more space, and will trigger automatic volume resizing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-12-resizing-persistent-volumes-using-kubernetes/pvc-storageclass.png&#34; alt=&#34;PVC StorageClass&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;file-system-expansion&#34;&gt;File System Expansion&lt;/h2&gt;

&lt;p&gt;Block storage volume types such as GCE-PD, AWS-EBS, Azure Disk, Cinder, and Ceph RBD typically require a file system expansion before the additional space of an expanded volume is usable by pods. Kubernetes takes care of this automatically whenever the pod(s) referencing your volume are restarted.&lt;/p&gt;

&lt;p&gt;Network attached file systems (like Glusterfs and Azure File) can be expanded without having to restart the referencing Pod, because these systems do not require special file system expansion.&lt;/p&gt;

&lt;p&gt;File system expansion must be triggered by terminating the pod using the volume. More specifically:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Edit the PVC to request more space.&lt;/li&gt;
&lt;li&gt;Once underlying volume has been expanded by the storage provider, then the PersistentVolume object will reflect the updated size and the PVC will have the &lt;code&gt;FileSystemResizePending&lt;/code&gt; condition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can verify this by running &lt;code&gt;kubectl get pvc &amp;lt;pvc_name&amp;gt; -o yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~&amp;gt; kubectl get pvc myclaim -o yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
  namespace: default
  uid: 02d4aa83-83cd-11e8-909d-42010af00004
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 14Gi
  storageClassName: standard
  volumeName: pvc-xxx
status:
  capacity:
    storage: 9G
  conditions:
  - lastProbeTime: null
    lastTransitionTime: 2018-07-11T14:51:10Z
    message: Waiting for user to (re-)start a pod to finish file system resize of
      volume on node.
    status: &amp;quot;True&amp;quot;
    type: FileSystemResizePending
  phase: Bound
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Once the PVC has the condition &lt;code&gt;FileSystemResizePending&lt;/code&gt; then pod that uses the PVC can be restarted to finish file system resizing on the node. Restart can be achieved by deleting and recreating the pod or by scaling down the deployment and then scaling it up again.&lt;/li&gt;
&lt;li&gt;Once file system resizing is done, the PVC will automatically be updated to reflect new size.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Any errors encountered while expanding file system should be available as events on pod.&lt;/p&gt;

&lt;h2 id=&#34;online-file-system-expansion&#34;&gt;Online File System Expansion&lt;/h2&gt;

&lt;p&gt;Kubernetes v1.11 also introduces an alpha feature called online file system expansion. This feature enables file system expansion while a volume is still in-use by a pod. Because this feature is alpha, it requires enabling the feature gate, &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt;. It is supported by the in-tree volume plugins GCE-PD, AWS-EBS, Cinder, and Ceph RBD. When this feature is enabled, pod referencing the resized volume do not need to be restarted. Instead, the file system will automatically be resized while in use as part of volume expansion. File system expansion does not happen until a pod references the resized volume, so if no pods referencing the volume are running file system expansion will not happen.&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Check out additional documentation on this feature here: &lt;a href=&#34;http://k8s.io/docs/concepts/storage/persistent-volumes&#34; target=&#34;_blank&#34;&gt;http://k8s.io/docs/concepts/storage/persistent-volumes&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Dynamic Kubelet Configuration</title>
      <link>https://docstest.github.io/blog/2018/07/11/dynamic-kubelet-configuration/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/11/dynamic-kubelet-configuration/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Michael Taufen (Google)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what’s new in Kubernetes 1.11&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-dynamic-kubelet-configuration&#34;&gt;Why Dynamic Kubelet Configuration?&lt;/h2&gt;

&lt;p&gt;Kubernetes provides API-centric tooling that significantly improves workflows for managing applications and infrastructure. Most Kubernetes installations, however, run the Kubelet as a native process on each host, outside the scope of standard Kubernetes APIs.&lt;/p&gt;

&lt;p&gt;In the past, this meant that cluster administrators and service providers could not rely on Kubernetes APIs to reconfigure Kubelets in a live cluster. In practice, this required operators to either ssh into machines to perform manual reconfigurations, use third-party configuration management automation tools, or create new VMs with the desired configuration already installed, then migrate work to the new machines. These approaches are environment-specific and can be expensive.&lt;/p&gt;

&lt;p&gt;Dynamic Kubelet configuration gives cluster administrators and service providers the ability to reconfigure Kubelets in a live cluster via Kubernetes APIs.&lt;/p&gt;

&lt;h2 id=&#34;what-is-dynamic-kubelet-configuration&#34;&gt;What is Dynamic Kubelet Configuration?&lt;/h2&gt;

&lt;p&gt;Kubernetes v1.10 made it possible to configure the Kubelet via a beta &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/&#34; target=&#34;_blank&#34;&gt;config file&lt;/a&gt; API. Kubernetes already provides the ConfigMap abstraction for storing arbitrary file data in the API server.&lt;/p&gt;

&lt;p&gt;Dynamic Kubelet configuration extends the Node object so that a Node can refer to a ConfigMap that contains the same type of config file. When a Node is updated to refer to a new ConfigMap, the associated Kubelet will attempt to use the new configuration.&lt;/p&gt;

&lt;h2 id=&#34;how-does-it-work&#34;&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;Dynamic Kubelet configuration provides the following core features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubelet attempts to use the dynamically assigned configuration.&lt;/li&gt;
&lt;li&gt;Kubelet &amp;ldquo;checkpoints&amp;rdquo; configuration to local disk, enabling restarts without API server access.&lt;/li&gt;
&lt;li&gt;Kubelet reports assigned, active, and last-known-good configuration sources in the Node status.&lt;/li&gt;
&lt;li&gt;When invalid configuration is dynamically assigned, Kubelet automatically falls back to a last-known-good configuration and reports errors in the Node status.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To use the dynamic Kubelet configuration feature, a cluster administrator or service provider will first post a ConfigMap containing the desired configuration, then set each Node.Spec.ConfigSource.ConfigMap reference to refer to the new ConfigMap. Operators can update these references at their preferred rate, giving them the ability to perform controlled rollouts of new configurations.&lt;/p&gt;

&lt;p&gt;Each Kubelet watches its associated Node object for changes. When the Node.Spec.ConfigSource.ConfigMap reference is updated, the Kubelet will &amp;ldquo;checkpoint&amp;rdquo; the new ConfigMap by writing the files it contains to local disk. The Kubelet will then exit, and the OS-level process manager will restart it. Note that if the Node.Spec.ConfigSource.ConfigMap reference is not set, the Kubelet uses the set of flags and config files local to the machine it is running on.&lt;/p&gt;

&lt;p&gt;Once restarted, the Kubelet will attempt to use the configuration from the new checkpoint. If the new configuration passes the Kubelet&amp;rsquo;s internal validation, the Kubelet will update Node.Status.Config to reflect that it is using the new configuration. If the new configuration is invalid, the Kubelet will fall back to its last-known-good configuration and report an error in Node.Status.Config.&lt;/p&gt;

&lt;p&gt;Note that the default last-known-good configuration is the combination of Kubelet command-line flags with the Kubelet&amp;rsquo;s local configuration file. Command-line flags that overlap with the config file always take precedence over both the local configuration file and dynamic configurations, for backwards-compatibility.&lt;/p&gt;

&lt;p&gt;See the following diagram for a high-level overview of a configuration update for a single Node:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-07-11-dynamic-kubelet-configuration/kubelet-diagram.png&#34; alt=&#34;kubelet-diagram&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;how-can-i-learn-more&#34;&gt;How can I learn more?&lt;/h2&gt;

&lt;p&gt;Please see the official tutorial at &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&#34; target=&#34;_blank&#34;&gt;https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&lt;/a&gt;, which contains more in-depth details on user workflow, how a configuration becomes &amp;ldquo;last-known-good,&amp;rdquo; how the Kubelet &amp;ldquo;checkpoints&amp;rdquo; config, and possible failure modes.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: CoreDNS GA for Kubernetes Cluster DNS</title>
      <link>https://docstest.github.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: John Belamaric (Infoblox)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what’s new in Kubernetes 1.11&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In Kubernetes 1.11, &lt;a href=&#34;https://coredns.io&#34; target=&#34;_blank&#34;&gt;CoreDNS&lt;/a&gt; has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.&lt;/p&gt;

&lt;p&gt;DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.&lt;/p&gt;

&lt;p&gt;CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.&lt;/p&gt;

&lt;p&gt;In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.&lt;/p&gt;

&lt;h2 id=&#34;implementation-differences&#34;&gt;Implementation differences&lt;/h2&gt;

&lt;p&gt;In kube-dns, several containers are used within a single pod: &lt;code&gt;kubedns&lt;/code&gt;, &lt;code&gt;dnsmasq&lt;/code&gt;, and &lt;code&gt;sidecar&lt;/code&gt;. The &lt;code&gt;kubedns&lt;/code&gt;
container watches the Kubernetes API and serves DNS records based on the &lt;a href=&#34;https://github.com/kubernetes/dns/blob/master/docs/specification.md&#34; target=&#34;_blank&#34;&gt;Kubernetes DNS specification&lt;/a&gt;, &lt;code&gt;dnsmasq&lt;/code&gt; provides caching and stub domain support, and &lt;code&gt;sidecar&lt;/code&gt; provides metrics and health checks.&lt;/p&gt;

&lt;p&gt;This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in &lt;code&gt;dnsmasq&lt;/code&gt; have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because &lt;code&gt;dnsmasq&lt;/code&gt; handles the stub domains,
but &lt;code&gt;kubedns&lt;/code&gt; handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see &lt;a href=&#34;https://github.com/kubernetes/dns/issues/131&#34; target=&#34;_blank&#34;&gt;dns#131&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.&lt;/p&gt;

&lt;h2 id=&#34;configuring-coredns&#34;&gt;Configuring CoreDNS&lt;/h2&gt;

&lt;p&gt;In kube-dns, you can &lt;a href=&#34;https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/&#34; target=&#34;_blank&#34;&gt;modify a ConfigMap&lt;/a&gt; to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.&lt;/p&gt;

&lt;p&gt;In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS &lt;a href=&#34;https://coredns.io/2017/07/23/corefile-explained/&#34; target=&#34;_blank&#34;&gt;Corefile&lt;/a&gt; to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.&lt;/p&gt;

&lt;p&gt;When upgrading from kube-dns to CoreDNS using &lt;code&gt;kubeadm&lt;/code&gt;, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/coredns/&#34; target=&#34;_blank&#34;&gt;Using CoreDNS for Service Discovery&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&#34;bug-fixes-and-enhancements&#34;&gt;Bug fixes and enhancements&lt;/h2&gt;

&lt;p&gt;There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/55&#34; target=&#34;_blank&#34;&gt;dns#55 - Custom DNS entries for kube-dns&lt;/a&gt; may be handled using the &amp;ldquo;fallthrough&amp;rdquo; mechanism in the &lt;a href=&#34;https://coredns.io/plugins/kubernetes&#34; target=&#34;_blank&#34;&gt;kubernetes plugin&lt;/a&gt;, using the &lt;a href=&#34;https://coredns.io/plugins/rewrite&#34; target=&#34;_blank&#34;&gt;rewrite plugin&lt;/a&gt;, or simply serving a subzone with a different plugin such as the &lt;a href=&#34;https://coredns.io/plugins/file&#34; target=&#34;_blank&#34;&gt;file plugin&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/116&#34; target=&#34;_blank&#34;&gt;dns#116 - Only one A record set for headless service with pods having single hostname&lt;/a&gt;. This issue is fixed without any additional configuration.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/131&#34; target=&#34;_blank&#34;&gt;dns#131 - externalName not using stubDomains settings&lt;/a&gt;. This issue is fixed without any additional configuration.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/167&#34; target=&#34;_blank&#34;&gt;dns#167 - enable skyDNS round robin A/AAAA records&lt;/a&gt;. The equivalent functionality can be configured using the &lt;a href=&#34;https://coredns.io/plugins/loadbalance&#34; target=&#34;_blank&#34;&gt;load balance plugin&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/190&#34; target=&#34;_blank&#34;&gt;dns#190 - kube-dns cannot run as non-root user&lt;/a&gt;. This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/dns/issues/232&#34; target=&#34;_blank&#34;&gt;dns#232 - fix pod hostname to be podname for dns srv records&lt;/a&gt; is an enhancement that is supported through the &amp;ldquo;endpoint_pod_names&amp;rdquo; feature described below.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;metrics&#34;&gt;Metrics&lt;/h2&gt;

&lt;p&gt;The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for &lt;code&gt;dnsmasq&lt;/code&gt; and &lt;code&gt;kubedns&lt;/code&gt; (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS &lt;a href=&#34;https://coredns.io/plugins/metrics/&#34; target=&#34;_blank&#34;&gt;Prometheus plugin&lt;/a&gt; page.&lt;/p&gt;

&lt;h2 id=&#34;some-special-features&#34;&gt;Some special features&lt;/h2&gt;

&lt;p&gt;The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the &lt;a href=&#34;https://github.com/kubernetes/dns/blob/master/docs/specification.md&#34; target=&#34;_blank&#34;&gt;Kubernetes DNS specification&lt;/a&gt;;
they enhance functionality but remain backward compatible. Since CoreDNS is not
&lt;em&gt;only&lt;/em&gt; made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.&lt;/p&gt;

&lt;h3 id=&#34;pods-verified-mode&#34;&gt;Pods verified mode&lt;/h3&gt;

&lt;p&gt;In kube-dns, pod name records are &amp;ldquo;fake&amp;rdquo;. That is, any &amp;ldquo;a-b-c-d.namespace.pod.cluster.local&amp;rdquo; query will
return the IP address &amp;ldquo;a.b.c.d&amp;rdquo;. In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a &amp;ldquo;pods verified&amp;rdquo; mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.&lt;/p&gt;

&lt;h3 id=&#34;endpoint-names-based-on-pod-names&#34;&gt;Endpoint names based on pod names&lt;/h3&gt;

&lt;p&gt;In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/47992&#34; target=&#34;_blank&#34;&gt;kubernetes#47992&lt;/a&gt; and &lt;a href=&#34;https://github.com/coredns/coredns/pull/1190&#34; target=&#34;_blank&#34;&gt;coredns#1190&lt;/a&gt;). To enable this in CoreDNS, you specify the &amp;ldquo;endpoint_pod_names&amp;rdquo; option in your Corefile, which results in this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;autopath&#34;&gt;Autopath&lt;/h3&gt;

&lt;p&gt;CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, &amp;ldquo;headless&amp;rdquo; above, rather than &amp;ldquo;headless.default.svc.cluster.local&amp;rdquo;. However,
when requesting an external name  - &amp;ldquo;infoblox.com&amp;rdquo;, for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to &lt;code&gt;dnsmasq&lt;/code&gt; and then to &lt;code&gt;kubedns&lt;/code&gt;, since &lt;a href=&#34;https://github.com/kubernetes/dns/issues/121&#34; target=&#34;_blank&#34;&gt;negative caching is disabled&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;infoblox.com.default.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li&gt;
&lt;li&gt;infoblox.com.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li&gt;
&lt;li&gt;infoblox.com.cluster.local -&amp;gt; NXDOMAIN&lt;/li&gt;
&lt;li&gt;infoblox.com.your-internal-domain.com -&amp;gt; NXDOMAIN&lt;/li&gt;
&lt;li&gt;infoblox.com -&amp;gt; returns a valid record&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In CoreDNS, an optional feature called &lt;a href=&#34;https://coredns.io/plugins/autopath&#34; target=&#34;_blank&#34;&gt;autopath&lt;/a&gt; can be enabled that will cause this search path to be followed
&lt;em&gt;in the server&lt;/em&gt;. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.&lt;/p&gt;

&lt;h3 id=&#34;a-few-other-kubernetes-specific-features&#34;&gt;A few other Kubernetes specific features&lt;/h3&gt;

&lt;p&gt;In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.&lt;/p&gt;

&lt;p&gt;You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.&lt;/p&gt;

&lt;h2 id=&#34;extensibility&#34;&gt;Extensibility&lt;/h2&gt;

&lt;p&gt;In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the &lt;a href=&#34;https://https://coredns.io/explugins/unbound&#34; target=&#34;_blank&#34;&gt;unbound plugin&lt;/a&gt;, to server records directly from a database with the &lt;a href=&#34;https://coredns.io/explugins/pdsql&#34; target=&#34;_blank&#34;&gt;pdsql plugin&lt;/a&gt;, and to allow multiple CoreDNS instances to share a common level 2 cache with the &lt;a href=&#34;https://coredns.io/explugins/redisc&#34; target=&#34;_blank&#34;&gt;redisc plugin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Many other interesting extensions have been added, which you will find on the &lt;a href=&#34;https://coredns.io/explugins/&#34; target=&#34;_blank&#34;&gt;External Plugins&lt;/a&gt; page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the &lt;a href=&#34;https://coredns.io/explugins/kubernetai&#34; target=&#34;_blank&#34;&gt;kubernetai plugin&lt;/a&gt;, which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.&lt;/p&gt;

&lt;h2 id=&#34;what-s-next&#34;&gt;What&amp;rsquo;s Next?&lt;/h2&gt;

&lt;p&gt;CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!&lt;/p&gt;

&lt;p&gt;The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the &lt;a href=&#34;https://coredns.io/blog&#34; target=&#34;_blank&#34;&gt;CoreDNS Blog&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;get-involved-with-coredns&#34;&gt;Get involved with CoreDNS&lt;/h3&gt;

&lt;p&gt;CoreDNS is an incubated &lt;a href=&#34;https:://cncf.io&#34; target=&#34;_blank&#34;&gt;CNCF&lt;/a&gt; project.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re most active on Slack (and Github):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Slack: #coredns on &lt;a href=&#34;https://slack.cncf.io&#34; target=&#34;_blank&#34;&gt;https://slack.cncf.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&#34;https://github.com/coredns/coredns&#34; target=&#34;_blank&#34;&gt;https://github.com/coredns/coredns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More resources can be found:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Website: &lt;a href=&#34;https://coredns.io&#34; target=&#34;_blank&#34;&gt;https://coredns.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Blog: &lt;a href=&#34;https://blog.coredns.io&#34; target=&#34;_blank&#34;&gt;https://blog.coredns.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/corednsio&#34; target=&#34;_blank&#34;&gt;@corednsio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mailing list/group: &lt;a href=&#34;mailto:coredns-discuss@googlegroups.com&#34; target=&#34;_blank&#34;&gt;coredns-discuss@googlegroups.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Meet Our Contributors - Monthly Streaming YouTube Mentoring Series</title>
      <link>https://docstest.github.io/blog/2018/07/10/meet-our-contributors-monthly-streaming-youtube-mentoring-series/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/10/meet-our-contributors-monthly-streaming-youtube-mentoring-series/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Paris Pittman (Google)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-05-meet-our-contributors-youtube-mentoring-series/meet-our-contributors.png&#34; alt=&#34;meet_our_contributors&#34; /&gt;&lt;/p&gt;

&lt;p&gt;July 11th at 2:30pm and 8pm UTC kicks off our next installment of Meet Our Contributors YouTube series. This month is special: members of the steering committee will be on to answer any and all questions from the community on the first 30 minutes of the 8pm UTC session. More on submitting questions below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/mentoring/meet-our-contributors.md&#34; target=&#34;_blank&#34;&gt;Meet Our Contributors&lt;/a&gt; was created to give an opportunity to new and current contributors alike to get time in front of our upstream community to ask questions that you would typically ask a mentor. We have 3-6 contributors on each session (an AM and PM session depending on where you are in the world!) answer questions &lt;a href=&#34;https://www.youtube.com/c/KubernetesCommunity/live&#34; target=&#34;_blank&#34;&gt;live on a YouTube stream&lt;/a&gt;. If you miss it, don’t stress, the recording is up after it’s over. Check out a past episode &lt;a href=&#34;https://www.youtube.com/watch?v=EVsXi3Zhlo0&amp;amp;list=PL69nYSiGNLP3QpQrhZq_sLYo77BVKv09F&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As you can imagine, the questions span broadly from introductory - “what’s a SIG?” to more advanced - “why’s my test flaking?” You’ll also hear growth related advice questions such as “what’s my best path to becoming an approver?” We’re happy to do a live code/docs review or explain part of the codebase as long as we have a few days notice.&lt;/p&gt;

&lt;p&gt;We answer at least 10 questions per session and have helped 500+ people to date. This is a scalable mentoring initiative that makes it easy for all parties to share information, get advice, and get going with what they are trying to accomplish. We encourage you to submit questions for our next session:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Join the Kubernetes Slack channel - #meet-our-contributors - to ask your question or for more detailed information. DM paris@ if you would like to remain anonymous.&lt;/li&gt;
&lt;li&gt;Twitter works, too, with the hashtag #k8smoc&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are contributor reading this that has wanted to mentor but just can’t find the time - this is for you! &lt;a href=&#34;https://goo.gl/forms/ZcnFiqNR5EQH03zm2&#34; target=&#34;_blank&#34;&gt;Reach out to us&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can join us live on June 6th at 2:30pm and 8pm UTC, and every first Wednesday of the month, on the &lt;a href=&#34;https://www.youtube.com/c/KubernetesCommunity/live&#34; target=&#34;_blank&#34;&gt;Kubernetes Community live stream&lt;/a&gt;. We look forward to seeing you there!&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title>
      <link>https://docstest.github.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Editor’s note: this post is part of a &lt;a href=&#34;https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/&#34; target=&#34;_blank&#34;&gt;series of in-depth articles&lt;/a&gt; on what’s new in Kubernetes 1.11&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Per &lt;a href=&#34;https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/&#34; target=&#34;_blank&#34;&gt;the Kubernetes 1.11 release blog post &lt;/a&gt;, we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.&lt;/p&gt;

&lt;h2 id=&#34;what-is-ipvs&#34;&gt;What Is IPVS?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;IPVS&lt;/strong&gt; (&lt;strong&gt;IP Virtual Server&lt;/strong&gt;) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.&lt;/p&gt;

&lt;p&gt;IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.&lt;/p&gt;

&lt;h2 id=&#34;why-ipvs-for-kubernetes&#34;&gt;Why IPVS for Kubernetes?&lt;/h2&gt;

&lt;p&gt;As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.&lt;/p&gt;

&lt;p&gt;Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.&lt;/p&gt;

&lt;p&gt;Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.&lt;/p&gt;

&lt;p&gt;On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.&lt;/p&gt;

&lt;h2 id=&#34;ipvs-based-kube-proxy&#34;&gt;IPVS-based Kube-proxy&lt;/h2&gt;

&lt;h3 id=&#34;parameter-changes&#34;&gt;Parameter Changes&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &amp;ndash;proxy-mode&lt;/strong&gt; In addition to existing userspace and iptables modes, IPVS mode is configured via &lt;code&gt;--proxy-mode=ipvs&lt;/code&gt;. It implicitly uses IPVS NAT mode for service port mapping.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &amp;ndash;ipvs-scheduler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being &lt;code&gt;--ipvs-scheduler&lt;/code&gt;. If it’s not configured, then round-robin (rr) is the default value.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rr: round-robin&lt;/li&gt;
&lt;li&gt;lc: least connection&lt;/li&gt;
&lt;li&gt;dh: destination hashing&lt;/li&gt;
&lt;li&gt;sh: source hashing&lt;/li&gt;
&lt;li&gt;sed: shortest expected delay&lt;/li&gt;
&lt;li&gt;nq: never queue&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &lt;code&gt;--cleanup-ipvs&lt;/code&gt;&lt;/strong&gt; Similar to the &lt;code&gt;--cleanup-iptables&lt;/code&gt; parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &lt;code&gt;--ipvs-sync-period&lt;/code&gt;&lt;/strong&gt; Maximum interval of how often IPVS rules are refreshed (e.g. &amp;lsquo;5s&amp;rsquo;, &amp;lsquo;1m&amp;rsquo;). Must be greater than 0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &lt;code&gt;--ipvs-min-sync-period&lt;/code&gt;&lt;/strong&gt; Minimum interval of how often the IPVS rules are refreshed (e.g. &amp;lsquo;5s&amp;rsquo;, &amp;lsquo;1m&amp;rsquo;). Must be greater than 0.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Parameter: &lt;code&gt;--ipvs-exclude-cidrs&lt;/code&gt;&lt;/strong&gt;  A comma-separated list of CIDR&amp;rsquo;s which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can&amp;rsquo;t distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.&lt;/p&gt;

&lt;h3 id=&#34;design-considerations&#34;&gt;Design Considerations&lt;/h3&gt;

&lt;h4 id=&#34;ipvs-service-network-topology&#34;&gt;IPVS Service Network Topology&lt;/h4&gt;

&lt;p&gt;When creating a ClusterIP type Service, IPVS proxier will do the following three things:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Make sure a dummy interface exists in the node, defaults to kube-ipvs0&lt;/li&gt;
&lt;li&gt;Bind Service IP addresses to the dummy interface&lt;/li&gt;
&lt;li&gt;Create IPVS virtual servers for each Service IP address respectively&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here comes an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
Type:           ClusterIP
IP:             10.102.128.4
Port:           http    3080/TCP
Endpoints:      10.244.0.235:8080,10.244.1.237:8080
Session Affinity:   None

# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.102.128.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn     
TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0         
  -&amp;gt; 10.244.1.237:8080            Masq    1      0          0   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please note that the relationship between a Kubernetes Service and IPVS virtual servers is &lt;code&gt;1:N&lt;/code&gt;. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is &lt;code&gt;1:1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.&lt;/p&gt;

&lt;h4 id=&#34;port-mapping&#34;&gt;Port Mapping&lt;/h4&gt;

&lt;p&gt;There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0         
  -&amp;gt; 10.244.1.237:8080            Masq    1      0       
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;session-affinity&#34;&gt;Session Affinity&lt;/h4&gt;

&lt;p&gt;IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
IP:             10.102.128.4
Port:           http    3080/TCP
Session Affinity:   ClientIP

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr persistent 10800
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;iptables-ipset-in-ipvs-proxier&#34;&gt;Iptables &amp;amp; Ipset in IPVS Proxier&lt;/h4&gt;

&lt;p&gt;IPVS is for load balancing and it can&amp;rsquo;t handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.&lt;/p&gt;

&lt;p&gt;IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy start with &amp;ndash;masquerade-all=true&lt;/li&gt;
&lt;li&gt;Specify cluster CIDR in kube-proxy startup&lt;/li&gt;
&lt;li&gt;Support Loadbalancer type service&lt;/li&gt;
&lt;li&gt;Support NodePort type service&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, we don&amp;rsquo;t want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;set name&lt;/th&gt;
&lt;th&gt;members&lt;/th&gt;
&lt;th&gt;usage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;KUBE-CLUSTER-IP&lt;/td&gt;
&lt;td&gt;All Service IP + port&lt;/td&gt;
&lt;td&gt;masquerade for cases that &lt;code&gt;masquerade-all=true&lt;/code&gt; or &lt;code&gt;clusterCIDR&lt;/code&gt; specified&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-LOOP-BACK&lt;/td&gt;
&lt;td&gt;All Service IP + port + IP&lt;/td&gt;
&lt;td&gt;masquerade for resolving hairpin issue&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-EXTERNAL-IP&lt;/td&gt;
&lt;td&gt;Service External IP + port&lt;/td&gt;
&lt;td&gt;masquerade for packets to external IPs&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-LOAD-BALANCER&lt;/td&gt;
&lt;td&gt;Load Balancer ingress IP + port&lt;/td&gt;
&lt;td&gt;masquerade for packets to Load Balancer type service&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-LOAD-BALANCER-LOCAL&lt;/td&gt;
&lt;td&gt;Load Balancer ingress IP + port with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;accept packets to Load Balancer with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-LOAD-BALANCER-FW&lt;/td&gt;
&lt;td&gt;Load Balancer ingress IP + port with &lt;code&gt;loadBalancerSourceRanges&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Drop packets for Load Balancer type Service with &lt;code&gt;loadBalancerSourceRanges&lt;/code&gt; specified&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-LOAD-BALANCER-SOURCE-CIDR&lt;/td&gt;
&lt;td&gt;Load Balancer ingress IP + port + source CIDR&lt;/td&gt;
&lt;td&gt;accept packets for Load Balancer type Service with &lt;code&gt;loadBalancerSourceRanges&lt;/code&gt; specified&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-NODE-PORT-TCP&lt;/td&gt;
&lt;td&gt;NodePort type Service TCP port&lt;/td&gt;
&lt;td&gt;masquerade for packets to NodePort(TCP)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-NODE-PORT-LOCAL-TCP&lt;/td&gt;
&lt;td&gt;NodePort type Service TCP port with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;accept packets to NodePort Service with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-NODE-PORT-UDP&lt;/td&gt;
&lt;td&gt;NodePort type Service UDP port&lt;/td&gt;
&lt;td&gt;masquerade for packets to NodePort(UDP)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;KUBE-NODE-PORT-LOCAL-UDP&lt;/td&gt;
&lt;td&gt;NodePort type service UDP port with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;accept packets to NodePort Service with &lt;code&gt;externalTrafficPolicy=local&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.&lt;/p&gt;

&lt;h3 id=&#34;run-kube-proxy-in-ipvs-mode&#34;&gt;Run kube-proxy in IPVS Mode&lt;/h3&gt;

&lt;p&gt;Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (&lt;code&gt;KUBE_PROXY_MODE=ipvs&lt;/code&gt;) or specifying flag (&lt;code&gt;--proxy-mode=ipvs&lt;/code&gt;). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, for Kubernetes v1.10, feature gate &lt;code&gt;SupportIPVSProxyMode&lt;/code&gt; is set to &lt;code&gt;true&lt;/code&gt; by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable &lt;code&gt;--feature-gates=SupportIPVSProxyMode=true&lt;/code&gt; explicitly for Kubernetes before v1.10.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.
Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;
Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;
Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates
Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;
Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Airflow on Kubernetes (Part 1): A Different Kind of Operator</title>
      <link>https://docstest.github.io/blog/2018/06/28/airflow-on-kubernetes-part-1-a-different-kind-of-operator/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/06/28/airflow-on-kubernetes-part-1-a-different-kind-of-operator/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Daniel Imberman (Bloomberg LP)&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As part of Bloomberg&amp;rsquo;s &lt;a href=&#34;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/&#34; target=&#34;_blank&#34;&gt;continued commitment to developing the Kubernetes ecosystem&lt;/a&gt;, we are excited to announce the Kubernetes Airflow Operator; a mechanism for &lt;a href=&#34;https://airflow.apache.org/&#34; target=&#34;_blank&#34;&gt;Apache Airflow&lt;/a&gt;, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.&lt;/p&gt;

&lt;h2 id=&#34;what-is-airflow&#34;&gt;What Is Airflow?&lt;/h2&gt;

&lt;p&gt;Apache Airflow is one realization of the DevOps philosophy of &amp;ldquo;Configuration As Code.&amp;rdquo; Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow_dags.png&#34; width=&#34;85%&#34; alt=&#34;Airflow DAGs&#34; /&gt;
&lt;img src=&#34;https://docstest.github.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow.png&#34; width=&#34;85%&#34; alt=&#34;Airflow UI&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-airflow-on-kubernetes&#34;&gt;Why Airflow on Kubernetes?&lt;/h2&gt;

&lt;p&gt;Since its inception, Airflow&amp;rsquo;s greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.&lt;/p&gt;

&lt;p&gt;To address this issue, we&amp;rsquo;ve utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an &amp;ldquo;any job you want&amp;rdquo; workflow orchestrator.&lt;/p&gt;

&lt;h2 id=&#34;the-kubernetes-operator&#34;&gt;The Kubernetes Operator&lt;/h2&gt;

&lt;p&gt;Before we move any further, we should clarify that an &lt;a href=&#34;https://airflow.apache.org/concepts.html#operators&#34; target=&#34;_blank&#34;&gt;Operator&lt;/a&gt; in Airflow is a task definition. When a user creates a DAG, they would use an operator like the &amp;ldquo;SparkSubmitOperator&amp;rdquo; or the &amp;ldquo;PythonOperator&amp;rdquo; to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.&lt;/p&gt;

&lt;p&gt;Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Increased flexibility for deployments:&lt;/strong&gt;&lt;br /&gt;
Airflow&amp;rsquo;s plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Flexibility of configurations and dependencies:&lt;/strong&gt;
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires &lt;a href=&#34;https://www.scipy.org&#34; target=&#34;_blank&#34;&gt;SciPy&lt;/a&gt; and another that requires &lt;a href=&#34;http://www.numpy.org&#34; target=&#34;_blank&#34;&gt;NumPy&lt;/a&gt;, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Usage of kubernetes secrets for added security:&lt;/strong&gt;
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-airflow-architecture.png&#34; width=&#34;85%&#34; alt=&#34;Airflow Architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Kubernetes Operator uses the &lt;a href=&#34;https://github.com/kubernetes-client/Python&#34; target=&#34;_blank&#34;&gt;Kubernetes Python Client&lt;/a&gt; to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you&amp;rsquo;ve defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.&lt;/p&gt;

&lt;h1 id=&#34;using-the-kubernetes-operator&#34;&gt;Using the Kubernetes Operator&lt;/h1&gt;

&lt;h2 id=&#34;a-basic-example&#34;&gt;A Basic Example&lt;/h2&gt;

&lt;p&gt;The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the &lt;code&gt;passing-task&lt;/code&gt; pod should complete, while the &lt;code&gt;failing-task&lt;/code&gt; pod returns a failure to the Airflow webserver.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DAG
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;datetime&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; datetime, timedelta
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.contrib.operators.kubernetes_pod_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; KubernetesPodOperator
&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.operators.dummy_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DummyOperator


default_args &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;: datetime&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;utcnow(),
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow@example.com&amp;#39;&lt;/span&gt;],
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_failure&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_retry&amp;#39;&lt;/span&gt;: False,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retry_delay&amp;#39;&lt;/span&gt;: timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)
}

dag &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DAG(
    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;kubernetes_sample&amp;#39;&lt;/span&gt;, default_args&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;default_args, schedule_interval&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;))


start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DummyOperator(task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;run_this_first&amp;#39;&lt;/span&gt;, dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag)

passing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python:3.6&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-test&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )

failing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ubuntu:1604&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )

passing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)
failing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-05-25-Airflow-Kubernetes-Operator/2018-05-25-basic-dag-run.png&#34; width=&#34;85%&#34; alt=&#34;Basic DAG Run&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;but-how-does-this-relate-to-my-workflow&#34;&gt;But how does this relate to my workflow?&lt;/h2&gt;

&lt;p&gt;While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.&lt;/p&gt;

&lt;h3 id=&#34;1-pr-in-github&#34;&gt;1: PR in github&lt;/h3&gt;

&lt;p&gt;Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR&amp;rsquo;ing your code, and merge to the master branch to trigger an automated CI build.&lt;/p&gt;

&lt;h3 id=&#34;2-ci-cd-via-jenkins-docker-image&#34;&gt;2: CI/CD via Jenkins -&amp;gt; Docker Image&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://getintodevops.com/blog/building-your-first-docker-image-with-jenkins-2-guide-for-developers&#34; target=&#34;_blank&#34;&gt;Generate your Docker images and bump release version within your Jenkins build&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;3-airflow-launches-task&#34;&gt;3: Airflow launches task&lt;/h3&gt;

&lt;p&gt;Finally, update your DAGs to reflect the new release version and you should be ready to go!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;production_task &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,
                          &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span&gt;
                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span&gt;,
                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],
                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],
                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,
                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,
                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,
                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag
                          )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id=&#34;launching-a-test-deployment&#34;&gt;Launching a test deployment&lt;/h1&gt;

&lt;p&gt;Since the Kubernetes Operator is not yet released, we haven&amp;rsquo;t released an official &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;helm&lt;/a&gt; chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:&lt;/p&gt;

&lt;h2 id=&#34;step-1-set-your-kubeconfig-to-point-to-a-kubernetes-cluster&#34;&gt;Step 1: Set your kubeconfig to point to a kubernetes cluster&lt;/h2&gt;

&lt;h2 id=&#34;step-2-clone-the-airflow-repo&#34;&gt;Step 2: Clone the Airflow Repo:&lt;/h2&gt;

&lt;p&gt;Run &lt;code&gt;git clone https://github.com/apache/incubator-airflow.git&lt;/code&gt; to clone the official Airflow repo.&lt;/p&gt;

&lt;h2 id=&#34;step-3-run&#34;&gt;Step 3: Run&lt;/h2&gt;

&lt;p&gt;To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on, let&amp;rsquo;s discuss what these commands are doing:&lt;/p&gt;

&lt;h3 id=&#34;sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml&#34;&gt;sed -ie &amp;ldquo;s/KubernetesExecutor/LocalExecutor/g&amp;rdquo; scripts/ci/kubernetes/kube/configmaps.yaml&lt;/h3&gt;

&lt;p&gt;The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-docker-build-sh&#34;&gt;./scripts/ci/kubernetes/Docker/build.sh&lt;/h3&gt;

&lt;p&gt;This script will tar the Airflow master source code build a Docker container based on the Airflow distribution&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-kube-deploy-sh&#34;&gt;./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3&gt;

&lt;p&gt;Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml&lt;/p&gt;

&lt;h2 id=&#34;step-4-log-into-your-webserver&#34;&gt;Step 4: Log into your webserver&lt;/h2&gt;

&lt;p&gt;Now that your Airflow instance is running let&amp;rsquo;s take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WEB=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the Airflow UI will exist on &lt;a href=&#34;http://localhost:8080&#34; target=&#34;_blank&#34;&gt;http://localhost:8080&lt;/a&gt;. To log in simply enter &lt;code&gt;airflow&lt;/code&gt;/&lt;code&gt;airflow&lt;/code&gt; and you should have full access to the Airflow web UI.&lt;/p&gt;

&lt;h2 id=&#34;step-5-upload-a-test-document&#34;&gt;Step 5: Upload a test document&lt;/h2&gt;

&lt;p&gt;To modify/add your own DAGs, you can use &lt;code&gt;kubectl cp&lt;/code&gt; to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;kubectl cp &amp;lt;local file&amp;gt; &amp;lt;namespace&amp;gt;/&amp;lt;pod&amp;gt;:/root/airflow/dags -c scheduler&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;step-6-enjoy&#34;&gt;Step 6: Enjoy!&lt;/h2&gt;

&lt;h1 id=&#34;so-when-will-i-be-able-to-use-this&#34;&gt;So when will I be able to use this?&lt;/h1&gt;

&lt;p&gt;While this feature is still in the early stages, we hope to see it released for wide release in the next few months.&lt;/p&gt;

&lt;h1 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h1&gt;

&lt;p&gt;This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the &lt;a href=&#34;https://github.com/apache/incubator-airflow/tree/v1-10-test&#34; target=&#34;_blank&#34;&gt;1.10 release branch of Airflow&lt;/a&gt; (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributers can have a huge influence on the future of these features.&lt;/p&gt;

&lt;p&gt;For those interested in joining these efforts, I&amp;rsquo;d recommend checkint out these steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Join the airflow-dev mailing list at dev@airflow.apache.org.&lt;/li&gt;
&lt;li&gt;File an issue in &lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues/&#34; target=&#34;_blank&#34;&gt;Apache Airflow JIRA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join our SIG-BigData meetings on Wednesdays at 10am PST.&lt;/li&gt;
&lt;li&gt;Reach us on slack at #sig-big-data on kubernetes.slack.com&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Kubernetes 1.11: In-Cluster Load Balancing and CoreDNS Plugin Graduate to General Availability</title>
      <link>https://docstest.github.io/blog/2018/06/27/kubernetes-1.11-release-announcement/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/06/27/kubernetes-1.11-release-announcement/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Kubernetes 1.11 &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md&#34; target=&#34;_blank&#34;&gt;Release Team&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We’re pleased to announce the delivery of Kubernetes 1.11, our second release of 2018!&lt;/p&gt;

&lt;p&gt;Today’s release continues to advance maturity, scalability, and flexibility of Kubernetes, marking significant progress on features that the team has been hard at work on over the last year. This newest version graduates key features in networking, opens up two major features from SIG-API Machinery and SIG-Node for beta testing, and continues to enhance storage features that have been a focal point of the past two releases. The features in this release make it increasingly possible to plug any infrastructure, cloud or on-premise, into the Kubernetes system.&lt;/p&gt;

&lt;p&gt;Notable additions in this release include two highly-anticipated features graduating to general availability: IPVS-based In-Cluster Load Balancing and CoreDNS as a cluster DNS add-on option, which means increased scalability and flexibility for production applications.&lt;/p&gt;

&lt;p&gt;Let’s dive into the key features of this release:&lt;/p&gt;

&lt;h2 id=&#34;ipvs-based-in-cluster-service-load-balancing-graduates-to-general-availability&#34;&gt;IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability&lt;/h2&gt;

&lt;p&gt;In this release, &lt;a href=&#34;https://github.com/kubernetes/features/issues/265&#34; target=&#34;_blank&#34;&gt;IPVS-based in-cluster service load balancing&lt;/a&gt; has moved to stable. IPVS (IP Virtual Server) provides high-performance in-kernel load balancing, with a simpler programming interface than iptables. This change delivers better network throughput, better programming latency, and higher scalability limits for the cluster-wide distributed load-balancer that comprises the Kubernetes Service model. IPVS is not yet the default but clusters can begin to use it for production traffic.&lt;/p&gt;

&lt;h2 id=&#34;coredns-promoted-to-general-availability&#34;&gt;CoreDNS Promoted to General Availability&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://coredns.io&#34; target=&#34;_blank&#34;&gt;CoreDNS&lt;/a&gt; is now available as a &lt;a href=&#34;https://github.com/kubernetes/features/issues/427&#34; target=&#34;_blank&#34;&gt;cluster DNS add-on option&lt;/a&gt;, and is the default when using kubeadm. CoreDNS is a flexible, extensible authoritative DNS server and directly integrates with the Kubernetes API. CoreDNS has fewer moving parts than the previous DNS server, since it’s a single executable and a single process, and supports flexible use cases by creating custom DNS entries. It’s also written in Go making it memory-safe. You can learn more about CoreDNS &lt;a href=&#34;https://youtu.be/dz9S7R8r5gw&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;dynamic-kubelet-configuration-moves-to-beta&#34;&gt;Dynamic Kubelet Configuration Moves to Beta&lt;/h2&gt;

&lt;p&gt;This feature makes it possible for new Kubelet configurations to be rolled out in a live cluster.  Currently, Kubelets are configured via command-line flags, which makes it difficult to update Kubelet configurations in a running cluster. With this beta feature, &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/&#34; target=&#34;_blank&#34;&gt;users can configure Kubelets in a live cluster&lt;/a&gt; via the API server.&lt;/p&gt;

&lt;h2 id=&#34;custom-resource-definitions-can-now-define-multiple-versions&#34;&gt;Custom Resource Definitions Can Now Define Multiple Versions&lt;/h2&gt;

&lt;p&gt;Custom Resource Definitions are no longer restricted to defining a single version of the custom resource, a restriction that was difficult to work around. Now, with this beta &lt;a href=&#34;https://github.com/kubernetes/features/issues/544&#34; target=&#34;_blank&#34;&gt;feature&lt;/a&gt;, multiple versions of the resource can be defined. In the future, this will be expanded to support some automatic conversions; for now, this feature allows custom resource authors to “promote with safe changes, e.g. v1beta1 to v1,” and to create a migration path for resources which do have changes.&lt;/p&gt;

&lt;p&gt;Custom Resource Definitions now also support &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/customresources-subresources.md&#34; target=&#34;_blank&#34;&gt;&amp;ldquo;status&amp;rdquo; and &amp;ldquo;scale&amp;rdquo; subresources&lt;/a&gt;, which integrate with monitoring and high-availability frameworks. These two changes advance the ability to run cloud-native applications in production using Custom Resource Definitions.&lt;/p&gt;

&lt;h2 id=&#34;enhancements-to-csi&#34;&gt;Enhancements to CSI&lt;/h2&gt;

&lt;p&gt;Container Storage Interface (CSI) has been a major topic over the last few releases. After moving to &lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34; target=&#34;_blank&#34;&gt;beta in 1.10&lt;/a&gt;, the 1.11 release continues enhancing CSI with a number of features. The 1.11 release adds alpha support for raw block volumes to CSI, integrates CSI with the new kubelet plugin registration mechanism, and makes it easier to pass secrets to CSI plugins.&lt;/p&gt;

&lt;h2 id=&#34;new-storage-features&#34;&gt;New Storage Features&lt;/h2&gt;

&lt;p&gt;Support for &lt;a href=&#34;https://github.com/kubernetes/features/issues/284&#34; target=&#34;_blank&#34;&gt;online resizing of Persistent Volumes&lt;/a&gt; has been introduced as an alpha feature. This enables users to increase the size of PVs without having to terminate pods and unmount volume first. The user will update the PVC to request a new size and kubelet will resize the file system for the PVC.&lt;/p&gt;

&lt;p&gt;Support for &lt;a href=&#34;https://github.com/kubernetes/features/issues/554&#34; target=&#34;_blank&#34;&gt;dynamic maximum volume count&lt;/a&gt; has been introduced as an alpha feature. This new feature enables in-tree volume plugins to specify the maximum number of volumes that can be attached to a node and allows the limit to vary depending on the type of node. Previously, these limits were hard coded or configured via an environment variable.&lt;/p&gt;

&lt;p&gt;The StorageObjectInUseProtection feature is now stable and prevents the removal of both &lt;a href=&#34;https://github.com/kubernetes/features/issues/499&#34; target=&#34;_blank&#34;&gt;Persistent Volumes&lt;/a&gt; that are bound to a Persistent Volume Claim, and &lt;a href=&#34;https://github.com/kubernetes/features/issues/498&#34; target=&#34;_blank&#34;&gt;Persistent Volume Claims&lt;/a&gt; that are being used by a pod. This safeguard will help prevent issues from deleting a PV or a PVC that is currently tied to an active pod.&lt;/p&gt;

&lt;p&gt;Each Special Interest Group (SIG) within the community continues to deliver the most-requested enhancements, fixes, and functionality for their respective specialty areas. For a complete list of inclusions by SIG, please visit the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#111-release-notes&#34; target=&#34;_blank&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;availability&#34;&gt;Availability&lt;/h2&gt;

&lt;p&gt;Kubernetes 1.11 is available for &lt;a href=&#34;https://github.com/kubernetes/kubernetes/releases/tag/v1.11.0&#34; target=&#34;_blank&#34;&gt;download on GitHub&lt;/a&gt;. To get started with Kubernetes, check out these &lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34; target=&#34;_blank&#34;&gt;interactive tutorials&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can also install 1.11 using Kubeadm. Version 1.11.0 will be available as Deb and RPM packages, installable using the &lt;a href=&#34;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&#34; target=&#34;_blank&#34;&gt;Kubeadm cluster installer&lt;/a&gt; sometime on June 28th.&lt;/p&gt;

&lt;h2 id=&#34;4-day-features-blog-series&#34;&gt;4 Day Features Blog Series&lt;/h2&gt;

&lt;p&gt;If you’re interested in exploring these features more in depth, check back in two weeks for our 4 Days of Kubernetes series where we’ll highlight detailed walkthroughs of the following features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Day 1: &lt;a href=&#34;https://docstest.github.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/&#34;&gt;IPVS-Based In-Cluster Service Load Balancing Graduates to General Availability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 2: &lt;a href=&#34;https://docstest.github.io/blog/2018/07/10/coredns-ga-for-kubernetes-cluster-dns/&#34;&gt;CoreDNS Promoted to General Availability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 3: &lt;a href=&#34;https://docstest.github.io/blog/2018/07/11/dynamic-kubelet-configuration/&#34;&gt;Dynamic Kubelet Configuration Moves to Beta&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Day 4: &lt;a href=&#34;https://docstest.github.io/blog/2018/07/12/resizing-persistent-volumes-using-kubernetes/&#34;&gt;Resizing Persistent Volumes using Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;release-team&#34;&gt;Release team&lt;/h2&gt;

&lt;p&gt;This release is made possible through the effort of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href=&#34;https://github.com/kubernetes/sig-release/blob/master/releases/release-1.11/release_team.md&#34; target=&#34;_blank&#34;&gt;release team&lt;/a&gt; led by Josh Berkus, Kubernetes Community Manager at Red Hat. The 20 individuals on the release team coordinate many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p&gt;

&lt;p&gt;As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid clip. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has over 20,000 individual contributors to date and an active community of more than 40,000 people.&lt;/p&gt;

&lt;h2 id=&#34;project-velocity&#34;&gt;Project Velocity&lt;/h2&gt;

&lt;p&gt;The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;K8s DevStats&lt;/a&gt; illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. On average, 250 different companies and over 1,300 individuals contribute to Kubernetes each month. &lt;a href=&#34;https://devstats.k8s.io&#34; target=&#34;_blank&#34;&gt;Check out DevStats&lt;/a&gt; to learn more about the overall velocity of the Kubernetes project and community.&lt;/p&gt;

&lt;h2 id=&#34;user-highlights&#34;&gt;User Highlights&lt;/h2&gt;

&lt;p&gt;Established, global organizations are using &lt;a href=&#34;https://kubernetes.io/case-studies/&#34; target=&#34;_blank&#34;&gt;Kubernetes in production&lt;/a&gt; at massive scale. Recently published user stories from the community include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The New York Times&lt;/strong&gt;, known as the newspaper of record, &lt;a href=&#34;https://kubernetes.io/case-studies/newyorktimes/&#34; target=&#34;_blank&#34;&gt;moved out of its data centers and into the public cloud&lt;/a&gt; with the help of Google Cloud Platform and Kubernetes. This move meant a significant increase in speed of delivery, from 45 minutes to just a few seconds with Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Nordstrom&lt;/strong&gt;, a leading fashion retailer based in the U.S., began their cloud native journey by &lt;a href=&#34;https://kubernetes.io/case-studies/nordstrom/&#34; target=&#34;_blank&#34;&gt;adopting Docker containers orchestrated with Kubernetes&lt;/a&gt;. The results included a major increase in Ops efficiency, improving CPU utilization from 5x to 12x depending on the workload.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Squarespace&lt;/strong&gt;, a SaaS solution for easily building and hosting websites, &lt;a href=&#34;https://kubernetes.io/case-studies/squarespace/&#34; target=&#34;_blank&#34;&gt;moved their monolithic application to microservices with the help of Kubernetes&lt;/a&gt;. This resulted in a deployment time reduction of almost 85%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Crowdfire&lt;/strong&gt;, a leading social media management platform, moved from a monolithic application to a &lt;a href=&#34;https://kubernetes.io/case-studies/crowdfire/&#34; target=&#34;_blank&#34;&gt;custom Kubernetes-based setup&lt;/a&gt;. This move reduced deployment time from 15 minutes to less than a minute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Is Kubernetes helping your team? Share your story with the community.&lt;/p&gt;

&lt;h2 id=&#34;ecosystem-updates&#34;&gt;Ecosystem Updates&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The CNCF recently expanded its certification offerings to include a Certified Kubernetes Application Developer exam. The CKAD exam certifies an individual&amp;rsquo;s ability to design, build, configure, and expose cloud native applications for Kubernetes. More information can be found &lt;a href=&#34;https://www.cncf.io/blog/2018/03/16/cncf-announces-ckad-exam/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The CNCF recently added a new partner category, Kubernetes Training Partners (KTP). KTPs are a tier of vetted training providers who have deep experience in cloud native technology training. View partners and learn more &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CNCF also offers &lt;a href=&#34;https://www.cncf.io/certification/training/&#34; target=&#34;_blank&#34;&gt;online training&lt;/a&gt; that teaches the skills needed to create and configure a real-world Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Kubernetes documentation now features &lt;a href=&#34;https://k8s.io/docs/home/&#34; target=&#34;_blank&#34;&gt;user journeys&lt;/a&gt;: specific pathways for learning based on who readers are and what readers want to do. Learning Kubernetes is easier than ever for beginners, and more experienced users can find task journeys specific to cluster admins and application developers.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kubecon&#34;&gt;KubeCon&lt;/h2&gt;

&lt;p&gt;The world’s largest Kubernetes gathering, KubeCon + CloudNativeCon is coming to [Shanghai](&lt;a href=&#34;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/&#34; target=&#34;_blank&#34;&gt;https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2018/&lt;/a&gt; from November 14-15, 2018 and &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/&#34; target=&#34;_blank&#34;&gt;Seattle&lt;/a&gt; from December 11-13, 2018. This conference will feature technical sessions, case studies, developer deep dives, salons and more! The CFP for both event is currently open. &lt;a href=&#34;https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/program/call-for-proposals-cfp/&#34; target=&#34;_blank&#34;&gt;Submit your talk&lt;/a&gt; and &lt;a href=&#34;https://www.regonline.com/registration/Checkin.aspx?EventID=2246960&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt; today!&lt;/p&gt;

&lt;h2 id=&#34;webinar&#34;&gt;Webinar&lt;/h2&gt;

&lt;p&gt;Join members of the Kubernetes 1.11 release team on July 31st at 10am PDT to learn about the major features in this release including In-Cluster Load Balancing and the CoreDNS Plugin. Register &lt;a href=&#34;https://www.cncf.io/event/webinar-kubernetes-1-11/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;get-involved&#34;&gt;Get Involved&lt;/h2&gt;

&lt;p&gt;The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;Special Interest Groups&lt;/a&gt; (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;community meeting&lt;/a&gt;, and through the channels below.&lt;/p&gt;

&lt;p&gt;Thank you for your continued feedback and support.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Post questions (or answer questions) on &lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Join the community portal for advocates on &lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow us on Twitter &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt; for latest updates&lt;/li&gt;
&lt;li&gt;Chat with the community on &lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Share your Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;story&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Dynamic Ingress in Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/06/07/dynamic-ingress-in-kubernetes/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/06/07/dynamic-ingress-in-kubernetes/</guid>
      <description>
        
        
        

&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Richard Li (Datawire)&lt;/p&gt;

&lt;p&gt;Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services.  One approach is &lt;a href=&#34;https://www.getambassador.io&#34; target=&#34;_blank&#34;&gt;Ambassador&lt;/a&gt;, a Kubernetes-native open source API Gateway built on the &lt;a href=&#34;https://www.envoyproxy.io&#34; target=&#34;_blank&#34;&gt;Envoy Proxy&lt;/a&gt;. Ambassador is designed for dynamic environment where services may come and go frequently.&lt;/p&gt;

&lt;p&gt;Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.&lt;/p&gt;

&lt;h2 id=&#34;a-basic-ambassador-example&#34;&gt;A Basic Ambassador Example&lt;/h2&gt;

&lt;p&gt;Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: httpbin
  annotations:
    getambassador.io/config: |
      ---
      apiVersion: ambassador/v0
      kind:  Mapping
      name:  httpbin_mapping
      prefix: /httpbin/
      service: httpbin.org:80
      host_rewrite: httpbin.org
spec:
  type: ClusterIP
  ports:
    - port: 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP &lt;code&gt;host&lt;/code&gt; header should be set to httpbin.org.&lt;/p&gt;

&lt;h2 id=&#34;kubeflow&#34;&gt;Kubeflow&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kubeflow/kubeflow&#34; target=&#34;_blank&#34;&gt;Kubeflow&lt;/a&gt; provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow.png&#34; alt=&#34;kubeflow&#34; /&gt;
&lt;center&gt;&lt;i&gt;Kubeflow architecture, pre-Ambassador&lt;/center&gt;&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&#34;service-configuration&#34;&gt;Service configuration&lt;/h2&gt;

&lt;p&gt;With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
apiVersion: ambassador/v0
kind:  Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this example, the “test” service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.&lt;/p&gt;

&lt;h2 id=&#34;kubeflow-and-ambassador&#34;&gt;Kubeflow and Ambassador&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-01-dynamic-ingress-kubernetes/kubeflow-ambassador.png&#34; alt=&#34;kubeflow-ambassador&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services,  Kubeflow creates and annotates a K8s service so that the model will be served at https://&lt;ingress host&gt;/models/&lt;model name&gt;/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.&lt;/p&gt;

&lt;p&gt;If you’re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.&lt;/p&gt;

&lt;p&gt;If you’re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the &lt;a href=&#34;https://www.getambassador.io/user-guide/getting-started&#34; target=&#34;_blank&#34;&gt;Getting Started with Ambassador guide&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: 4 Years of K8s</title>
      <link>https://docstest.github.io/blog/2018/06/06/4-years-of-k8s/</link>
      <pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/06/06/4-years-of-k8s/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Joe Beda (CTO and Founder, Heptio)&lt;/p&gt;

&lt;p&gt;On June 6, 2014 I checked in the &lt;a href=&#34;https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56&#34; target=&#34;_blank&#34;&gt;first commit&lt;/a&gt; of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesn’t tell the whole story.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png&#34; alt=&#34;k8s_first_commit&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.&lt;/p&gt;

&lt;p&gt;Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.&lt;/p&gt;

&lt;p&gt;Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.&lt;/p&gt;

&lt;p&gt;After we got the nod, it was time to actually build the system.  We took Brendan’s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across.  By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith.  Once we had something working, someone had to sign up to clean things up to get it ready for public launch.  That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in.  So while I have the first public commit to the repo, there was work underway well before that.&lt;/p&gt;

&lt;p&gt;The version of Kubernetes at that point was really just a shadow of what it was to become.  The core concepts were there but it was very raw.  For example, Pods were called Tasks.  That was changed a day before we went public.  All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon.  You can watch that video here:&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YrxnVKZeqK8&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger.  Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt.  The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special.  The best expression of this is the &lt;a href=&#34;https://github.com/kubernetes/steering/blob/master/values.md&#34; target=&#34;_blank&#34;&gt;set of Kubernetes values&lt;/a&gt; that Sarah Novotny helped curate.&lt;/p&gt;

&lt;p&gt;Here is to another 4 years and beyond! 🎉🎉🎉&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Say Hello to Discuss Kubernetes</title>
      <link>https://docstest.github.io/blog/2018/05/30/say-hello-to-discuss-kubernetes/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://docstest.github.io/blog/2018/05/30/say-hello-to-discuss-kubernetes/</guid>
      <description>
        
        
        &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Jorge Castro (Heptio)&lt;/p&gt;

&lt;p&gt;Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way.&lt;/p&gt;

&lt;p&gt;Slack is great for casual and timely conversations and keeping up with other community members, but communication can&amp;rsquo;t be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like &amp;ldquo;What&amp;rsquo;s your favorite CI/CD tool&amp;rdquo; or &amp;ldquo;&lt;a href=&#34;https://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192&#34; target=&#34;_blank&#34;&gt;Kubectl tips and tricks&lt;/a&gt;&amp;rdquo; are offtopic there.&lt;/p&gt;

&lt;p&gt;While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of &lt;a href=&#34;https://www.discourse.org/features&#34; target=&#34;_blank&#34;&gt;Discourse&lt;/a&gt;, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There&amp;rsquo;s only one way to find out: Welcome to &lt;a href=&#34;https://discuss.kubernetes.io&#34; target=&#34;_blank&#34;&gt;discuss.kubernetes.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docstest.github.io/images/blog/2018-05-30-say-hello-to-discuss-kubernetes.png&#34; alt=&#34;discuss_screenshot&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email.&lt;/p&gt;

&lt;p&gt;Ecosystem partners and developers now have a place where they can &lt;a href=&#34;https://discuss.kubernetes.io/c/announcements&#34; target=&#34;_blank&#34;&gt;announce projects&lt;/a&gt; that they&amp;rsquo;re working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building.&lt;/p&gt;

&lt;p&gt;This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience.&lt;/p&gt;

&lt;p&gt;Hop in and take a look. We&amp;rsquo;re just getting started, so you might want to begin by &lt;a href=&#34;https://discuss.kubernetes.io/t/introduce-yourself-here/56&#34; target=&#34;_blank&#34;&gt;introducing yourself&lt;/a&gt; and then browsing around. Apps are also available for &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.discourse&amp;amp;hl=en_US&amp;amp;rdid=com.discourse&amp;amp;pli=1&#34; target=&#34;_blank&#34;&gt;Android &lt;/a&gt;and &lt;a href=&#34;https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8&#34; target=&#34;_blank&#34;&gt;iOS&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
