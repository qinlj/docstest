<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SuperMap iDesktop .NET – 帮助文档</title>
    <link>https://qinlj.github.io/zh/</link>
    <description>The Kubernetes project blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 03 Apr 2019 15:57:36 +0800</lastBuildDate>
    <image>
      <url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url>
      <title>Kubernetes.io</title>
      <link>https://qinlj.github.io/zh/</link>
    </image>
    
	<atom:link href="https://qinlj.github.io/zh/feed.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Blog: 新贡献者工作坊上海站</title>
      <link>https://qinlj.github.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid>
      <description>
        
        
        &lt;!-- 
---
layout: blog
title: &#39;New Contributor Workshop Shanghai&#39;
date: 2018-12-05
---
 --&gt;

&lt;!-- 
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
 --&gt;

&lt;p&gt;&lt;strong&gt;作者&lt;/strong&gt;: Josh Berkus (红帽), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (中兴通讯)&lt;/p&gt;

&lt;!-- 
&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png&#34;
         alt=&#34;KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png&#34;
         alt=&#34;KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;KubeCon 上海站新贡献者峰会与会者，摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!-- 
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
 --&gt;

&lt;p&gt;最近，在中国的首次 KubeCon 上，我们完成了在中国的首次新贡献者峰会。看到所有中国和亚洲的开发者（以及来自世界各地的一些人）有兴趣成为贡献者，这令人非常兴奋。在长达一天的课程中，他们了解了如何、为什么以及在何处为 Kubernetes 作出贡献，创建了 PR，参加了贡献者圆桌讨论，并签署了他们的 CLA。&lt;/p&gt;

&lt;!-- 
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
 --&gt;

&lt;p&gt;这是我们的第二届新贡献者工作坊（NCW），它由前一次贡献者体验 SIG 成员创建和领导的哥本哈根研讨会延伸而来。根据受众情况，本次活动采用了中英文两种语言，充分利用了 CNCF 赞助的一流的同声传译服务。同样，NCW 团队由社区成员组成，既有说英语的，也有说汉语的：Yang Li、XiangPeng Zhao、Puja Abbassi、Noah Abrahams、Tim Pepper、Zach Corleissen、Sen Lu 和 Josh Berkus。除了演讲和帮助学员外，团队的双语成员还将所有幻灯片翻译成了中文。共有五十一名学员参加。&lt;/p&gt;

&lt;!-- 
&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png&#34;
         alt=&#34;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png&#34;
         alt=&#34;Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Noah Abrahams 讲解 Kubernetes 沟通渠道。摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!-- 
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have &#34;guest speakers&#34; from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
 --&gt;

&lt;p&gt;NCW 让参与者完成了为 Kubernetes 作出贡献的各个阶段，从决定在哪里作出贡献开始，接着介绍了 SIG 系统和我们的代码仓库结构。我们还有来自文档和测试基础设施领域的「客座讲者」，他们负责讲解有关的贡献。最后，我们在创建 issue、提交并批准 PR 的实践练习后，结束了工作坊。&lt;/p&gt;

&lt;!-- 
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
 --&gt;

&lt;p&gt;这些实践练习使用一个名为&lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;贡献者游乐场&lt;/a&gt;的代码仓库，由贡献者体验 SIG 创建，让新贡献者尝试在一个 Kubernetes 仓库中执行各种操作。它修改了 Prow 和 Tide 自动化，使用与真实代码仓库类似的 Owners 文件。这可以让学员了解为我们的仓库做出贡献的有关机制，同时又不妨碍正常的开发流程。&lt;/p&gt;

&lt;!-- 
&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png&#34;
         alt=&#34;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png&#34;
         alt=&#34;Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Yang Li 讲到如何让你的 PR 通过评审。摄影：Josh Berkus&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!-- 
Both the &#34;Great Firewall&#34; and the language barrier prevent contributing Kubernetes from China from being straightforward. What&#39;s more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
 --&gt;

&lt;p&gt;「防火长城」和语言障碍都使得在中国为 Kubernetes 作出贡献变得困难。而且，中国的开源商业模式并不成熟，员工在开源项目上工作的时间有限。&lt;/p&gt;

&lt;!-- 
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don&#39;t know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
 --&gt;

&lt;p&gt;中国工程师渴望参与 Kubernetes 的研发，但他们中的许多人不知道从何处开始，因为 Kubernetes 是一个如此庞大的项目。通过本次工作坊，我们希望帮助那些想要参与贡献的人，不论他们希望修复他们遇到的一些错误、改进或本地化文档，或者他们需要在工作中用到 Kubernetes。我们很高兴看到越来越多的中国贡献者在过去几年里加入社区，我们也希望将来可以看到更多。&lt;/p&gt;

&lt;!-- 
&#34;I have been participating in the Kubernetes community for about three years,&#34; said XiangPeng Zhao. &#34;In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it&#39;s not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago.&#34;
 --&gt;

&lt;p&gt;「我已经参与了 Kubernetes 社区大约三年」，XiangPeng Zhao 说，「在社区，我注意到越来越多的中国开发者表现出对 Kubernetes 贡献的兴趣。但是，开始为这样一个项目做贡献并不容易。我尽力帮助那些我在社区遇到的人，但是，我认为可能仍有一些新的贡献者离开社区，因为他们在遇到麻烦时不知道从哪里获得帮助。幸运的是，社区在 KubeCon 哥本哈根站发起了 NCW，并在 KubeCon 上海站举办了第二届。我很高兴受到 Josh Berkus 的邀请，帮助组织这个工作坊。在工作坊期间，我当面见到了社区里的朋友，在练习中指导了与会者，等等。所有这些对我来说都是难忘的经历。作为有着多年贡献者经验的我，也学习到了很多。我希望几年前我开始为 Kubernetes 做贡献时参加过这样的工作坊」。&lt;/p&gt;

&lt;!-- 
&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png&#34;
         alt=&#34;Panel of contributors. Photo by Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Panel of contributors. Photo by Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

 --&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://qinlj.github.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png&#34;
         alt=&#34;贡献者圆桌讨论。摄影：Jerry Zhang&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;贡献者圆桌讨论。摄影：Jerry Zhang&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;!-- 
The workshop ended with a panel of current contributors, featuring Lucas Käldström, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor&#39;s journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
 --&gt;

&lt;p&gt;工作坊以现有贡献者圆桌讨论结束，嘉宾包括 Lucas Käldström、Janet Kuo、Da Ma、Pengfei Ni、Zefeng Wang 和 Chao Xu。这场圆桌讨论旨在让新的和现有的贡献者了解一些最活跃的贡献者和维护者的幕后日常工作，不论他们来自中国还是世界各地。嘉宾们讨论了从哪里开始贡献者的旅程，以及如何与评审者和维护者进行互动。他们进一步探讨了在中国参与贡献的主要问题，并向与会者预告了在 Kubernetes 的未来版本中可以期待的令人兴奋的功能。&lt;/p&gt;

&lt;!-- 
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, &#34;I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor.&#34; Another attendee, Jie Jia, said, &#34;The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!&#34;
 --&gt;

&lt;p&gt;工作坊结束后，XiangPeng Zhao 和一些与会者就他们的经历在微信和 Twitter 上进行了交谈。他们很高兴参加了 NCW，并就改进工作坊提出了一些建议。一位名叫 Mohammad 的与会者说：「我在工作坊上玩得很开心，学习了参与 k8s 贡献的整个过程。」另一位与会者 Jie Jia 说：「工作坊非常精彩。它系统地解释了如何为 Kubernetes 做出贡献。即使参与者之前对此一无所知，他（她）也可以理解这个过程。对于那些已经是贡献者的人，他们也可以学习到新东西。此外，我还可以在工作坊上结识来自国内外的新朋友。真是棒极了！」&lt;/p&gt;

&lt;!-- 
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
 --&gt;

&lt;p&gt;贡献者体验 SIG 将继续在未来的 KubeCon 上举办新贡献者工作坊，包括西雅图站、巴塞罗那站，然后在 2019 年六月回到上海。如果你今年未能参加，请在未来的 KubeCon 上注册。并且，如果你遇到工作坊的与会者，请务必欢迎他们加入社区。&lt;/p&gt;

&lt;!-- 
Links:
 --&gt;

&lt;p&gt;链接：&lt;/p&gt;

&lt;!-- 
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
 --&gt;

&lt;ul&gt;
&lt;li&gt;中文版幻灯片：&lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;英文版幻灯片：&lt;a href=&#34;https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; 或 &lt;a href=&#34;https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;带有演讲者笔记的 Google Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/contributor-playground&#34; target=&#34;_blank&#34;&gt;贡献者游乐场&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title>
      <link>https://qinlj.github.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link>
      <pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid>
      <description>
        
        
        &lt;!--

Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)

Editor’s note: this post is part of a series of in-depth articles on what’s new in Kubernetes 1.11

--&gt;

&lt;p&gt;作者: Jun Du(华为), Haibin Xie(华为), Wei Liang(华为)&lt;/p&gt;

&lt;p&gt;注意: 这篇文章出自 系列深度文章 介绍 Kubernetes 1.11 的新特性&lt;/p&gt;

&lt;!--

Introduction

Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.

--&gt;

&lt;p&gt;介绍&lt;/p&gt;

&lt;p&gt;根据 Kubernetes 1.11 发布的博客文章, 我们宣布基于 IPVS 的集群内部服务负载均衡已达到一般可用性。 在这篇博客中，我们将带您深入了解该功能。&lt;/p&gt;

&lt;!--

What Is IPVS?

IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.

IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.

--&gt;

&lt;p&gt;什么是 IPVS ?&lt;/p&gt;

&lt;p&gt;IPVS (IP Virtual Server)是在 Netfilter 上层构建的，并作为 Linux 内核的一部分，实现传输层负载均衡。&lt;/p&gt;

&lt;p&gt;IPVS 集成在 LVS（Linux Virtual Server，Linux 虚拟服务器）中，它在主机上运行，并在物理服务器集群前作为负载均衡器。IPVS 可以将基于 TCP 和 UDP 服务的请求定向到真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。 因此，IPVS 自然支持 Kubernetes 服务。&lt;/p&gt;

&lt;!--

Why IPVS for Kubernetes?

As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.

Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.

Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.

On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.

--&gt;

&lt;p&gt;为什么为 Kubernetes 选择 IPVS ?&lt;/p&gt;

&lt;p&gt;随着 Kubernetes 的使用增长，其资源的可扩展性变得越来越重要。特别是，服务的可扩展性对于运行大型工作负载的开发人员/公司采用 Kubernetes 至关重要。&lt;/p&gt;

&lt;p&gt;Kube-proxy 是服务路由的构建块，它依赖于经过强化攻击的 iptables 来实现支持核心的服务类型，如 ClusterIP 和 NodePort。 但是，iptables 难以扩展到成千上万的服务，因为它纯粹是为防火墙而设计的，并且基于内核规则列表。&lt;/p&gt;

&lt;p&gt;尽管 Kubernetes 在版本v1.6中已经支持5000个节点，但使用 iptables 的 kube-proxy 实际上是将集群扩展到5000个节点的瓶颈。 一个例子是，在5000节点集群中使用 NodePort 服务，如果我们有2000个服务并且每个服务有10个 pod，这将在每个工作节点上至少产生20000个 iptable 记录，这可能使内核非常繁忙。&lt;/p&gt;

&lt;p&gt;另一方面，使用基于 IPVS 的集群内服务负载均衡可以为这种情况提供很多帮助。 IPVS 专门用于负载均衡，并使用更高效的数据结构（哈希表），允许几乎无限的规模扩张。&lt;/p&gt;

&lt;!--

IPVS-based Kube-proxy

Parameter Changes

Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.

--&gt;

&lt;p&gt;基于 IPVS 的 Kube-proxy&lt;/p&gt;

&lt;p&gt;参数更改&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;proxy-mode 除了现有的用户空间和 iptables 模式，IPVS 模式通过&amp;ndash;proxy-mode = ipvs 进行配置。 它隐式使用 IPVS NAT 模式进行服务端口映射。&lt;/p&gt;

&lt;!--

Parameter: --ipvs-scheduler

A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If it’s not configured, then round-robin (rr) is the default value.

- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue

In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.

--&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-scheduler&lt;/p&gt;

&lt;p&gt;添加了一个新的 kube-proxy 参数来指定 IPVS 负载均衡算法，参数为 &amp;ndash;ipvs-scheduler。 如果未配置，则默认为 round-robin 算法（rr）。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rr: round-robin&lt;/li&gt;
&lt;li&gt;lc: least connection&lt;/li&gt;
&lt;li&gt;dh: destination hashing&lt;/li&gt;
&lt;li&gt;sh: source hashing&lt;/li&gt;
&lt;li&gt;sed: shortest expected delay&lt;/li&gt;
&lt;li&gt;nq: never queue&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将来，我们可以实现特定于服务的调度程序（可能通过注释），该调度程序具有更高的优先级并覆盖该值。&lt;/p&gt;

&lt;!--

Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.

Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. &#39;5s&#39;, &#39;1m&#39;). Must be greater than 0.

Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. &#39;5s&#39;, &#39;1m&#39;). Must be greater than 0.

--&gt;

&lt;p&gt;参数: &amp;ndash;cleanup-ipvs 类似于 &amp;ndash;cleanup-iptables 参数，如果为 true，则清除在 IPVS 模式下创建的 IPVS 配置和 IPTables 规则。&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-sync-period 刷新 IPVS 规则的最大间隔时间（例如&amp;rsquo;5s&amp;rsquo;，&amp;rsquo;1m&amp;rsquo;）。 必须大于0。&lt;/p&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-min-sync-period 刷新 IPVS 规则的最小间隔时间间隔（例如&amp;rsquo;5s&amp;rsquo;，&amp;rsquo;1m&amp;rsquo;）。 必须大于0。&lt;/p&gt;

&lt;!--

Parameter: --ipvs-exclude-cidrs  A comma-separated list of CIDR&#39;s which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can&#39;t distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.

--&gt;

&lt;p&gt;参数: &amp;ndash;ipvs-exclude-cidrs  清除 IPVS 规则时 IPVS 代理不应触及的 CIDR 的逗号分隔列表，因为 IPVS 代理无法区分 kube-proxy 创建的 IPVS 规则和用户原始规则 IPVS 规则。 如果您在环境中使用 IPVS proxier 和您自己的 IPVS 规则，则应指定此参数，否则将清除原始规则。&lt;/p&gt;

&lt;!--

Design Considerations

IPVS Service Network Topology

When creating a ClusterIP type Service, IPVS proxier will do the following three things:

- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
  --&gt;

&lt;p&gt;设计注意事项&lt;/p&gt;

&lt;p&gt;IPVS 服务网络拓扑&lt;/p&gt;

&lt;p&gt;创建 ClusterIP 类型服务时，IPVS proxier 将执行以下三项操作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确保节点中存在虚拟接口，默认为 kube-ipvs0&lt;/li&gt;
&lt;li&gt;将服务 IP 地址绑定到虚拟接口&lt;/li&gt;
&lt;li&gt;分别为每个服务 IP 地址创建 IPVS  虚拟服务器&lt;/li&gt;
&lt;/ul&gt;

&lt;!--

Here comes an example:

    # kubectl describe svc nginx-service
    Name:           nginx-service
    ...
    Type:           ClusterIP
    IP:             10.102.128.4
    Port:           http    3080/TCP
    Endpoints:      10.244.0.235:8080,10.244.1.237:8080
    Session Affinity:   None
    
    # ip addr
    ...
    73: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN qlen 1000
        link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
        inet 10.102.128.4/32 scope global kube-ipvs0
           valid_lft forever preferred_lft forever
    
    # ipvsadm -ln
    IP Virtual Server version 1.2.1 (size=4096)
    Prot LocalAddress:Port Scheduler Flags
      -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn     
    TCP  10.102.128.4:3080 rr
      -&gt; 10.244.0.235:8080            Masq    1      0          0         
      -&gt; 10.244.1.237:8080            Masq    1      0          0   

--&gt;

&lt;p&gt;这是一个例子:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
Type:           ClusterIP
IP:             10.102.128.4
Port:           http    3080/TCP
Endpoints:      10.244.0.235:8080,10.244.1.237:8080
Session Affinity:   None

# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
    inet 10.102.128.4/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn     
TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0         
  -&amp;gt; 10.244.1.237:8080            Masq    1      0          0   
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.

Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.

Port Mapping

There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.

--&gt;

&lt;p&gt;请注意，Kubernetes 服务和 IPVS 虚拟服务器之间的关系是“1：N”。 例如，考虑具有多个 IP 地址的 Kubernetes 服务。 外部 IP 类型服务有两个 IP 地址 - 集群IP和外部 IP。 然后，IPVS 代理将创建2个 IPVS 虚拟服务器 - 一个用于集群 IP，另一个用于外部 IP。 Kubernetes 的 endpoint（每个IP +端口对）与 IPVS 虚拟服务器之间的关系是“1：1”。&lt;/p&gt;

&lt;p&gt;删除 Kubernetes 服务将触发删除相应的 IPVS 虚拟服务器，IPVS 物理服务器及其绑定到虚拟接口的 IP 地址。&lt;/p&gt;

&lt;p&gt;端口映射&lt;/p&gt;

&lt;p&gt;IPVS 中有三种代理模式：NAT（masq），IPIP 和 DR。 只有 NAT 模式支持端口映射。 Kube-proxy 利用 NAT 模式进行端口映射。 以下示例显示 IPVS 服务端口3080到Pod端口8080的映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TCP  10.102.128.4:3080 rr
  -&amp;gt; 10.244.0.235:8080            Masq    1      0          0         
  -&amp;gt; 10.244.1.237:8080            Masq    1      0       
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Session Affinity

IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:

--&gt;

&lt;p&gt;会话关系&lt;/p&gt;

&lt;p&gt;IPVS 支持客户端 IP 会话关联（持久连接）。 当服务指定会话关系时，IPVS 代理将在 IPVS 虚拟服务器中设置超时值（默认为180分钟= 10800秒）。 例如：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kubectl describe svc nginx-service
Name:           nginx-service
...
IP:             10.102.128.4
Port:           http    3080/TCP
Session Affinity:   ClientIP

# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&amp;gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.102.128.4:3080 rr persistent 10800
&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Iptables &amp; Ipset in IPVS Proxier

IPVS is for load balancing and it can&#39;t handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.

IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:

- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service

However, we don&#39;t want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:

--&gt;

&lt;p&gt;IPVS 代理中的 Iptables 和 Ipset&lt;/p&gt;

&lt;p&gt;IPVS 用于负载均衡，它无法处理 kube-proxy 中的其他问题，例如 包过滤，数据包欺骗，SNAT 等&lt;/p&gt;

&lt;p&gt;IPVS proxier 在上述场景中利用 iptables。 具体来说，ipvs proxier 将在以下4种情况下依赖于 iptables：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kube-proxy 以 &amp;ndash;masquerade-all = true 开头&lt;/li&gt;
&lt;li&gt;在 kube-proxy 启动中指定集群 CIDR&lt;/li&gt;
&lt;li&gt;支持 Loadbalancer 类型服务&lt;/li&gt;
&lt;li&gt;支持 NodePort 类型的服务&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是，我们不想创建太多的 iptables 规则。 所以我们采用 ipset 来减少 iptables 规则。 以下是 IPVS proxier 维护的 ipset 集表：&lt;/p&gt;

&lt;!--

  set name                          members                                     usage                                   
  KUBE-CLUSTER-IP                   All Service IP + port                       masquerade for cases that masquerade-all=true or clusterCIDR specified
  KUBE-LOOP-BACK                    All Service IP + port + IP                  masquerade for resolving hairpin issue  
  KUBE-EXTERNAL-IP                  Service External IP + port                  masquerade for packets to external IPs  
  KUBE-LOAD-BALANCER                Load Balancer ingress IP + port             masquerade for packets to Load Balancer type service
  KUBE-LOAD-BALANCER-LOCAL          Load Balancer ingress IP + port with externalTrafficPolicy=local    accept packets to Load Balancer with externalTrafficPolicy=local
  KUBE-LOAD-BALANCER-FW             Load Balancer ingress IP + port with loadBalancerSourceRanges   Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
  KUBE-LOAD-BALANCER-SOURCE-CIDR    Load Balancer ingress IP + port + source CIDR   accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
  KUBE-NODE-PORT-TCP                NodePort type Service TCP port              masquerade for packets to NodePort(TCP) 
  KUBE-NODE-PORT-LOCAL-TCP          NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
  KUBE-NODE-PORT-UDP                NodePort type Service UDP port              masquerade for packets to NodePort(UDP) 
  KUBE-NODE-PORT-LOCAL-UDP          NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local

--&gt;

&lt;p&gt;设置名称                              成员                                          用法&lt;br /&gt;
  KUBE-CLUSTER-IP                   所有服务 IP + 端口                                masquerade-all=true 或 clusterCIDR 指定的情况下进行伪装
  KUBE-LOOP-BACK                    所有服务 IP +端口+ IP                             解决数据包欺骗问题&lt;br /&gt;
  KUBE-EXTERNAL-IP                  服务外部 IP +端口                                 将数据包伪装成外部 IP&lt;br /&gt;
  KUBE-LOAD-BALANCER                负载均衡器入口 IP +端口                              将数据包伪装成 Load Balancer 类型的服务&lt;br /&gt;
  KUBE-LOAD-BALANCER-LOCAL          负载均衡器入口 IP +端口 以及 externalTrafficPolicy=local   接受数据包到 Load Balancer externalTrafficPolicy=local
  KUBE-LOAD-BALANCER-FW             负载均衡器入口 IP +端口 以及 loadBalancerSourceRanges  使用指定的 loadBalancerSourceRanges 丢弃 Load Balancer类型Service的数据包
  KUBE-LOAD-BALANCER-SOURCE-CIDR    负载均衡器入口 IP +端口 + 源 CIDR                     接受 Load Balancer 类型 Service 的数据包，并指定loadBalancerSourceRanges
  KUBE-NODE-PORT-TCP                NodePort 类型服务 TCP                           将数据包伪装成 NodePort（TCP）&lt;br /&gt;
  KUBE-NODE-PORT-LOCAL-TCP          NodePort 类型服务 TCP 端口，带有 externalTrafficPolicy=local 接受数据包到 NodePort 服务 使用 externalTrafficPolicy=local
  KUBE-NODE-PORT-UDP                NodePort 类型服务 UDP 端口                        将数据包伪装成 NodePort(UDP)&lt;br /&gt;
  KUBE-NODE-PORT-LOCAL-UDP          NodePort 类型服务 UDP 端口 使用 externalTrafficPolicy=local 接受数据包到NodePort服务 使用 externalTrafficPolicy=local&lt;/p&gt;

&lt;!--

In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.

--&gt;

&lt;p&gt;通常，对于 IPVS proxier，无论我们有多少 Service/ Pod，iptables 规则的数量都是静态的。&lt;/p&gt;

&lt;!--

Run kube-proxy in IPVS Mode

Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.

    ip_vs
    ip_vs_rr
    ip_vs_wrr
    ip_vs_sh
    nf_conntrack_ipv4

Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.

--&gt;

&lt;p&gt;在 IPVS 模式下运行 kube-proxy&lt;/p&gt;

&lt;p&gt;目前，本地脚本，GCE 脚本和 kubeadm 支持通过导出环境变量（KUBE_PROXY_MODE=ipvs）或指定标志（&amp;ndash;proxy-mode=ipvs）来切换 IPVS 代理模式。 在运行IPVS 代理之前，请确保已安装 IPVS 所需的内核模块。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后，对于 Kubernetes v1.10，“SupportIPVSProxyMode” 默认设置为 “true”。 对于 Kubernetes v1.11 ，该选项已完全删除。 但是，您需要在v1.10之前为Kubernetes 明确启用 &amp;ndash;feature-gates = SupportIPVSProxyMode = true。&lt;/p&gt;

&lt;!--

Get Involved

The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.

Thank you for your continued feedback and support.

Post questions (or answer questions) on Stack Overflow

Join the community portal for advocates on K8sPort

Follow us on Twitter @Kubernetesio for latest updates

Chat with the community on Slack

Share your Kubernetes story

--&gt;

&lt;p&gt;参与其中&lt;/p&gt;

&lt;p&gt;参与 Kubernetes 的最简单方法是加入众多&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-list.md&#34; target=&#34;_blank&#34;&gt;特别兴趣小组&lt;/a&gt; (SIG）中与您的兴趣一致的小组。 你有什么想要向 Kubernetes 社区广播的吗？ 在我们的每周&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting&#34; target=&#34;_blank&#34;&gt;社区会议&lt;/a&gt;或通过以下渠道分享您的声音。&lt;/p&gt;

&lt;p&gt;感谢您的持续反馈和支持。
在&lt;a href=&#34;http://stackoverflow.com/questions/tagged/kubernetes&#34; target=&#34;_blank&#34;&gt;Stack Overflow&lt;/a&gt;上发布问题（或回答问题）&lt;/p&gt;

&lt;p&gt;加入&lt;a href=&#34;http://k8sport.org/&#34; target=&#34;_blank&#34;&gt;K8sPort&lt;/a&gt;的倡导者社区门户网站&lt;/p&gt;

&lt;p&gt;在 Twitter 上关注我们 &lt;a href=&#34;https://twitter.com/kubernetesio&#34; target=&#34;_blank&#34;&gt;@Kubernetesio&lt;/a&gt;获取最新更新&lt;/p&gt;

&lt;p&gt;在&lt;a href=&#34;http://slack.k8s.io/&#34; target=&#34;_blank&#34;&gt;Slack&lt;/a&gt;上与社区聊天&lt;/p&gt;

&lt;p&gt;分享您的 Kubernetes &lt;a href=&#34;https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform&#34; target=&#34;_blank&#34;&gt;故事&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Airflow在Kubernetes中的使用（第一部分）：一种不同的操作器</title>
      <link>https://qinlj.github.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid>
      <description>
        
        
        

&lt;!--

Author: Daniel Imberman (Bloomberg LP)

--&gt;

&lt;p&gt;作者: Daniel Imberman (Bloomberg LP)&lt;/p&gt;

&lt;!--

## Introduction

 

As part of Bloomberg&#39;s continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.

--&gt;

&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;

&lt;p&gt;作为Bloomberg [继续致力于开发Kubernetes生态系统]的一部分（&lt;a href=&#34;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes&#34; target=&#34;_blank&#34;&gt;https://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/），我们很高兴能够宣布Kubernetes&lt;/a&gt; Airflow Operator的发布; [Apache Airflow]（&lt;a href=&#34;https://airflow.apache.org/）的机制，一种流行的工作流程编排框架，使用Kubernetes&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/）的机制，一种流行的工作流程编排框架，使用Kubernetes&lt;/a&gt; API可以在本机启动任意的Kubernetes Pod。&lt;/p&gt;

&lt;!--

## What Is Airflow?

 

Apache Airflow is one realization of the DevOps philosophy of &#34;Configuration As Code.&#34; Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.

 





--&gt;

&lt;h2 id=&#34;什么是airflow&#34;&gt;什么是Airflow?&lt;/h2&gt;

&lt;p&gt;Apache Airflow是DevOps“Configuration As Code”理念的一种实现。 Airflow允许用户使用简单的Python对象DAG（有向无环图）启动多步骤流水线。 您可以在易于阅读的UI中定义依赖关系，以编程方式构建复杂的工作流，并监视调度的作业。&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.png”width =“85％”alt =“Airflow DAGs”/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.png”width =“85％”alt =“Airflow UI”/&gt;&lt;/p&gt;

&lt;!--

## Why Airflow on Kubernetes?

 

Since its inception, Airflow&#39;s greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.

 

To address this issue, we&#39;ve utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an &#34;any job you want&#34; workflow orchestrator.

--&gt;

&lt;h2 id=&#34;为什么在kubernetes上使用airflow&#34;&gt;为什么在Kubernetes上使用Airflow?&lt;/h2&gt;

&lt;p&gt;自成立以来，Airflow的最大优势在于其灵活性。 Airflow提供广泛的服务集成，包括Spark和HBase，以及各种云提供商的服务。 Airflow还通过其插件框架提供轻松的可扩展性。但是，该项目的一个限制是Airflow用户仅限于执行时Airflow站点上存在的框架和客户端。单个组织可以拥有各种Airflow工作流程，范围从数据科学流到应用程序部署。用例中的这种差异会在依赖关系管理中产生问题，因为两个团队可能会在其工作流程使用截然不同的库。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，我们使Kubernetes允许用户启动任意Kubernetes pod和配置。 Airflow用户现在可以在其运行时环境，资源和机密上拥有全部权限，基本上将Airflow转变为“您想要的任何工作”工作流程协调器。&lt;/p&gt;

&lt;!--

## The Kubernetes Operator

 

Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the &#34;SparkSubmitOperator&#34; or the &#34;PythonOperator&#34; to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.

 

Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:

--&gt;

&lt;h2 id=&#34;kubernetes运营商&#34;&gt;Kubernetes运营商&lt;/h2&gt;

&lt;p&gt;在进一步讨论之前，我们应该澄清Airflow中的[Operator]（&lt;a href=&#34;https://airflow.apache.org/concepts.html#operators）是一个任务定义。&#34; target=&#34;_blank&#34;&gt;https://airflow.apache.org/concepts.html#operators）是一个任务定义。&lt;/a&gt; 当用户创建DAG时，他们将使用像“SparkSubmitOperator”或“PythonOperator”这样的operator分别提交/监视Spark作业或Python函数。 Airflow附带了Apache Spark，BigQuery，Hive和EMR等框架的内置运算符。 它还提供了一个插件入口点，允许DevOps工程师开发自己的连接器。&lt;/p&gt;

&lt;p&gt;Airflow用户一直在寻找更易于管理部署和ETL流的方法。 在增加监控的同时，任何解耦流程的机会都可以减少未来的停机等问题。 以下是Airflow Kubernetes Operator提供的好处：&lt;/p&gt;

&lt;!--

 * Increased flexibility for deployments:  

Airflow&#39;s plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.

--&gt;

&lt;ul&gt;
&lt;li&gt;提高部署灵活性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Airflow的插件API一直为希望在其DAG中测试新功能的工程师提供了重要的福利。 不利的一面是，每当开发人员想要创建一个新的operator时，他们就必须开发一个全新的插件。 现在，任何可以在Docker容器中运行的任务都可以通过完全相同的运算符访问，而无需维护额外的Airflow代码。&lt;/p&gt;

&lt;!--

 * Flexibility of configurations and dependencies:

For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.  

--&gt;

&lt;ul&gt;
&lt;li&gt;配置和依赖的灵活性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于在静态Airflow工作程序中运行的operator，依赖关系管理可能变得非常困难。 如果开发人员想要运行一个需要[SciPy]（&lt;a href=&#34;https://www.scipy.org）&#34; target=&#34;_blank&#34;&gt;https://www.scipy.org）&lt;/a&gt; 的任务和另一个需要[NumPy]（&lt;a href=&#34;http://www.numpy.org）&#34; target=&#34;_blank&#34;&gt;http://www.numpy.org）&lt;/a&gt; 的任务，开发人员必须维护所有Airflow节点中的依赖关系或将任务卸载到其他计算机（如果外部计算机以未跟踪的方式更改，则可能导致错误）。 自定义Docker镜像允许用户确保任务环境，配置和依赖关系完全是幂等的。&lt;/p&gt;

&lt;!--

 * Usage of kubernetes secrets for added security:

Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.

--&gt;

&lt;ul&gt;
&lt;li&gt;使用kubernetes Secret以增加安全性：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;处理敏感数据是任何开发工程师的核心职责。 Airflow用户总有机会在严格条款的基础上隔离任何API密钥，数据库密码和登录凭据。 使用Kubernetes运算符，用户可以利用Kubernetes Vault技术存储所有敏感数据。 这意味着Airflow工作人员将永远无法访问此信息，并且可以容易地请求仅使用他们需要的密码信息构建pod。&lt;/p&gt;

&lt;!--

# Architecture

 



 

The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you&#39;ve defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.

--&gt;

&lt;p&gt;＃架构&lt;/p&gt;

&lt;p&gt;&lt;img src =“/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.png”width =“85％”alt =“Airflow Architecture”/&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes Operator使用[Kubernetes Python客户端]（&lt;a href=&#34;https://github.com/kubernetes-client/Python)生成由APIServer处理的请求（1）。&#34; target=&#34;_blank&#34;&gt;https://github.com/kubernetes-client/Python)生成由APIServer处理的请求（1）。&lt;/a&gt; 然后，Kubernetes将使用您定义的需求启动您的pod（2）。映像文件中将加载环境变量，Secret和依赖项，执行单个命令。 一旦启动作业，operator只需要监视跟踪日志的状况（3）。 用户可以选择将日志本地收集到调度程序或当前位于其Kubernetes集群中的任何分布式日志记录服务。&lt;/p&gt;

&lt;!--

# Using the Kubernetes Operator

 

## A Basic Example

 

The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.

--&gt;

&lt;p&gt;＃使用Kubernetes Operator&lt;/p&gt;

&lt;p&gt;##一个基本的例子&lt;/p&gt;

&lt;p&gt;以下DAG可能是我们可以编写的最简单的示例，以显示Kubernetes Operator的工作原理。 这个DAG在Kubernetes上创建了两个pod：一个带有Python的Linux发行版和一个没有它的基本Ubuntu发行版。 Python pod将正确运行Python请求，而没有Python的那个将向用户报告失败。 如果Operator正常工作，则应该完成“passing-task”pod，而“falling-task”pod则向Airflow网络服务器返回失败。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DAG

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;datetime&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; datetime, timedelta

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.contrib.operators.kubernetes_pod_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; KubernetesPodOperator

&lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#00f;font-weight:bold&#34;&gt;airflow.operators.dummy_operator&lt;/span&gt; &lt;span style=&#34;color:#a2f;font-weight:bold&#34;&gt;import&lt;/span&gt; DummyOperator


default_args &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; {

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;owner&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow&amp;#39;&lt;/span&gt;,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;depends_on_past&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;start_date&amp;#39;&lt;/span&gt;: datetime&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;utcnow(),

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email&amp;#39;&lt;/span&gt;: [&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;airflow@example.com&amp;#39;&lt;/span&gt;],

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_failure&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;email_on_retry&amp;#39;&lt;/span&gt;: False,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retries&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#666&#34;&gt;1&lt;/span&gt;,

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;retry_delay&amp;#39;&lt;/span&gt;: timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;5&lt;/span&gt;)

}

 

dag &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DAG(

    &lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;kubernetes_sample&amp;#39;&lt;/span&gt;, default_args&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;default_args, schedule_interval&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;timedelta(minutes&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#666&#34;&gt;10&lt;/span&gt;))

start &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; DummyOperator(task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;run_this_first&amp;#39;&lt;/span&gt;, dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag)
passing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python:3.6&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-test&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;passing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )

 failing &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;ubuntu:1604&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          labels&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;foo&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;bar&amp;#34;&lt;/span&gt;},

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )

passing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)

failing&lt;span style=&#34;color:#666&#34;&gt;.&lt;/span&gt;set_upstream(start)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--

## But how does this relate to my workflow?

 

While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.

 

### 1: PR in github

Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR&#39;ing your code, and merge to the master branch to trigger an automated CI build.

 

### 2: CI/CD via Jenkins -&gt; Docker Image

 

Generate your Docker images and bump release version within your Jenkins build.

 

### 3: Airflow launches task

 

Finally, update your DAGs to reflect the new release version and you should be ready to go!

--&gt;

&lt;p&gt;##但这与我的工作流程有什么关系？&lt;/p&gt;

&lt;p&gt;虽然这个例子只使用基本映像，但Docker的神奇之处在于，这个相同的DAG可以用于您想要的任何图像/命令配对。 以下是推荐的CI / CD管道，用于在Airflow DAG上运行生产就绪代码。&lt;/p&gt;

&lt;h3 id=&#34;1-github中的pr&#34;&gt;1：github中的PR&lt;/h3&gt;

&lt;p&gt;使用Travis或Jenkins运行单元和集成测试，请您的朋友PR您的代码，并合并到主分支以触发自动CI构建。&lt;/p&gt;

&lt;h3 id=&#34;2-ci-cd构建jenkins-docker-image&#34;&gt;2：CI / CD构建Jenkins - &amp;gt; Docker Image&lt;/h3&gt;

&lt;p&gt;[在Jenkins构建中生成Docker镜像和缓冲版本]（&lt;a href=&#34;https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers）。&#34; target=&#34;_blank&#34;&gt;https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers）。&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;3-airflow启动任务&#34;&gt;3：Airflow启动任务&lt;/h3&gt;

&lt;p&gt;最后，更新您的DAG以反映新版本，您应该准备好了！&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;production_task &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; KubernetesPodOperator(namespace&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#39;default&amp;#39;&lt;/span&gt;,

                          &lt;span style=&#34;color:#080;font-style:italic&#34;&gt;# image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span&gt;

                          image&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span&gt;,

                          cmds&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;Python&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;],

                          arguments&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span&gt;],

                          name&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;fail&amp;#34;&lt;/span&gt;,

                          task_id&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#b44&#34;&gt;&amp;#34;failing-task&amp;#34;&lt;/span&gt;,

                          get_logs&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;True,

                          dag&lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt;dag

                          )&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;!--

# Launching a test deployment

 

Since the Kubernetes Operator is not yet released, we haven&#39;t released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below  and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:

 

## Step 1: Set your kubeconfig to point to a kubernetes cluster

 

## Step 2: Clone the Airflow Repo:

 

Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.

 

## Step 3: Run

 

To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:

--&gt;

&lt;p&gt;＃启动测试部署&lt;/p&gt;

&lt;p&gt;由于Kubernetes运营商尚未发布，我们尚未发布官方&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;helm&lt;/a&gt; 图表或operator（但两者目前都在进行中）。 但是，我们在下面列出了基本部署的说明，并且正在积极寻找测试人员来尝试这一新功能。 要试用此系统，请按以下步骤操作：&lt;/p&gt;

&lt;p&gt;##步骤1：将kubeconfig设置为指向kubernetes集群&lt;/p&gt;

&lt;p&gt;##步骤2：clone Airflow 仓库：&lt;/p&gt;

&lt;p&gt;运行git clone https：// github.com / apache / incubator-airflow.git来clone官方Airflow仓库。&lt;/p&gt;

&lt;p&gt;##步骤3：运行&lt;/p&gt;

&lt;p&gt;为了运行这个基本Deployment，我们正在选择我们目前用于Kubernetes Executor的集成测试脚本（将在本系列的下一篇文章中对此进行解释）。 要启动此部署，请运行以下三个命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml

./scripts/ci/kubernetes/Docker/build.sh

./scripts/ci/kubernetes/kube/deploy.sh

&lt;/code&gt;&lt;/pre&gt;

&lt;!--

Before we move on, let&#39;s discuss what these commands are doing:

 

### sed -ie &#34;s/KubernetesExecutor/LocalExecutor/g&#34; scripts/ci/kubernetes/kube/configmaps.yaml

 

The Kubernetes Executor is another Airflow feature that allows for dynamic allocation  of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.

 

### ./scripts/ci/kubernetes/Docker/build.sh

 

This script will tar the Airflow master source code build a Docker container based on the Airflow distribution

 

### ./scripts/ci/kubernetes/kube/deploy.sh

 

Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml

 

## Step 4: Log into your webserver

 

Now that your Airflow instance is running let&#39;s take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run

--&gt;

&lt;p&gt;在我们继续之前，让我们讨论这些命令正在做什么：&lt;/p&gt;

&lt;h3 id=&#34;sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml&#34;&gt;sed -ie“s / KubernetesExecutor / LocalExecutor / g”scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3&gt;

&lt;p&gt;Kubernetes Executor是另一种Airflow功能，允许动态分配任务已解决幂等pod的问题。我们将其切换到LocalExecutor的原因只是一次引入一个功能。如果您想尝试Kubernetes Executor，欢迎您跳过此步骤，但我们将在以后的文章中详细介绍。&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-docker-build-sh&#34;&gt;./scripts/ci/kubernetes/Docker/build.sh&lt;/h3&gt;

&lt;p&gt;此脚本将对Airflow主分支代码进行打包，以根据Airflow的发行文件构建Docker容器&lt;/p&gt;

&lt;h3 id=&#34;scripts-ci-kubernetes-kube-deploy-sh&#34;&gt;./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3&gt;

&lt;p&gt;最后，我们在您的群集上创建完整的Airflow部署。这包括Airflow配置，postgres后端，webserver +调度程序以及之间的所有必要服务。需要注意的一点是，提供的角色绑定是集群管理员，因此如果您没有该集群的权限级别，可以在scripts / ci / kubernetes / kube / airflow.yaml中进行修改。&lt;/p&gt;

&lt;p&gt;##步骤4：登录您的网络服务器&lt;/p&gt;

&lt;p&gt;现在您的Airflow实例正在运行，让我们来看看UI！用户界面位于Airflow pod的8080端口，因此只需运行即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
WEB=$(kubectl get pods -o go-template --template &#39;{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}&#39; | grep &amp;quot;airflow&amp;quot; | head -1)

kubectl port-forward $WEB 8080:8080

&lt;/code&gt;&lt;/pre&gt;

&lt;!--

 Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.

 

## Step 5: Upload a test document

 

To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:

--&gt;

&lt;p&gt;现在，Airflow UI将存在于&lt;a href=&#34;http://localhost:8080上。&#34; target=&#34;_blank&#34;&gt;http://localhost:8080上。&lt;/a&gt; 要登录，只需输入airflow /airflow，您就可以完全访问Airflow Web UI。&lt;/p&gt;

&lt;p&gt;##步骤5：上传测试文档&lt;/p&gt;

&lt;p&gt;要修改/添加自己的DAG，可以使用kubectl cp将本地文件上传到Airflow调度程序的DAG文件夹中。 然后，Airflow将读取新的DAG并自动将其上传到其系统。 以下命令将任何本地文件上载到正确的目录中：&lt;/p&gt;

&lt;p&gt;kubectl cp &lt;local file&gt; &lt;namespace&gt;/&lt;pod&gt;:/root/airflow/dags -c scheduler&lt;/p&gt;

&lt;!--

## Step 6: Enjoy!

 

# So when will I be able to use this?

 

 While this feature is still in the early stages, we hope to see it released for wide release in the next few months.

 

# Get Involved

 

This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributers can have a huge influence on the future of these features.

 

For those interested in joining these efforts, I&#39;d recommend checkint out these steps:

 

 * Join the airflow-dev mailing list at dev@airflow.apache.org.

 * File an issue in Apache Airflow JIRA

 * Join our SIG-BigData meetings on Wednesdays at 10am PST.

 * Reach us on slack at #sig-big-data on kubernetes.slack.com

 

Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.

--&gt;

&lt;p&gt;##步骤6：使用它！&lt;/p&gt;

&lt;p&gt;#那么我什么时候可以使用它？&lt;/p&gt;

&lt;p&gt;虽然此功能仍处于早期阶段，但我们希望在未来几个月内发布该功能以进行广泛发布。&lt;/p&gt;

&lt;p&gt;#参与其中&lt;/p&gt;

&lt;p&gt;此功能只是将Apache Airflow集成到Kubernetes中的多项主要工作的开始。 Kubernetes Operator已合并到[Airflow的1.10发布分支]（&lt;a href=&#34;https://github.com/apache/incubator-airflow/tree/v1-10-test）（实验模式中的执行模块），以及完整的k8s本地调度程序称为Kubernetes&#34; target=&#34;_blank&#34;&gt;https://github.com/apache/incubator-airflow/tree/v1-10-test）（实验模式中的执行模块），以及完整的k8s本地调度程序称为Kubernetes&lt;/a&gt; Executor（即将发布文章）。这些功能仍处于早期采用者/贡献者可能对这些功能的未来产生巨大影响的阶段。&lt;/p&gt;

&lt;p&gt;对于有兴趣加入这些工作的人，我建议按照以下步骤：&lt;/p&gt;

&lt;p&gt;*加入airflow-dev邮件列表dev@airflow.apache.org。&lt;/p&gt;

&lt;p&gt;*在[Apache Airflow JIRA]中提出问题（&lt;a href=&#34;https://issues.apache.org/jira/projects/AIRFLOW/issues/）&#34; target=&#34;_blank&#34;&gt;https://issues.apache.org/jira/projects/AIRFLOW/issues/）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;*周三上午10点太平洋标准时间加入我们的SIG-BigData会议。&lt;/p&gt;

&lt;p&gt;*在kubernetes.slack.com上的＃sig-big-data找到我们。&lt;/p&gt;

&lt;p&gt;特别感谢Apache Airflow和Kubernetes社区，特别是Grant Nicholas，Ben Goldberg，Anirudh Ramanathan，Fokko Dreisprong和Bolke de Bruin，感谢您对这些功能的巨大帮助以及我们未来的努力。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: Principles of Container-based Application Design</title>
      <link>https://qinlj.github.io/blog/2018/03/principles-of-container-app-design/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/blog/2018/03/principles-of-container-app-design/</guid>
      <description>
        
        
        

&lt;!-- It&#39;s possible nowadays to put almost any application in a container and run it. Creating cloud-native applications, however—containerized applications that are automated and orchestrated effectively by a cloud-native platform such as Kubernetes—requires additional effort. Cloud-native applications anticipate failure; they run and scale reliably even when their infrastructure experiences outages. To offer such capabilities, cloud-native platforms like Kubernetes impose a set of contracts and constraints on applications. These contracts ensure that applications they run conform to certain constraints and allow the platform to automate application management. --&gt;

&lt;p&gt;现如今，几乎所有的的应用程序都可以在容器中运行。但创建云原生应用，通过诸如 Kubernetes 的云原生平台更有效地自动化运行、管理容器化的应用却需要额外的工作。
云原生应用需要考虑故障；即使是在底层架构发生故障时也需要可靠地运行。
为了提供这样的功能，像 Kubernetes 这样的云原生平台需要向运行的应用程序强加一些契约和约束。
这些契约确保应用可以在符合某些约束的条件下运行，从而使得平台可以自动化应用管理。&lt;/p&gt;

&lt;!-- I&#39;ve outlined [seven principles][1]for containerized applications to follow in order to be fully cloud-native. --&gt;

&lt;p&gt;我已经为容器化应用如何之为云原生应用概括出了&lt;a href=&#34;https://www.redhat.com/en/resources/cloud-native-container-design-whitepaper&#34; target=&#34;_blank&#34;&gt;七项原则&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;| &amp;mdash;&amp;ndash; |
| &lt;img src=&#34;https://lh5.googleusercontent.com/1XqojkVC0CET1yKCJqZ3-0VWxJ3W8Q74zPLlqnn6eHSJsjHOiBTB7EGUX5o_BOKumgfkxVdgBeLyoyMfMIXwVm9p2QXkq_RRy2mDJG1qEExJDculYL5PciYcWfPAKxF2-DGIdiLw&#34; alt=&#34;&#34; /&gt;  |
| Container Design Principles |&lt;/p&gt;

&lt;!-- These seven principles cover both build time and runtime concerns. --&gt;

&lt;p&gt;这里所述的七项原则涉及到构建时和运行时，两类关注点。&lt;/p&gt;

&lt;!-- ####  Build time --&gt;

&lt;h4 id=&#34;构建时&#34;&gt;构建时&lt;/h4&gt;

&lt;!-- * **Single Concern:** Each container addresses a single concern and does it well.
* **Self-Containment:** A container relies only on the presence of the Linux kernel. Additional libraries are added when the container is built.
* **Image Immutability:** Containerized applications are meant to be immutable, and once built are not expected to change between different environments. --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;单一关注点：&lt;/strong&gt; 每个容器只解决一个关注点，并且完成的很好。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自包含：&lt;/strong&gt; 一个容器只依赖Linux内核。额外的库要求可以在构建容器时加入。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;镜像不变性：&lt;/strong&gt; 容器化的应用意味着不变性，一旦构建完成，不需要根据环境的不同而重新构建。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ####  Runtime --&gt;

&lt;h4 id=&#34;运行时&#34;&gt;运行时&lt;/h4&gt;

&lt;!-- * **High Observability:** Every container must implement all necessary APIs to help the platform observe and manage the application in the best way possible.
* **Lifecycle Conformance:** A container must have a way to read events coming from the platform and conform by reacting to those events.
* **Process Disposability:** Containerized applications must be as ephemeral as possible and ready to be replaced by another container instance at any point in time.
* **Runtime Confinement:** Every container must declare its resource requirements and restrict resource use to the requirements indicated. --&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高可观测性：&lt;/strong&gt; 每个容器必须实现所有必要的 API 来帮助平台以最好的方式来观测、管理应用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生命周期一致性：&lt;/strong&gt; 一个容器必须要能从平台中获取事件信息，并作出相应的反应。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进程易处理性：&lt;/strong&gt; 容器化应用的寿命一定要尽可能的短暂，这样，可以随时被另一个容器所替换。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;运行时限制：&lt;/strong&gt; 每个容器都必须要声明自己的资源需求，并将资源使用限制在所需要的范围之内。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- The build time principles ensure that containers have the right granularity, consistency, and structure in place. The runtime principles dictate what functionalities must be implemented in order for containerized applications to possess cloud-native function. Adhering to these principles helps ensure that your applications are suitable for automation in Kubernetes. --&gt;

&lt;p&gt;编译时原则保证了容器拥有合适的粒度，一致性以及结构。运行时原则明确了容器化必须要实现那些功能才能成为云原生函数。遵循这些原则可以帮助你的应用适应 Kubernetes 上的自动化。&lt;/p&gt;

&lt;!-- The white paper is freely available for download: --&gt;

&lt;p&gt;白皮书可以免费下载：&lt;/p&gt;

&lt;!-- To read more about designing cloud-native applications for Kubernetes, check out my [Kubernetes Patterns][3] book. --&gt;

&lt;p&gt;想要了解更多关于如何面向 Kubernetes 设计云原生应用，可以看看我的 &lt;a href=&#34;http://leanpub.com/k8spatterns/&#34; target=&#34;_blank&#34;&gt;Kubernetes 模式&lt;/a&gt; 一书。&lt;/p&gt;

&lt;!-- — [Bilgin Ibryam][4], Principal Architect, Red Hat --&gt;

&lt;p&gt;— &lt;a href=&#34;http://twitter.com/bibryam&#34; target=&#34;_blank&#34;&gt;Bilgin Ibryam&lt;/a&gt;, 首席架构师, Red Hat&lt;/p&gt;

&lt;p&gt;Twitter:  
Blog: &lt;a href=&#34;http://www.ofbizian.com/&#34; target=&#34;_blank&#34;&gt;http://www.ofbizian.com&lt;/a&gt;
Linkedin:&lt;/p&gt;

&lt;!-- Bilgin Ibryam (@bibryam) is a principal architect at Red Hat, open source committer at ASF, blogger, author, and speaker. He is the author of Camel Design Patterns and Kubernetes Patterns books. In his day-to-day job, Bilgin enjoys mentoring, training and leading teams to be successful with distributed systems, microservices, containers, and cloud-native applications in general. --&gt;

&lt;p&gt;Bilgin Ibryam (@bibryam) 是 Red Hat 的一名首席架构师， ASF 的开源贡献者，博主，作者以及演讲者。
他是 Camel 设计模式、 Kubernetes 模式的作者。在他的日常生活中，他非常享受指导、培训以及帮助各个团队更加成功地使用分布式系统、微服务、容器，以及云原生应用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: </title>
      <link>https://qinlj.github.io/zh/blog/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/zh/blog/1/01/01/</guid>
      <description>
        
        
        

&lt;hr /&gt;

&lt;p&gt;title: &amp;ldquo;SIG-Networking: Kubernetes Network Policy APIs Coming in 1.3 &amp;ldquo;
date: 2016-04-18
slug: kubernetes-network-policy-apis&lt;/p&gt;

&lt;h2 id=&#34;url-blog-2016-04-kubernetes-network-policy-apis&#34;&gt;url: /blog/2016/04/Kubernetes-Network-Policy-APIs&lt;/h2&gt;

&lt;!-- _Editor’s note: This week we’re featuring [Kubernetes Special Interest Groups](https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)); Today’s post is by the Network-SIG team describing network policy APIs coming in 1.3 - policies for security, isolation and multi-tenancy._ --&gt;

&lt;p&gt;编者按：这一周，我们的封面主题是 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs)&#34; target=&#34;_blank&#34;&gt;Kubernetes 特别兴趣小组&lt;/a&gt;；今天的文章由网络兴趣小组撰写，来谈谈 1.3 版本中即将出现的网络策略 API - 针对安全，隔离和多租户的策略。&lt;/p&gt;

&lt;!-- The [Kubernetes network SIG](https://kubernetes.slack.com/messages/sig-network/) has been meeting regularly since late last year to work on bringing network policy to Kubernetes and we’re starting to see the results of this effort. --&gt;

&lt;p&gt;自去年下半年起，&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-network/&#34; target=&#34;_blank&#34;&gt;Kubernetes 网络特别兴趣小组&lt;/a&gt;经常定期开会，讨论如何将网络策略带入到 Kubernetes 之中，现在，我们也将慢慢看到这些工作的成果。&lt;/p&gt;

&lt;!-- One problem many users have is that the open access network policy of Kubernetes is not suitable for applications that need more precise control over the traffic that accesses a pod or service. Today, this could be a multi-tier application where traffic is only allowed from a tier’s neighbor. But as new Cloud Native applications are built by composing microservices, the ability to control traffic as it flows among these services becomes even more critical. --&gt;

&lt;p&gt;很多用户经常会碰到的一个问题是， Kubernetes 的开放访问网络策略并不能很好地满足那些需要对 pod 或服务( service )访问进行更为精确控制的场景。今天，这个场景可以是在多层应用中，只允许临近层的访问。然而，随着组合微服务构建原生应用程序潮流的发展，如何控制流量在不同服务之间的流动会别的越发的重要。&lt;/p&gt;

&lt;!-- In most IaaS environments (both public and private) this kind of control is provided by allowing VMs to join a ‘security group’ where traffic to members of the group is defined by a network policy or Access Control List (ACL) and enforced by a network packet filter. --&gt;

&lt;p&gt;在大多数的（公共的或私有的） IaaS 环境中，这种网络控制通常是将 VM 和“安全组”结合，其中安全组中成员的通信都是通过一个网络策略或者访问控制表（ Access Control List, ACL ）来定义，以及借助于网络包过滤器来实现。&lt;/p&gt;

&lt;!-- The Network SIG started the effort by identifying [specific use case scenarios](https://docs.google.com/document/d/1blfqiH4L_fpn33ZrnQ11v7LcYP0lmpiJ_RaapAPBbNU/edit?pref=2&amp;pli=1#) that require basic network isolation for enhanced security. Getting the API right for these simple and common use cases is important because they are also the basis for the more sophisticated network policies necessary for multi-tenancy within Kubernetes. --&gt;

&lt;p&gt;“网络特别兴趣小组”刚开始的工作是确定 &lt;a href=&#34;https://docs.google.com/document/d/1blfqiH4L_fpn33ZrnQ11v7LcYP0lmpiJ_RaapAPBbNU/edit?pref=2&amp;amp;pli=1#&#34; target=&#34;_blank&#34;&gt;特定的使用场景&lt;/a&gt; ，这些用例需要基本的网络隔离来提升安全性。
让这些API恰如其分地满足简单、共通的用例尤其重要，因为它们将为那些服务于 Kubernetes 内多租户，更为复杂的网络策略奠定基础。&lt;/p&gt;

&lt;!-- From these scenarios several possible approaches were considered and a minimal [policy specification](https://docs.google.com/document/d/1qAm-_oSap-f1d6a-xRTj6xaH1sYQBfK36VyjB5XOZug/edit) was defined. The basic idea is that if isolation were enabled on a per namespace basis, then specific pods would be selected where specific traffic types would be allowed. --&gt;

&lt;p&gt;根据这些应用场景，我们考虑了集中不同的方法，然后定义了一个最简&lt;a href=&#34;https://docs.google.com/document/d/1qAm-_oSap-f1d6a-xRTj6xaH1sYQBfK36VyjB5XOZug/edit&#34; target=&#34;_blank&#34;&gt;策略规范&lt;/a&gt;。
基本的想法是，如果是根据命名空间的不同来进行隔离，那么就会根据所被允许的流量类型的不同，来选择特定的 pods 。&lt;/p&gt;

&lt;!-- The simplest way to quickly support this experimental API is in the form of a ThirdPartyResource extension to the API Server, which is possible today in Kubernetes 1.2. --&gt;

&lt;p&gt;快速支持这个实验性 API 的办法是往 API 服务器上加入一个 &lt;code&gt;ThirdPartyResource&lt;/code&gt; 扩展，这在 Kubernetes 1.2 就能办到。&lt;/p&gt;

&lt;!-- If you’re not familiar with how this works, the Kubernetes API can be extended by defining ThirdPartyResources that create a new API endpoint at a specified URL. --&gt;

&lt;p&gt;如果你还不是很熟悉这其中的细节， Kubernetes API 是可以通过定义 &lt;code&gt;ThirdPartyResources&lt;/code&gt; 扩展在特定的 URL 上创建一个新的 API 端点。&lt;/p&gt;

&lt;h4 id=&#34;third-party-res-def-yaml&#34;&gt;third-party-res-def.yaml&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;kind: ThirdPartyResource
apiVersion: extensions/v1beta1
metadata:
    - name: network-policy.net.alpha.kubernetes.io
description: &amp;quot;Network policy specification&amp;quot;
versions:
    - name: v1alpha1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$kubectl create -f third-party-res-def.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- This will create an API endpoint (one for each namespace): --&gt;

&lt;p&gt;这条命令会创建一个 API 端点（每个命名空间各一个）:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/net.alpha.kubernetes.io/v1alpha1/namespace/default/networkpolicys/
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- Third party network controllers can now listen on these endpoints and react as necessary when resources are created, modified or deleted. _Note: With the upcoming release of Kubernetes 1.3 - when the Network Policy API is released in beta form - there will be no need to create a ThirdPartyResource API endpoint as shown above._ --&gt;

&lt;p&gt;第三方网络控制器可以监听这些端点，根据资源的创建，修改或者删除作出必要的响应。
&lt;em&gt;注意：在接下来的 Kubernetes 1.3 发布中， Network Policy API 会以 beta API 的形式出现，这也就不需要像上面那样，创建一个 &lt;code&gt;ThirdPartyResource&lt;/code&gt; API 端点了。&lt;/em&gt;&lt;/p&gt;

&lt;!-- Network isolation is off by default so that all pods can communicate as they normally do. However, it’s important to know that once network isolation is enabled, all traffic to all pods, in all namespaces is blocked, which means that enabling isolation is going to change the behavior of your pods --&gt;

&lt;p&gt;网络隔离默认是关闭的，因而，所有的 pods 之间可以自由地通信。
然而，很重要的一点是，一旦开通了网络隔离，所有命名空间下的所有 pods 之间的通信都会被阻断，换句话说，开通隔离会改变 pods 的行为。&lt;/p&gt;

&lt;!-- Network isolation is enabled by defining the _network-isolation_ annotation on namespaces as shown below: --&gt;

&lt;p&gt;网络隔离可以通过定义命名空间， &lt;code&gt;net.alpha.kubernetes.io&lt;/code&gt; 里的 &lt;code&gt;network-isolation&lt;/code&gt; 注释来开通关闭:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;net.alpha.kubernetes.io/network-isolation: [on | off]
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- Once network isolation is enabled, explicit network policies **must be applied** to enable pod communication. --&gt;

&lt;p&gt;一旦开通了网络隔离，&lt;strong&gt;一定需要使用&lt;/strong&gt; 显示的网络策略来允许 pod 间的通信。&lt;/p&gt;

&lt;!-- A policy specification can be applied to a namespace to define the details of the policy as shown below: --&gt;

&lt;p&gt;一个策略规范可以被用到一个命名空间中，来定义策略的细节（如下所示）：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;POST /apis/net.alpha.kubernetes.io/v1alpha1/namespaces/tenant-a/networkpolicys/
{
  &amp;quot;kind&amp;quot;: &amp;quot;NetworkPolicy&amp;quot;,
  &amp;quot;metadata&amp;quot;: {
    &amp;quot;name&amp;quot;: &amp;quot;pol1&amp;quot;
  },
  &amp;quot;spec&amp;quot;: {
    &amp;quot;allowIncoming&amp;quot;: {
      &amp;quot;from&amp;quot;: [
        {
          &amp;quot;pods&amp;quot;: {
            &amp;quot;segment&amp;quot;: &amp;quot;frontend&amp;quot;
          }
        }
      ],
      &amp;quot;toPorts&amp;quot;: [
        {
          &amp;quot;port&amp;quot;: 80,
          &amp;quot;protocol&amp;quot;: &amp;quot;TCP&amp;quot;
        }
      ]
    },
    &amp;quot;podSelector&amp;quot;: {
      &amp;quot;segment&amp;quot;: &amp;quot;backend&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- In this example, the ‘ **tenant-a** ’ namespace would get policy ‘ **pol1** ’ applied as indicated. Specifically, pods with the **segment** label ‘ **backend** ’ would allow TCP traffic on port 80 from pods with the **segment** label ‘ **frontend** ’ to be received. --&gt;

&lt;p&gt;在这个例子中，&lt;strong&gt;tenant-a&lt;/strong&gt; 空间将会使用 &lt;strong&gt;pol1&lt;/strong&gt; 策略。
具体而言，带有 &lt;strong&gt;segment&lt;/strong&gt; 标签为 &lt;strong&gt;backend&lt;/strong&gt; 的 pods 会允许 &lt;strong&gt;segment&lt;/strong&gt; 标签为 &lt;strong&gt;frontend&lt;/strong&gt; 的 pods 访问其端口 80 。&lt;/p&gt;

&lt;!-- Today, [Romana](http://romana.io/), [OpenShift](https://www.openshift.com/), [OpenContrail](http://www.opencontrail.org/) and [Calico](http://projectcalico.org/) support network policies applied to namespaces and pods. Cisco and VMware are working on implementations as well. Both Romana and Calico demonstrated these capabilities with Kubernetes 1.2 recently at KubeCon. You can watch their presentations here: [Romana](https://www.youtube.com/watch?v=f-dLKtK6qCs) ([slides](http://www.slideshare.net/RomanaProject/kubecon-london-2016-ronana-cloud-native-sdn)), [Calico](https://www.youtube.com/watch?v=p1zfh4N4SX0) ([slides](http://www.slideshare.net/kubecon/kubecon-eu-2016-secure-cloudnative-networking-with-project-calico)).&amp;nbsp; --&gt;

&lt;p&gt;今天，&lt;a href=&#34;http://romana.io/&#34; target=&#34;_blank&#34;&gt;Romana&lt;/a&gt;, &lt;a href=&#34;https://www.openshift.com/&#34; target=&#34;_blank&#34;&gt;OpenShift&lt;/a&gt;, &lt;a href=&#34;http://www.opencontrail.org/&#34; target=&#34;_blank&#34;&gt;OpenContrail&lt;/a&gt; 以及 &lt;a href=&#34;http://projectcalico.org/&#34; target=&#34;_blank&#34;&gt;Calico&lt;/a&gt; 都已经支持在命名空间和pods中使用网络策略。
而 Cisco 和 VMware 也在努力实现支持之中。
Romana 和 Calico 已经在最近的 KubeCon 中展示了如何在 Kubernetes 1.2 下使用这些功能。
你可以在这里看到他们的演讲：
&lt;a href=&#34;https://www.youtube.com/watch?v=f-dLKtK6qCs&#34; target=&#34;_blank&#34;&gt;Romana&lt;/a&gt; (&lt;a href=&#34;http://www.slideshare.net/RomanaProject/kubecon-london-2016-ronana-cloud-native-sdn&#34; target=&#34;_blank&#34;&gt;幻灯片&lt;/a&gt;),
&lt;a href=&#34;https://www.youtube.com/watch?v=p1zfh4N4SX0&#34; target=&#34;_blank&#34;&gt;Calico&lt;/a&gt; (&lt;a href=&#34;http://www.slideshare.net/kubecon/kubecon-eu-2016-secure-cloudnative-networking-with-project-calico&#34; target=&#34;_blank&#34;&gt;幻灯片&lt;/a&gt;).&lt;/p&gt;

&lt;!-- **How does it work?** --&gt;

&lt;p&gt;&lt;strong&gt;这是如何工作的&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Each solution has their their own specific implementation details. Today, they rely on some kind of on-host enforcement mechanism, but future implementations could also be built that apply policy on a hypervisor, or even directly by the network itself.&amp;nbsp; --&gt;

&lt;p&gt;每套解决方案都有自己不同的具体实现。尽管今天，他们都借助于每种主机上（ on-host ）的实现机制，但未来的实现可以通过将策略使用在 hypervisor 上，亦或是直接使用到网络本身上来达到同样的目的。&lt;/p&gt;

&lt;!-- External policy control software (specifics vary across implementations) will watch the new API endpoint for pods being created and/or new policies being applied. When an event occurs that requires policy configuration, the listener will recognize the change and a controller will respond by configuring the interface and applying the policy. &amp;nbsp;The diagram below shows an API listener and policy controller responding to updates by applying a network policy locally via a host agent. The network interface on the pods is configured by a CNI plugin on the host (not shown). --&gt;

&lt;p&gt;外部策略控制软件（不同实现各有不同）可以监听 pods 创建以及新加载策略的 API 端点。
当产生一个需要策略配置的事件之后，监听器会确认这个请求，相应的，控制器会配置接口，使用该策略。
下面的图例展示了 API 监视器和策略控制器是如何通过主机代理在本地应用网络策略的。
这些 pods 的网络接口是使用过主机上的 CNI 插件来进行配置的（并未在图中注明）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh5.googleusercontent.com/zMEpLMYmask-B-rYWnbMyGb0M7YusPQFPS6EfpNOSLbkf-cM49V7rTDBpA6k9-Zdh2soMul39rz9rHFJfL-jnEn_mHbpg0E1WlM-wjU-qvQu9KDTQqQ9uBmdaeWynDDNhcT3UjX5&#34; alt=&#34;controller.jpg&#34; /&gt;&lt;/p&gt;

&lt;!-- If you’ve been holding back on developing applications with Kubernetes because of network isolation and/or security concerns, these new network policies go a long way to providing the control you need. No need to wait until Kubernetes 1.3 since network policy is available now as an experimental API enabled as a ThirdPartyResource. --&gt;

&lt;p&gt;如果你一直受网络隔离或安全考虑的困扰，而犹豫要不要使用 Kubernetes 来开发应用程序，这些新的网络策略将会极大地解决你这方面的需求。并不需要等到 Kubernetes 1.3 ，现在就可以通过 &lt;code&gt;ThirdPartyResource&lt;/code&gt; 的方式来使用这个实现性 API 。&lt;/p&gt;

&lt;!-- If you’re interested in Kubernetes and networking, there are several ways to participate - join us at:

- Our [Networking slack channel](https://kubernetes.slack.com/messages/sig-network/)
- Our [Kubernetes Networking Special Interest Group](https://groups.google.com/forum/#!forum/kubernetes-sig-network) email list --&gt;

&lt;p&gt;如果你对 Kubernetes 和网络感兴趣，可以通过下面的方式参与、加入其中：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;我们的&lt;a href=&#34;https://kubernetes.slack.com/messages/sig-network/&#34; target=&#34;_blank&#34;&gt;网络 slack channel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;我们的&lt;a href=&#34;https://groups.google.com/forum/#!forum/kubernetes-sig-network&#34; target=&#34;_blank&#34;&gt;Kubernetes 特别网络兴趣小组&lt;/a&gt; 邮件列表&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- The Networking “Special Interest Group,” which meets bi-weekly at 3pm (15h00) Pacific Time at [SIG-Networking hangout](https://zoom.us/j/5806599998). --&gt;

&lt;p&gt;网络“特别兴趣小组”每两周下午三点（太平洋时间）开会，地址是&lt;a href=&#34;https://zoom.us/j/5806599998&#34; target=&#34;_blank&#34;&gt;SIG-Networking hangout&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&amp;ndash;Chris Marino, Co-Founder, Pani Networks&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Blog: </title>
      <link>https://qinlj.github.io/zh/blog/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://qinlj.github.io/zh/blog/1/01/01/</guid>
      <description>
        
        
        

&lt;hr /&gt;

&lt;p&gt;title: &amp;ldquo; 如何在AWS上部署安全，可审计，可复现的k8s集群 &amp;ldquo;
date: 2016-04-15
slug: kubernetes-on-aws_15&lt;/p&gt;

&lt;h2 id=&#34;url-blog-2016-04-kubernetes-on-aws-15&#34;&gt;url: /blog/2016/04/Kubernetes-On-Aws_15&lt;/h2&gt;

&lt;!-- _Today’s guest post is written by Colin Hom, infrastructure engineer at [CoreOS](https://coreos.com/), the company delivering Google’s Infrastructure for Everyone Else (#GIFEE) and running the world&#39;s containers securely on CoreOS Linux, Tectonic and Quay._

_Join us at [CoreOS Fest Berlin](https://coreos.com/fest/), the Open Source Distributed Systems Conference, and learn more about CoreOS and Kubernetes._ --&gt;

&lt;p&gt;&lt;em&gt;今天的客座文章是由Colin Hom撰写，&lt;a href=&#34;https://coreos.com/&#34; target=&#34;_blank&#34;&gt;CoreOS&lt;/a&gt;的基础架构工程师。CoreOS致力于推广谷歌的基础架构模式（Google’s Infrastructure for Everyone Else， #GIFEE），让全世界的容器都能在CoreOS Linux, Tectonic 和 Quay上安全运行。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;加入到我们的&lt;a href=&#34;https://coreos.com/fest/&#34; target=&#34;_blank&#34;&gt;柏林CoreOS盛宴&lt;/a&gt;，这是一个开源分布式系统主题的会议，在这里可以了解到更多关于CoreOS和Kubernetes的信息。&lt;/em&gt;&lt;/p&gt;

&lt;!-- At CoreOS, we&#39;re all about deploying Kubernetes in production at scale. Today we are excited to share a tool that makes deploying Kubernetes on Amazon Web Services (AWS) a breeze. Kube-aws is a tool for deploying auditable and reproducible Kubernetes clusters to AWS, currently used by CoreOS to spin up production clusters. --&gt;

&lt;p&gt;在CoreOS, 我们一直都是在生产环境中大规模部署Kubernetes。今天我们非常兴奋地想分享一款工具，它能让你的Kubernetes生产环境大规模部署更加的轻松。Kube-aws这个工具可以用来在AWS上部署可审计，可复现的k8s集群，而CoreOS本身就在生产环境中使用它。&lt;/p&gt;

&lt;!-- Today you might be putting the Kubernetes components together in a more manual way. With this helpful tool, Kubernetes is delivered in a streamlined package to save time, minimize interdependencies and quickly create production-ready deployments. --&gt;

&lt;p&gt;也许今天，你更多的可能是用手工的方式来拼接Kubernetes组件。但有了这个工具之后，Kubernetes可以流水化地打包、交付，节省时间，减少了相互间的依赖，更加快捷地实现生产环境的部署。&lt;/p&gt;

&lt;!-- A simple templating system is leveraged to generate cluster configuration as a set of declarative configuration templates that can be version controlled, audited and re-deployed. Since the entirety of the provisioning is by [AWS CloudFormation](https://aws.amazon.com/cloudformation/) and cloud-init, there’s no need for external configuration management tools on your end. Batteries included! --&gt;

&lt;p&gt;借助于一个简单的模板系统，来生成集群配置，这么做是因为一套声明式的配置模板可以版本控制，审计以及重复部署。而且，由于整个创建过程只用到了&lt;a href=&#34;https://aws.amazon.com/cloudformation/&#34; target=&#34;_blank&#34;&gt;AWS CloudFormation&lt;/a&gt; 和 cloud-init，你也就不需要额外用到其它的配置管理工具。开箱即用！&lt;/p&gt;

&lt;!-- To skip the talk and go straight to the project, check out [the latest release of kube-aws](https://github.com/coreos/coreos-kubernetes/releases), which supports Kubernetes 1.2.x. To get your cluster running, [check out the documentation](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html). --&gt;

&lt;p&gt;如果要跳过演讲，直接了解这个项目，可以看看&lt;a href=&#34;https://github.com/coreos/coreos-kubernetes/releases&#34; target=&#34;_blank&#34;&gt;kube-aws的最新发布&lt;/a&gt;，支持Kubernetes 1.2.x。如果要部署集群，可以参考[文档]](&lt;a href=&#34;https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html&#34; target=&#34;_blank&#34;&gt;https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html&lt;/a&gt;).&lt;/p&gt;

&lt;!-- **Why kube-aws? Security, auditability and reproducibility** --&gt;

&lt;p&gt;&lt;strong&gt;为什么是kube-aws？安全，可审计，可复现&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Kube-aws is designed with three central goals in mind. --&gt;

&lt;p&gt;Kube-aws设计初衷有三个目标。&lt;/p&gt;

&lt;!-- **Secure** : TLS assets are encrypted via the [AWS Key Management Service (KMS)](https://aws.amazon.com/kms/) before being embedded in the CloudFormation JSON. By managing [IAM policy](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) for the KMS key independently, an operator can decouple operational access to the CloudFormation stack from access to the TLS secrets. --&gt;

&lt;p&gt;&lt;strong&gt;安全&lt;/strong&gt; : TLS 资源在嵌入到CloudFormation JSON之前，通过&lt;a href=&#34;https://aws.amazon.com/kms/&#34; target=&#34;_blank&#34;&gt;AWS 秘钥管理服务&lt;/a&gt;加密。通过单独管理KMS密钥的&lt;a href=&#34;http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html&#34; target=&#34;_blank&#34;&gt;IAM 策略&lt;/a&gt;，可以将CloudFormation栈的访问与TLS秘钥的访问分离开。&lt;/p&gt;

&lt;!-- **Auditable** : kube-aws is built around the concept of cluster assets. These configuration and credential assets represent the complete description of the cluster. Since KMS is used to encrypt TLS assets, you can feel free to check your unencrypted stack JSON into version control as well! --&gt;

&lt;p&gt;&lt;strong&gt;可审计&lt;/strong&gt; : kube-aws是围绕集群资产的概念来创建。这些配置和账户资产是对集群的完全描述。由于KMS被用来加密TLS资产，因而可以无所顾忌地将未加密的CloudFormation栈 JSON签入到版本控制服务中。&lt;/p&gt;

&lt;!-- **Reproducible** : The _--export_ option packs your parameterized cluster definition into a single JSON file which defines a CloudFormation stack. This file can be version controlled and submitted directly to the CloudFormation API via existing deployment tooling, if desired. --&gt;

&lt;p&gt;&lt;strong&gt;可重复&lt;/strong&gt; : &lt;em&gt;&amp;ndash;export&lt;/em&gt; 选项将参数化的集群定义打包成一整个JSON文件，对应一个CloudFormation栈。这个文件可以版本控制，然后，如果需要的话，通过现有的部署工具直接提交给CloudFormation API。&lt;/p&gt;

&lt;!-- **How to get started with kube-aws** --&gt;

&lt;p&gt;&lt;strong&gt;如何开始用kube-aws&lt;/strong&gt;&lt;/p&gt;

&lt;!-- On top of this foundation, kube-aws implements features that make Kubernetes deployments on AWS easier to manage and more flexible. Here are some examples. --&gt;

&lt;p&gt;在此基础之上，kube-aws也实现了一些功能，使得在AWS上部署Kubernetes集群更加容易，灵活。下面是一些例子。&lt;/p&gt;

&lt;!-- **Route53 Integration** : Kube-aws can manage your cluster DNS records as part of the provisioning process. --&gt;

&lt;p&gt;&lt;strong&gt;Route53集成&lt;/strong&gt; : Kube-aws 可以管理你的集群DNS记录，作为配置过程的一部分。&lt;/p&gt;

&lt;p&gt;cluster.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;externalDNSName: my-cluster.kubernetes.coreos.com

createRecordSet: true

hostedZone: kubernetes.coreos.com

recordSetTTL: 300
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- **Existing VPC Support** : Deploy your cluster to an existing VPC. --&gt;

&lt;p&gt;&lt;strong&gt;现有VPC支持&lt;/strong&gt; : 将集群部署到现有的VPC上。&lt;/p&gt;

&lt;p&gt;cluster.yaml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vpcId: vpc-xxxxx

routeTableId: rtb-xxxxx
&lt;/code&gt;&lt;/pre&gt;

&lt;!-- **Validation** : Kube-aws supports validation of cloud-init and CloudFormation definitions, along with any external resources that the cluster stack will integrate with. For example, here’s a cloud-config with a misspelled parameter: --&gt;

&lt;p&gt;&lt;strong&gt;验证&lt;/strong&gt; : kube-aws 支持验证 cloud-init 和 CloudFormation定义，以及集群栈会集成用到的外部资源。例如，下面就是一个cloud-config，外带一个拼写错误的参数：&lt;/p&gt;

&lt;p&gt;userdata/cloud-config-worker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#cloud-config

coreos:

  flannel:
    interrface: $private\_ipv4
    etcd\_endpoints: {{ .ETCDEndpoints }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$ kube-aws validate&lt;/p&gt;

&lt;p&gt;&amp;gt; Validating UserData&amp;hellip;
     Error: cloud-config validation errors:
     UserDataWorker: line 4: warning: unrecognized key &amp;ldquo;interrface&amp;rdquo;&lt;/p&gt;

&lt;!-- To get started, check out the [kube-aws documentation](https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html). --&gt;

&lt;p&gt;考虑如何起步？看看&lt;a href=&#34;https://coreos.com/kubernetes/docs/latest/kubernetes-on-aws.html&#34; target=&#34;_blank&#34;&gt;kube-aws 文档&lt;/a&gt;！&lt;/p&gt;

&lt;!-- **Future Work** --&gt;

&lt;p&gt;&lt;strong&gt;未来的工作&lt;/strong&gt;&lt;/p&gt;

&lt;!-- As always, the goal with kube-aws is to make deployments that are production ready. While we use kube-aws in production on AWS today, this project is pre-1.0 and there are a number of areas in which kube-aws needs to evolve. --&gt;

&lt;p&gt;一如既往，kube-aws的目标是让生产环境部署更加的简单。尽管我们现在在AWS下使用kube-aws进行生产环境部署，但是这个项目还是pre-1.0，所以还有很多的地方，kube-aws需要考虑、扩展。&lt;/p&gt;

&lt;!-- **Fault tolerance** : At CoreOS we believe Kubernetes on AWS is a potent platform for fault-tolerant and self-healing deployments. In the upcoming weeks, kube-aws will be rising to a new challenge: surviving the [Chaos Monkey](https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey) – control plane and all! --&gt;

&lt;p&gt;&lt;strong&gt;容错&lt;/strong&gt; : CoreOS坚信 Kubernetes on AWS是强健的平台，适于容错、自恢复部署。在接下来的几个星期，kube-aws将会迎接新的考验：混世猴子（&lt;a href=&#34;https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey&#34; target=&#34;_blank&#34;&gt;Chaos Monkey&lt;/a&gt;）测试 - 控制平面以及全部！&lt;/p&gt;

&lt;!-- **Zero-downtime updates** : Updating CoreOS nodes and Kubernetes components can be done without downtime and without interdependency with the correct instance replacement strategy. --&gt;

&lt;p&gt;&lt;strong&gt;零停机更新&lt;/strong&gt; : 更新CoreOS节点和Kubernetes组件不需要停机，也不需要考虑实例更新策略（instance replacement strategy）的影响。&lt;/p&gt;

&lt;!-- A [github issue](https://github.com/coreos/coreos-kubernetes/issues/340) tracks the work towards this goal. We look forward to seeing you get involved with the project by filing issues or contributing directly. --&gt;

&lt;p&gt;有一个&lt;a href=&#34;https://github.com/coreos/coreos-kubernetes/issues/340&#34; target=&#34;_blank&#34;&gt;github issue&lt;/a&gt;来追踪这些工作进展。我们期待你的参与，提交issue，或是直接贡献。&lt;/p&gt;

&lt;!-- _Learn more about Kubernetes and meet the community at [CoreOS Fest Berlin](https://coreos.com/fest/) - May 9-10, 2016_ --&gt;

&lt;p&gt;&lt;em&gt;想要更多地了解Kubernetes，来&lt;a href=&#34;https://coreos.com/fest/&#34; target=&#34;_blank&#34;&gt;柏林CoreOS盛宴&lt;/a&gt;看看，- 五月 9-10, 2016&lt;/em&gt;&lt;/p&gt;

&lt;!-- _– Colin Hom, infrastructure engineer, CoreOS_ --&gt;

&lt;p&gt;&lt;em&gt;– Colin Hom, 基础架构工程师, CoreOS&lt;/em&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
